crypten_nn_minibatch.py:168: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  int_training_data = torch.tensor(training_data.data).float()
crypten_nn_minibatch.py:169: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  int_training_labels = torch.tensor(training_data.targets).long()
training encrypted model
trial 0 batch size 32
/home/cc/chz_sok_experiments/chz-sok-nn-experiments/nn_venv/lib/python3.8/site-packages/crypten/nn/onnx_converter.py:176: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
  param = torch.from_numpy(numpy_helper.to_array(node))
/home/cc/chz_sok_experiments/chz-sok-nn-experiments/nn_venv/lib/python3.8/site-packages/crypten/nn/onnx_converter.py:176: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
  param = torch.from_numpy(numpy_helper.to_array(node))
ready to train
loss: 2.3204803466796875  [32/60000], time: 2.182579642976634
loss: 2.306610107421875  [3232/60000], time: 2.2415381229948252
loss: 2.288848876953125  [6432/60000], time: 2.1883569590281695
loss: 2.2627410888671875  [9632/60000], time: 2.2376153729856014
loss: 2.2449188232421875  [12832/60000], time: 2.2117867070483044
loss: 2.2418670654296875  [16032/60000], time: 2.256052140961401
loss: 2.2247467041015625  [19232/60000], time: 2.1627307479502633
loss: 2.208282470703125  [22432/60000], time: 2.1518161119893193
loss: 2.2038116455078125  [25632/60000], time: 2.132974779000506
loss: 2.1373748779296875  [28832/60000], time: 2.1402092499192804
loss: 2.094512939453125  [32032/60000], time: 2.18350660498254
loss: 2.0692291259765625  [35232/60000], time: 2.1132078850641847
loss: 2.1624603271484375  [38432/60000], time: 2.226738099940121
loss: 2.057281494140625  [41632/60000], time: 2.1108473950298503
loss: 2.0232696533203125  [44832/60000], time: 2.121680877986364
loss: 1.9830169677734375  [48032/60000], time: 2.1529399099526927
loss: 2.0518951416015625  [51232/60000], time: 2.1553490420337766
loss: 1.96942138671875  [54432/60000], time: 2.132495555910282
loss: 1.867218017578125  [57632/60000], time: 2.112184653058648
tensor(1.8903)
batch number: 1/157
tensor(96.1127)
batch number: 51/157
tensor(190.0692)
batch number: 101/157
tensor(284.1915)
batch number: 151/157
Test Error: 
 Accuracy: 0.5472, Avg loss: 1.8823298215866089 

Epoch 0 took 4041.6072013300145 seconds
loss: 1.8763885498046875  [32/60000], time: 2.1411576869431883
loss: 1.90447998046875  [3232/60000], time: 2.111676040920429
loss: 1.8202972412109375  [6432/60000], time: 2.1493648909963667
loss: 1.7789154052734375  [9632/60000], time: 2.115336071001366
loss: 1.7139129638671875  [12832/60000], time: 2.102713751955889
loss: 1.758941650390625  [16032/60000], time: 2.2608848699601367
loss: 1.602264404296875  [19232/60000], time: 2.208807850955054
loss: 1.583343505859375  [22432/60000], time: 2.189996554981917
loss: 1.583251953125  [25632/60000], time: 2.220736938994378
loss: 1.539642333984375  [28832/60000], time: 2.14333932090085
loss: 1.4112701416015625  [32032/60000], time: 2.2020099069923162
loss: 1.3544769287109375  [35232/60000], time: 2.0715980150271207
loss: 1.55450439453125  [38432/60000], time: 2.091887532034889
loss: 1.40667724609375  [41632/60000], time: 2.1211107739945874
loss: 1.32415771484375  [44832/60000], time: 2.197451640968211
loss: 1.2870330810546875  [48032/60000], time: 2.246126616024412
loss: 1.3839569091796875  [51232/60000], time: 2.2124916100874543
loss: 1.375457763671875  [54432/60000], time: 2.248347304062918
loss: 1.2120513916015625  [57632/60000], time: 2.190906836069189
tensor(1.2579)
batch number: 1/157
tensor(64.8366)
batch number: 51/157
tensor(128.2653)
batch number: 101/157
tensor(191.3780)
batch number: 151/157
Test Error: 
 Accuracy: 0.6255, Avg loss: 1.266748070716858 

Epoch 1 took 4058.9423923089635 seconds
loss: 1.26513671875  [32/60000], time: 2.1578952179988846
loss: 1.2532806396484375  [3232/60000], time: 2.2265698859700933
loss: 1.251068115234375  [6432/60000], time: 2.2421077709877864
loss: 1.30926513671875  [9632/60000], time: 2.174064693041146
loss: 1.1620635986328125  [12832/60000], time: 2.110369583009742
loss: 1.3224334716796875  [16032/60000], time: 2.1675682280911133
loss: 1.0502777099609375  [19232/60000], time: 2.20012582000345
loss: 1.0658416748046875  [22432/60000], time: 2.149929358973168
loss: 1.1278839111328125  [25632/60000], time: 2.0511238570325077
loss: 1.2013397216796875  [28832/60000], time: 2.1921236489433795
loss: 1.035308837890625  [32032/60000], time: 2.1896881280699745
loss: 0.985198974609375  [35232/60000], time: 2.096256464952603
loss: 1.260406494140625  [38432/60000], time: 2.2610626260284334
loss: 1.0842132568359375  [41632/60000], time: 2.149370788014494
loss: 1.0130615234375  [44832/60000], time: 2.131457805982791
loss: 0.9334716796875  [48032/60000], time: 2.061223329976201
loss: 1.0523223876953125  [51232/60000], time: 2.153075151029043
loss: 1.152618408203125  [54432/60000], time: 2.112531032995321
loss: 0.9328155517578125  [57632/60000], time: 2.242027058964595
tensor(1.0035)
batch number: 1/157
tensor(51.5554)
batch number: 51/157
tensor(102.1308)
batch number: 101/157
tensor(152.3970)
batch number: 151/157
Test Error: 
 Accuracy: 0.6551, Avg loss: 1.0080249309539795 

Epoch 2 took 4049.698870537919 seconds
/home/cc/chz_sok_experiments/chz-sok-nn-experiments/nn_venv/lib/python3.8/site-packages/crypten/nn/onnx_converter.py:176: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
  param = torch.from_numpy(numpy_helper.to_array(node))
/home/cc/chz_sok_experiments/chz-sok-nn-experiments/nn_venv/lib/python3.8/site-packages/crypten/nn/onnx_converter.py:176: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
  param = torch.from_numpy(numpy_helper.to_array(node))
ready to train
loss: 11.614974975585938  [32/60000], time: 2.0862797669833526
Model parameters [1] did not change
loss: 0.87353515625  [3232/60000], time: 2.143357937107794
loss: 0.5946807861328125  [6432/60000], time: 2.1444370120298117
Model parameters [1] did not change
loss: 0.9772491455078125  [9632/60000], time: 2.198740754975006
loss: 0.3985595703125  [12832/60000], time: 2.180706663057208
loss: 0.9840087890625  [16032/60000], time: 2.2348330379463732
loss: 0.4853515625  [19232/60000], time: 2.153227736009285
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.28369140625  [22432/60000], time: 2.179083519964479
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.5684356689453125  [25632/60000], time: 2.217080007074401
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.5018463134765625  [28832/60000], time: 2.1772618260001764
Model parameters [1] did not change
loss: 0.6444244384765625  [32032/60000], time: 2.1984381050569937
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.471160888671875  [35232/60000], time: 2.163080850034021
Model parameters [1] did not change
loss: 0.3857269287109375  [38432/60000], time: 2.2420931400265545
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.6032867431640625  [41632/60000], time: 2.249616759014316
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.6212921142578125  [44832/60000], time: 2.0942578739486635
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.4937896728515625  [48032/60000], time: 2.21393958106637
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.4273529052734375  [51232/60000], time: 2.2275277810404077
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.4584503173828125  [54432/60000], time: 2.151031623943709
loss: 0.6884918212890625  [57632/60000], time: 2.133742026053369
Model parameters [1] did not change
Model parameters [1] did not change
tensor(2.2827)
batch number: 1/157
tensor(116.2803)
batch number: 51/157
tensor(230.2475)
batch number: 101/157
tensor(344.2109)
batch number: 151/157
Test Error: 
 Accuracy: 0.4592, Avg loss: 2.2794559001922607 

Epoch 0 took 4078.487996773096 seconds
loss: 0.388946533203125  [32/60000], time: 2.114168612053618
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.5297393798828125  [3232/60000], time: 2.2208405788987875
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.2972412109375  [6432/60000], time: 2.156704201013781
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.6674957275390625  [9632/60000], time: 2.181912550004199
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.2772369384765625  [12832/60000], time: 2.1339877180289477
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.646026611328125  [16032/60000], time: 2.1316649400396273
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.34271240234375  [19232/60000], time: 2.171192893991247
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.194366455078125  [22432/60000], time: 2.173334980965592
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.433624267578125  [25632/60000], time: 2.194765100022778
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.4166259765625  [28832/60000], time: 2.073054309003055
Model parameters [1] did not change
loss: 0.514190673828125  [32032/60000], time: 2.124459363054484
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.3812103271484375  [35232/60000], time: 2.1202864419901744
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.3116455078125  [38432/60000], time: 2.1990243550390005
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.4939727783203125  [41632/60000], time: 2.1816287169931456
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.500701904296875  [44832/60000], time: 2.162782556959428
Model parameters [1] did not change
Model parameters [5] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.4416351318359375  [48032/60000], time: 2.183192315045744
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.3127288818359375  [51232/60000], time: 2.147990770987235
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.390899658203125  [54432/60000], time: 2.127433105953969
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.64306640625  [57632/60000], time: 2.190829277038574
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
tensor(2.2793)
batch number: 1/157
tensor(116.1286)
batch number: 51/157
tensor(229.9496)
batch number: 101/157
tensor(343.7637)
batch number: 151/157
Test Error: 
 Accuracy: 0.4779, Avg loss: 2.2764806747436523 

Epoch 1 took 4064.787293766043 seconds
loss: 0.2693023681640625  [32/60000], time: 2.2315383750246838
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.3475494384765625  [3232/60000], time: 2.187792357057333
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.2755279541015625  [6432/60000], time: 2.232694573001936
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.5152740478515625  [9632/60000], time: 2.2426663750084117
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.2893218994140625  [12832/60000], time: 2.119135070941411
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.620635986328125  [16032/60000], time: 2.2534283340210095
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.309051513671875  [19232/60000], time: 2.19362859998364
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.21148681640625  [22432/60000], time: 2.1878751170588657
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.365142822265625  [25632/60000], time: 2.1504195579327643
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [3] did not change
Model parameters [1] did not change
loss: 0.377227783203125  [28832/60000], time: 2.1826695880154148
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.4884185791015625  [32032/60000], time: 2.1738317089620978
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.3130950927734375  [35232/60000], time: 2.1536080490332097
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.3030548095703125  [38432/60000], time: 2.1952976350439712
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.4329986572265625  [41632/60000], time: 2.150755667942576
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.476409912109375  [44832/60000], time: 2.2512210230343044
Model parameters [1] did not change
Model parameters [5] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.4081268310546875  [48032/60000], time: 2.191088603925891
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.2974090576171875  [51232/60000], time: 2.228091527009383
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.327362060546875  [54432/60000], time: 2.2454533559503034
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.59906005859375  [57632/60000], time: 2.2391159150283784
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
tensor(2.2778)
batch number: 1/157
tensor(116.0517)
batch number: 51/157
tensor(229.7935)
batch number: 101/157
tensor(343.5255)
batch number: 151/157
Test Error: 
 Accuracy: 0.4808, Avg loss: 2.27489972114563 

Epoch 2 took 4090.1280679010088 seconds
trial 0 batch size 64
/home/cc/chz_sok_experiments/chz-sok-nn-experiments/nn_venv/lib/python3.8/site-packages/crypten/nn/onnx_converter.py:176: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
  param = torch.from_numpy(numpy_helper.to_array(node))
/home/cc/chz_sok_experiments/chz-sok-nn-experiments/nn_venv/lib/python3.8/site-packages/crypten/nn/onnx_converter.py:176: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
  param = torch.from_numpy(numpy_helper.to_array(node))
ready to train
loss: 2.3171234130859375  [64/60000], time: 2.7630465150577947
loss: 2.30767822265625  [6464/60000], time: 2.7611960920039564
loss: 2.28570556640625  [12864/60000], time: 2.890544476918876
loss: 2.27001953125  [19264/60000], time: 2.7541835110168904
loss: 2.2472076416015625  [25664/60000], time: 2.728443791042082
loss: 2.222991943359375  [32064/60000], time: 2.698234323062934
loss: 2.23052978515625  [38464/60000], time: 2.7348191239871085
loss: 2.20001220703125  [44864/60000], time: 2.713286084937863
loss: 2.19561767578125  [51264/60000], time: 2.7623097710311413
loss: 2.1428070068359375  [57664/60000], time: 2.7484287309926003
tensor(2.1552)
batch number: 1/157
tensor(109.8293)
batch number: 51/157
tensor(217.4850)
batch number: 101/157
tensor(325.2786)
batch number: 151/157
Test Error: 
 Accuracy: 0.3092, Avg loss: 2.154348134994507 

Epoch 0 took 2587.3678469280712 seconds
loss: 2.1723785400390625  [64/60000], time: 2.7667007700074464
loss: 2.1668243408203125  [6464/60000], time: 2.6917206560028717
loss: 2.109710693359375  [12864/60000], time: 2.711865352001041
loss: 2.110382080078125  [19264/60000], time: 2.7622899799607694
loss: 2.053802490234375  [25664/60000], time: 2.7349755230825394
loss: 2.002288818359375  [32064/60000], time: 2.748639337019995
loss: 2.022613525390625  [38464/60000], time: 2.710131034022197
loss: 1.9512939453125  [44864/60000], time: 2.7339506370481104
loss: 1.9533843994140625  [51264/60000], time: 2.7528047310188413
loss: 1.856781005859375  [57664/60000], time: 2.811575354891829
tensor(1.8815)
batch number: 1/157
tensor(95.7966)
batch number: 51/157
tensor(189.5799)
batch number: 101/157
tensor(283.4588)
batch number: 151/157
Test Error: 
 Accuracy: 0.5011, Avg loss: 1.8772456645965576 

Epoch 1 took 2569.206345650018 seconds
loss: 1.9209747314453125  [64/60000], time: 2.7773995000170544
loss: 1.89410400390625  [6464/60000], time: 2.5527756640221924
loss: 1.7839508056640625  [12864/60000], time: 2.779881836962886
loss: 1.80364990234375  [19264/60000], time: 2.7039243769831955
loss: 1.682830810546875  [25664/60000], time: 2.69856195198372
loss: 1.6569366455078125  [32064/60000], time: 2.7553525279508904
loss: 1.665252685546875  [38464/60000], time: 2.765183347975835
loss: 1.581634521484375  [44864/60000], time: 2.744823455926962
loss: 1.60125732421875  [51264/60000], time: 2.7611137570347637
loss: 1.4825439453125  [57664/60000], time: 2.693486840929836
tensor(1.5136)
batch number: 1/157
tensor(77.5765)
batch number: 51/157
tensor(153.4614)
batch number: 101/157
tensor(229.1650)
batch number: 151/157
Test Error: 
 Accuracy: 0.5715, Avg loss: 1.5172710418701172 

Epoch 2 took 2577.8002150290413 seconds
/home/cc/chz_sok_experiments/chz-sok-nn-experiments/nn_venv/lib/python3.8/site-packages/crypten/nn/onnx_converter.py:176: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
  param = torch.from_numpy(numpy_helper.to_array(node))
/home/cc/chz_sok_experiments/chz-sok-nn-experiments/nn_venv/lib/python3.8/site-packages/crypten/nn/onnx_converter.py:176: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
  param = torch.from_numpy(numpy_helper.to_array(node))
ready to train
loss: 9.737625122070312  [64/60000], time: 2.731891040923074
Model parameters [1] did not change
loss: 0.831329345703125  [6464/60000], time: 2.691249912022613
Model parameters [1] did not change
loss: 0.521087646484375  [12864/60000], time: 2.6019619069993496
loss: 0.618927001953125  [19264/60000], time: 2.7795842019841075
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.606292724609375  [25664/60000], time: 2.7884702109731734
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.5068511962890625  [32064/60000], time: 2.792987943976186
Loss was negative
Negative loss: -31100084.0
Output: tensor([[ 1.9658e+00,  3.7799e+00,  4.2548e-01,  9.9610e+00, -2.2087e-01,
         -1.0562e+00,  1.3805e-01, -4.6578e+00, -1.2564e+00, -1.7583e+00],
        [ 3.1166e+00,  8.7045e-01,  3.4082e+00,  3.2365e+00,  2.5579e+00,
         -2.9630e+00,  3.0549e+00, -6.2153e+00, -1.2253e+00, -5.4171e+00],
        [ 1.0645e-01, -3.0642e+00,  2.5827e+00, -7.6071e-01,  2.7835e+00,
         -2.4529e+00,  5.1210e+00, -3.0475e+00,  1.1000e+00, -4.4116e+00],
        [-1.1618e+00, -2.3443e+00,  5.6825e+00, -2.0032e+00,  6.7675e+00,
         -3.2990e+00,  4.8374e+00, -4.5060e+00,  6.4523e-01, -5.5111e+00],
        [ 2.9595e+00,  3.2248e+00,  1.2518e+00,  4.5792e+00, -7.8754e-01,
         -1.8708e+00,  1.6221e+00, -4.1514e+00, -2.3286e+00, -3.7306e+00],
        [-3.4471e+00, -4.1753e+00, -2.3851e+00, -6.3005e+00, -1.9488e+00,
          6.7329e+00, -1.5820e+00,  7.8183e+00,  3.0220e+00,  1.4572e+01],
        [ 1.1011e+00, -1.7644e+00,  1.0186e+01, -9.8247e-01,  4.2365e+00,
         -5.7315e+00,  4.3340e+00, -7.8542e+00, -1.2433e+00, -6.0933e+00],
        [ 5.7975e+00, -1.8416e+00,  2.0264e-02,  2.8870e-02, -1.4816e+00,
         -1.1906e+00,  2.9261e+00, -3.5924e+00, -1.6807e+00, -2.6373e+00],
        [-4.5763e+00, -2.4579e+00, -3.8372e+00, -5.1872e+00, -6.8858e+00,
          4.3011e+00, -4.1594e+00,  9.9870e-01, -2.9216e-01,  1.0615e+01],
        [-1.1715e+00, -3.2312e-01,  6.0053e+00, -1.3261e+00,  6.4553e+00,
         -3.0578e+00,  5.2100e+00, -8.2165e+00, -1.1930e+00, -7.7232e+00],
        [ 2.9327e+00, -5.2995e-01, -2.3163e-01,  3.3299e+00,  2.6865e+00,
         -2.3408e+00,  1.2628e+00, -4.8006e+00,  8.0092e-01, -1.8468e+00],
        [ 1.3593e+00,  2.7145e-01,  8.9116e+00, -3.6553e+00,  6.7024e+00,
         -5.5490e+00,  6.6240e+00, -7.5153e+00,  1.9678e-01, -9.8905e+00],
        [ 3.0091e+00,  1.2165e+01,  1.6112e+00,  4.8374e+00,  1.1882e-01,
         -3.0895e+00,  7.3042e-01, -3.1528e+00, -6.3782e-01, -3.9412e+00],
        [ 5.9215e-01,  9.8103e+00, -5.3979e-01,  1.3118e+00,  1.5402e-01,
         -9.2079e-01, -1.3015e+00, -3.2594e+00, -4.1946e-01, -2.5746e+00],
        [-4.0107e+00, -7.5134e+00,  1.2382e+00, -1.2290e+01, -3.4747e+00,
          5.9342e+00,  1.1540e+00,  4.2006e-01,  1.6758e+00,  2.6328e+00],
        [-4.3784e+00, -6.1894e+00, -3.1586e+00, -7.9998e+00, -6.0126e-01,
          1.3142e+00,  4.0868e-01,  1.3161e+01,  2.6201e+00,  7.7606e-01],
        [ 4.2168e+00,  1.6092e+00, -9.8352e-01,  6.6384e+00,  1.3762e+00,
         -2.5378e+00,  2.3147e+00, -2.2939e+00,  5.0468e-01,  7.9121e-01],
        [ 1.7223e-01, -1.0628e+00,  4.0989e+00, -1.2753e-01,  5.3503e+00,
         -2.8820e+00,  1.4407e+00, -3.7611e+00,  3.5187e-02, -4.0215e+00],
        [-2.7787e+00, -3.1007e+00, -1.8549e+00, -7.9581e+00, -4.7701e+00,
          1.0757e+01, -3.6508e+00,  5.9275e+00,  1.1847e-01,  6.7760e+00],
        [-7.5223e+00, -5.4521e+00,  3.4907e+00, -3.0337e+00,  1.1251e+00,
         -4.8877e-01, -2.1576e+00, -3.9858e+00,  1.0533e+01, -4.6286e-01],
        [-2.1683e+00, -4.2716e+00, -1.2850e+00, -4.4962e+00, -1.4057e+00,
          4.6799e+00, -1.2046e+00,  1.7030e+00,  7.8492e+00,  1.6888e+00],
        [ 1.0637e+00, -3.6250e-01,  6.3210e+00, -3.0778e+00,  3.4979e+00,
         -3.6012e+00,  1.7622e+00, -2.7975e+00, -4.1250e+00, -5.3251e+00],
        [ 2.1941e+00,  1.0063e+01, -6.3217e-01,  2.1442e+00, -2.3521e-01,
         -9.8079e-01,  3.8792e-01, -8.2428e-01, -6.9383e-01, -1.3566e+00],
        [-3.3710e+00, -7.3851e+00, -4.6229e+00, -6.4593e+00, -5.5247e+00,
          4.9507e+00, -6.2207e-01,  7.7078e+00,  3.4992e+00,  1.1169e+01],
        [ 2.6289e+00,  5.8969e+00, -4.0189e-01,  3.8825e+00,  1.1503e+00,
          8.8150e-01,  9.9202e-01, -1.0028e+00, -1.8875e+00, -8.9099e-01],
        [ 2.7368e+00,  8.0542e+00,  2.4380e+00,  4.1767e+00,  1.6983e-01,
         -3.3715e+00,  3.1701e+00, -3.9240e+00, -8.6411e-01, -2.7861e+00],
        [ 2.5794e+00, -1.8102e+00,  7.1880e+00,  1.4363e+00,  3.3855e+00,
         -2.8760e+00,  5.2562e+00, -4.6904e+00,  2.6189e+00, -7.1632e+00],
        [-3.7354e+00, -9.6761e+00, -7.6083e-01, -1.1186e+01, -3.2945e+00,
          1.8492e+01, -5.4977e+00,  6.1903e+00,  4.3189e+00,  6.1240e+00],
        [ 1.0458e+01, -8.8617e-01,  1.2105e+00,  3.6650e+00, -1.8249e+00,
         -4.2387e+00,  4.1208e+00, -7.2571e+00, -1.9733e+00, -2.7624e+00],
        [-1.5479e+00, -4.8011e+00, -6.3005e-01, -7.2704e-01, -1.3199e+00,
          1.3159e+00, -1.6512e+00,  1.7198e+00, -1.0125e+00,  6.1155e+00],
        [-2.3598e+00, -6.2676e+00,  6.5994e-02, -8.9302e+00, -2.3920e+00,
          3.3463e+00, -5.8014e-02,  8.8076e+00,  3.5697e+00, -2.9547e+00],
        [ 3.0928e+00,  1.1297e+01,  2.1269e-01,  2.1314e+00, -1.1960e+00,
         -1.0162e+00,  1.0785e+00, -4.9019e-01, -1.3018e+00, -3.3672e+00],
        [ 3.6228e+08, -2.8663e+08, -5.3861e+07, -1.6934e+08,  1.4225e+08,
         -2.3858e+08, -1.1605e+08,  2.3331e+08,  2.1510e+08, -1.3148e+08],
        [-1.0738e+00, -1.1011e+01,  1.9469e+00, -9.0358e+00,  8.6496e-01,
          1.4511e+01, -2.3618e+00,  6.0350e+00, -1.4444e+00,  1.6055e+00],
        [ 6.8542e-01, -1.0656e+00,  1.8678e+00, -1.4060e+00,  4.5585e+00,
         -1.0217e+00,  1.9287e+00, -1.1842e+00,  1.9076e+00, -4.4692e+00],
        [-1.8466e+00, -6.7640e+00,  1.6278e-01, -5.8631e+00, -3.0798e+00,
          9.6591e+00, -2.8005e+00,  1.3293e+00, -3.8277e-01, -1.3101e+00],
        [-3.5784e+00, -4.8369e+00, -4.6189e+00, -8.9153e+00, -3.7894e+00,
          4.4630e+00, -1.8852e+00,  1.1931e+01,  3.3722e-03,  4.2725e+00],
        [ 1.2092e+00,  5.3356e+00, -2.5900e-01,  1.3739e+01, -1.8750e+00,
          3.1409e+00, -1.2918e+00, -4.6789e+00, -2.1123e+00, -1.2765e+00],
        [ 6.8366e+00,  2.1396e+00,  2.4343e+00,  3.3021e+00,  3.1295e+00,
         -2.7016e+00,  4.8723e+00, -6.4272e+00, -4.5204e-01, -3.1662e+00],
        [-3.4820e+00, -8.9706e+00, -1.9903e+00, -9.9092e+00, -9.4832e-01,
          3.3439e+00,  6.6315e-01,  1.4026e+01,  1.5404e+00, -1.7458e+00],
        [ 1.2734e+00,  9.6635e-01,  1.2670e+00,  5.5732e+00,  6.4449e+00,
         -3.2494e+00,  4.1110e-01, -4.8484e+00, -1.6515e+00, -2.2092e+00],
        [ 1.1100e+01, -8.9034e-01,  1.7294e+00,  1.0729e+00, -2.1785e+00,
         -6.5367e+00,  7.2968e+00, -9.4406e+00, -1.4730e+00, -7.6146e+00],
        [-3.0788e+00, -7.2125e+00, -3.8380e+00, -8.7965e+00, -9.9130e-01,
          6.6736e+00, -4.1816e+00,  8.6307e+00,  3.2294e+00,  3.1261e+00],
        [ 1.6367e-01, -5.6753e+00, -3.8346e+00, -9.6618e+00,  9.6649e-02,
          5.4788e-01,  7.6501e-01,  8.0752e+00,  2.0307e+00,  5.3836e-01],
        [-5.1353e-01, -2.4038e+00,  1.1069e+00,  4.5839e+00,  7.7148e-01,
         -8.1633e-01, -2.0795e-01, -4.0836e-01,  9.1846e+00, -1.5421e-01],
        [ 4.6640e+00,  7.6569e-02,  1.2646e+00,  4.3787e+00,  1.5445e+00,
         -2.3294e+00,  3.1470e+00, -4.1188e+00,  2.6186e-01, -3.9382e+00],
        [ 2.4193e-01, -8.3075e+00, -1.6885e+00, -8.9028e+00, -1.8054e+00,
          1.1380e+01, -1.6958e+00,  1.7531e+00, -1.6872e+00,  1.2425e+00],
        [-1.7958e+00, -4.1883e+00, -2.5974e+00, -8.9114e+00, -3.0303e+00,
          7.3820e+00, -2.0883e-01,  7.0092e+00,  4.2861e+00,  1.0788e+01],
        [ 6.3575e+00,  5.7877e-02,  2.9698e+00,  2.3874e+00, -1.9577e-01,
         -3.9897e+00,  3.9585e+00, -6.0833e+00, -4.9536e-01, -4.4801e+00],
        [ 1.2668e+00,  7.1075e-01,  4.3147e+00,  3.3463e+00,  5.6942e+00,
         -1.2741e+00,  2.3204e+00, -1.9688e+00,  1.9052e-01, -2.1839e+00],
        [ 1.6072e+01, -3.2766e+00,  4.9940e+00,  1.3250e+00, -2.3024e+00,
         -4.7106e+00,  7.7549e+00, -9.9015e+00, -2.2307e+00, -8.1610e+00],
        [ 3.5196e+00,  1.5547e+01, -3.7643e-02,  5.6346e+00, -1.3761e+00,
         -1.4389e+00,  1.0578e+00, -6.2215e-01, -3.1463e+00, -4.4662e+00],
        [ 2.1514e+00, -1.7393e+00,  3.3830e+00, -4.9783e+00,  2.1198e+00,
         -1.1774e+00,  5.5609e+00, -4.4597e+00,  5.7513e+00, -6.9078e+00],
        [-4.7585e+00, -3.8688e+00, -3.0475e+00, -6.0915e+00, -2.6488e+00,
          5.5303e+00, -2.6350e+00,  8.5901e+00,  1.6187e+00,  3.0301e+00],
        [ 4.5008e+00,  4.5340e-01,  4.2386e-01,  5.4265e+00, -3.1102e+00,
         -1.8517e+00,  2.7206e+00, -3.9033e+00, -1.8597e+00,  4.2915e-01],
        [ 7.5556e+00, -4.7853e-01,  1.8067e+00,  5.6710e+00,  1.3542e+00,
         -2.7506e+00,  4.6111e+00, -7.2097e+00, -4.1510e-01, -2.8688e+00],
        [-2.9022e+00, -7.9084e+00, -5.7300e-01, -1.3076e+01,  2.4762e+00,
          4.1133e+00, -1.8169e-01,  1.8636e+00,  1.0686e+01,  2.0361e+00],
        [ 1.0511e+00, -1.2528e+00,  8.2039e+00, -2.1075e+00,  3.3732e+00,
         -4.3108e+00,  4.9536e+00, -8.4317e+00, -1.6441e+00, -4.4700e+00],
        [ 1.7388e+00, -1.0092e+00,  2.4510e+00,  7.5564e+00,  3.6034e+00,
         -1.7055e+00,  1.1465e+00, -4.7310e+00, -2.8994e+00, -3.4942e+00],
        [ 2.8777e+00,  1.1311e+01, -1.9668e+00,  6.1767e+00, -1.1970e+00,
         -3.1139e-01, -5.4573e-01, -3.7159e+00, -2.1709e+00, -1.2245e+00],
        [ 1.7405e+00, -2.2623e+00,  1.1027e+01, -1.1163e+01,  4.9214e+00,
         -3.4063e+00,  6.1463e+00, -8.1809e+00,  6.1988e+00, -9.6073e+00],
        [ 8.4243e+00, -1.9904e+00, -1.2265e+00,  3.3997e+00, -2.3862e+00,
         -3.0344e+00,  6.5552e+00, -6.0187e+00, -1.0948e+00, -4.3125e+00],
        [-2.5045e+00, -1.0446e+01, -1.4662e+00, -1.2783e+01, -5.7704e+00,
          1.7395e+01, -2.6871e+00,  2.0206e+00, -1.8799e+00,  4.8195e-01],
        [ 2.8896e+00, -3.6182e-01,  2.8223e+00,  5.5414e-01,  1.8305e+00,
         -1.9353e+00,  1.5396e+00, -3.7529e+00, -7.1077e-01, -6.0206e+00]])
y_enc: tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],
        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],
        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])
X_enc: tensor([[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],

        [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],

        [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],

        ...,

        [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],

        [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         ...,
         [  0.,  21.,  52.,  ..., 205., 255.,  77.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],

        [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]])
Model parameters [0]: tensor([[ 0.0042,  0.0296, -0.0088,  ...,  0.0222,  0.0138, -0.0239],
        [-0.0266, -0.0091, -0.0351,  ..., -0.0258, -0.0124, -0.0090],
        [ 0.0323, -0.0198,  0.0272,  ..., -0.0220,  0.0260, -0.0106],
        ...,
        [-0.0056, -0.0309, -0.0309,  ...,  0.0260,  0.0018, -0.0319],
        [-0.0315, -0.0148, -0.0101,  ..., -0.0049, -0.0045, -0.0061],
        [-0.0139, -0.0065,  0.0287,  ..., -0.0233,  0.0129,  0.0245]])
Previous Model parameters [0]: tensor([[ 0.0042,  0.0296, -0.0088,  ...,  0.0222,  0.0138, -0.0239],
        [-0.0266, -0.0091, -0.0351,  ..., -0.0258, -0.0124, -0.0090],
        [ 0.0323, -0.0198,  0.0272,  ..., -0.0220,  0.0260, -0.0106],
        ...,
        [-0.0056, -0.0309, -0.0309,  ...,  0.0260,  0.0018, -0.0319],
        [-0.0315, -0.0148, -0.0101,  ..., -0.0049, -0.0045, -0.0061],
        [-0.0139, -0.0065,  0.0287,  ..., -0.0233,  0.0129,  0.0245]])
Previous Previous Model parameters [0]: tensor([[ 0.0042,  0.0296, -0.0088,  ...,  0.0222,  0.0138, -0.0239],
        [-0.0266, -0.0091, -0.0351,  ..., -0.0258, -0.0124, -0.0090],
        [ 0.0323, -0.0198,  0.0272,  ..., -0.0220,  0.0260, -0.0106],
        ...,
        [-0.0056, -0.0309, -0.0309,  ...,  0.0260,  0.0018, -0.0319],
        [-0.0315, -0.0148, -0.0101,  ..., -0.0049, -0.0045, -0.0061],
        [-0.0139, -0.0065,  0.0287,  ..., -0.0233,  0.0129,  0.0245]])
loss: 65929956.0  [38464/60000], time: 2.8094409439945593
loss: -140606352.0  [44864/60000], time: 2.687931193038821
loss: 113876072.0  [51264/60000], time: 2.856324505060911
loss: 130503784.0  [57664/60000], time: 2.792648645932786
tensor(1.5927e+08)
batch number: 1/157
tensor(1.3019e+08)
batch number: 51/157
tensor(-2.6309e+09)
batch number: 101/157
tensor(-4.5033e+09)
batch number: 151/157
Test Error: 
 Accuracy: 0.0989, Avg loss: -29176822.0 

Epoch 0 took 2588.822172913933 seconds
loss: 335784128.0  [64/60000], time: 2.695798235014081
Loss was negative
Negative loss: -365790624.0
Output: tensor([[-2.6250e+09,  8.3452e+08,  1.9453e+08, -9.0903e+08, -2.1350e+09,
         -1.6049e+09, -2.5594e+09, -4.5516e+08,  3.1896e+09,  9.1030e+08],
        [-1.4604e+09,  2.7617e+09,  1.7825e+09,  1.1178e+09, -1.6851e+09,
         -2.3319e+09, -2.8420e+09, -2.2242e+09, -1.8356e+09, -3.6766e+08],
        [-1.6301e+09, -6.8269e+08, -3.7022e+08, -1.8410e+09, -1.0198e+08,
          3.2829e+09, -5.7760e+08, -3.5553e+09, -1.4578e+07, -3.9740e+08],
        [ 4.8980e+08, -2.4876e+08, -1.9084e+09, -2.6966e+08, -4.4967e+08,
         -3.1217e+09, -3.2262e+09,  1.8504e+09,  1.1768e+08, -1.0640e+09],
        [ 1.2090e+08,  1.1653e+09, -2.1005e+08,  1.6534e+08, -2.6207e+09,
         -2.3815e+09,  1.2879e+09, -5.3645e+08,  7.8841e+08, -3.6840e+09],
        [ 3.1047e+08,  7.4989e+08, -1.2134e+08,  4.1228e+08, -2.6891e+09,
         -1.2096e+09,  6.4684e+08,  2.3578e+09,  1.8882e+09,  7.3418e+08],
        [ 2.7901e+08,  6.3939e+08,  2.2226e+09,  5.2290e+08, -3.3307e+09,
          3.3585e+08, -4.7864e+08,  2.7135e+09, -2.1075e+09, -1.3402e+09],
        [ 5.2576e+08, -7.6860e+08, -1.1855e+09, -8.2488e+08,  8.5565e+07,
          2.2089e+09,  3.3555e+08, -1.8638e+09, -1.7758e+09, -1.6338e+09],
        [ 7.7731e+08,  4.9131e+08, -3.8615e+08, -3.2143e+09,  1.9409e+08,
          2.3243e+08, -2.2038e+09,  8.8361e+08, -2.6811e+09, -5.8300e+08],
        [-1.8264e+09, -4.2963e+08,  3.8345e+09, -3.8238e+08,  5.7927e+08,
          1.4640e+09,  9.0321e+08,  2.7902e+09,  1.3054e+08,  2.0296e+09],
        [ 1.7821e+09,  3.7151e+09, -2.0395e+08,  1.4354e+09, -3.0402e+09,
          1.1887e+08,  2.8943e+08,  8.5994e+08,  1.3222e+09, -2.9649e+09],
        [-4.6393e+08,  7.4755e+08,  3.1012e+09,  3.3933e+09,  2.4979e+09,
          1.9295e+09,  8.8645e+08, -5.7720e+08,  1.1330e+08,  2.6316e+09],
        [-4.5519e+08,  1.0510e+09,  5.1000e+08, -4.0637e+08,  2.8794e+07,
         -5.6540e+08, -2.2267e+09, -1.8516e+09, -1.1722e+09, -3.9127e+08],
        [-1.5918e+09,  6.8097e+08, -1.0090e+09,  7.7349e+08,  1.4346e+09,
          1.6987e+09,  1.3327e+09,  1.1336e+09,  1.0790e+09,  1.8926e+09],
        [-5.3337e+08, -1.3921e+09,  6.4941e+08, -1.1616e+09,  2.3054e+09,
         -9.0435e+08, -3.7031e+09,  3.4438e+09,  3.3465e+09, -1.2372e+09],
        [-2.6793e+09, -2.2212e+09,  1.7597e+09,  2.2765e+08, -2.4782e+09,
         -4.5575e+08,  2.4077e+09,  1.2254e+09,  9.5244e+08, -2.8313e+09],
        [-2.9390e+08,  3.3421e+09, -3.1609e+09, -3.3669e+09,  1.1066e+09,
         -2.1494e+09, -9.2839e+08, -2.5236e+09, -3.0265e+09, -2.3527e+09],
        [-6.1369e+08, -3.4746e+09, -2.0899e+09, -9.1047e+08,  3.2458e+08,
          3.4262e+09, -5.8142e+08, -8.3229e+08, -1.9938e+07, -2.1574e+09],
        [ 2.6357e+09, -1.0374e+09, -7.9063e+08, -4.0556e+08,  2.4280e+09,
          1.4464e+09, -2.2534e+09, -5.9396e+08, -1.4974e+08,  2.2376e+09],
        [ 2.9795e+09,  7.0692e+08,  1.7568e+09,  9.9468e+08,  1.3713e+08,
         -2.7162e+08,  1.4440e+09, -1.2220e+09, -4.0004e+09, -1.9375e+09],
        [-1.0368e+08,  1.6275e+09,  5.7856e+08,  3.2377e+09, -1.4409e+09,
          2.4640e+09, -1.0288e+09, -1.6497e+09,  2.3154e+08,  3.4471e+09],
        [ 2.6230e+09,  1.1568e+09, -8.3860e+08, -6.2771e+08,  1.1040e+09,
         -3.2216e+09, -1.5196e+09, -2.2418e+09, -1.0611e+09,  6.8309e+08],
        [-3.7349e+08, -1.1266e+09, -2.6106e+09,  1.7958e+09, -1.2861e+09,
          9.2361e+08,  1.6375e+09,  6.7241e+08,  6.8719e+08, -1.9875e+09],
        [ 1.8290e+09,  2.5105e+09, -3.8322e+09, -7.9453e+08,  1.3959e+09,
         -1.7288e+09, -6.5925e+08, -7.4905e+08, -1.7889e+08,  1.3831e+07],
        [-1.2869e+09,  2.5196e+09,  6.1762e+08, -1.3740e+09,  2.2600e+08,
          2.8629e+09,  3.6192e+09, -1.4811e+09,  3.0809e+08, -2.5945e+09],
        [ 4.9967e+08, -1.5375e+09, -8.9363e+08,  6.3552e+08, -1.1415e+08,
         -6.1281e+08,  5.2397e+08, -3.8620e+08,  1.9554e+09,  1.4481e+09],
        [ 1.2786e+09,  5.6350e+08,  1.4473e+09,  2.4702e+09, -1.5813e+09,
         -5.2549e+08,  5.9746e+08, -1.2196e+09,  8.4861e+08, -4.3240e+08],
        [ 1.3439e+09, -1.8422e+09,  1.5723e+09,  2.1122e+09, -8.9313e+08,
          6.7058e+07, -3.1622e+09,  2.3319e+09, -3.8356e+09, -1.9530e+09],
        [ 9.8673e+08,  3.5191e+09, -3.2559e+09, -3.0394e+09,  1.3016e+09,
         -3.3822e+09,  1.3691e+09,  1.1359e+09,  2.3930e+08, -6.3655e+08],
        [ 2.0332e+09,  2.6222e+09, -2.9814e+09,  7.2987e+08, -2.4069e+09,
          2.3155e+09,  1.1328e+09,  1.0669e+09, -3.3118e+09,  1.3851e+09],
        [-3.2736e+09, -1.2813e+09,  2.6575e+09,  4.6458e+08, -4.8648e+08,
         -1.0975e+09,  2.3867e+09,  8.0102e+08, -2.0151e+09,  5.0453e+08],
        [ 7.9069e+08, -1.2388e+09,  2.2256e+09, -1.8778e+09, -2.1898e+09,
          8.8603e+08,  2.6898e+09,  1.3075e+06, -2.1633e+09,  1.3863e+09],
        [ 3.1149e+09,  2.0594e+09,  5.0865e+07,  3.6085e+08, -1.1241e+09,
          8.0282e+08, -2.2166e+09,  2.3931e+08,  1.7718e+09, -1.8388e+09],
        [ 2.8877e+09,  2.3085e+09,  6.4706e+08, -2.6085e+09,  3.0442e+09,
         -1.8814e+09,  2.1857e+09, -3.8882e+09, -1.8891e+09, -8.2523e+08],
        [-7.5293e+08, -4.1590e+08, -2.2446e+09, -2.5418e+09,  1.2232e+09,
          9.5097e+08, -3.0017e+09, -2.4942e+08,  2.7256e+09,  2.9004e+09],
        [ 3.3747e+09,  3.4884e+09,  3.3104e+09, -5.1648e+08, -7.7223e+08,
         -2.6910e+09,  1.7270e+09,  1.5077e+09, -2.3862e+09, -3.1016e+09],
        [ 1.8863e+09,  2.5404e+09, -8.7593e+08, -3.4706e+09,  4.9363e+08,
          2.1427e+09, -3.9432e+09,  1.4553e+09, -8.5772e+08, -6.7776e+08],
        [ 2.5065e+09, -1.7386e+09,  3.0223e+08, -1.0662e+09, -1.8335e+09,
         -1.5369e+09, -1.6493e+09,  1.3612e+09, -1.3511e+08, -2.9358e+08],
        [-3.3736e+09, -4.1486e+08, -1.3261e+09,  5.5266e+08, -2.9159e+08,
         -8.5511e+08,  2.0098e+09,  2.9083e+09,  1.8856e+09, -4.2126e+08],
        [-3.0790e+09,  4.8038e+08,  7.5630e+08, -4.7565e+08, -1.9611e+09,
          3.1733e+09,  1.8910e+09, -2.6589e+09, -2.1270e+09, -1.8160e+09],
        [ 2.4180e+09,  5.2196e+08,  1.5969e+09,  2.1063e+09, -2.2685e+09,
         -1.0926e+08,  2.1745e+09,  1.2531e+09,  1.1031e+09,  5.2785e+08],
        [-3.4261e+08, -1.3510e+09,  1.1211e+09,  1.2884e+08,  2.2224e+09,
         -5.9609e+08,  1.2047e+09,  7.8104e+08, -3.0559e+09,  2.2791e+06],
        [-1.1730e+09, -3.1727e+08, -3.9919e+09, -1.5488e+09,  1.5718e+09,
          1.7137e+09,  4.3504e+08,  1.2397e+09, -5.4596e+08,  1.8369e+08],
        [ 3.3191e+08,  1.5638e+09, -5.1814e+08,  3.5867e+09, -3.8808e+08,
          3.7564e+09, -1.8758e+09, -8.3456e+08, -1.8810e+09,  2.1534e+08],
        [-1.1216e+09,  2.3619e+08,  7.6444e+08,  1.9729e+09, -2.7316e+09,
         -3.1274e+09,  2.0279e+09,  4.2705e+08, -4.1326e+08, -4.4443e+08],
        [ 3.1477e+09, -3.0918e+09, -2.9546e+09, -1.5243e+09,  4.7271e+08,
          2.8264e+09,  2.2656e+09, -1.1839e+09, -2.4093e+09, -1.2615e+09],
        [-2.2690e+09,  8.6160e+08,  4.1187e+08, -1.0010e+09, -3.6818e+09,
          5.4340e+08, -1.5497e+09,  3.4250e+08,  2.9842e+09, -5.2272e+08],
        [-1.8752e+09, -1.3409e+09,  2.4609e+09, -1.2653e+09,  1.5951e+09,
          1.6300e+09, -3.5024e+09, -1.4755e+09, -1.6397e+09, -1.8140e+09],
        [-7.0664e+08,  8.2455e+08, -3.5055e+09,  2.6990e+06, -1.1625e+08,
         -2.9078e+09,  7.1633e+07, -2.0700e+08,  1.0923e+08,  9.0448e+08],
        [-3.5416e+08,  9.3677e+08,  8.6093e+08, -3.2216e+09, -8.3762e+08,
          1.5795e+09, -3.7222e+08, -1.3317e+09,  1.1258e+09,  2.7776e+08],
        [-1.9761e+09,  2.4240e+09,  9.0793e+08, -2.4607e+09, -3.8893e+09,
         -2.1459e+08,  1.8207e+08, -2.2574e+09,  1.0792e+09, -2.0130e+09],
        [-2.1357e+09,  7.5561e+08,  2.3398e+09, -5.8799e+08, -1.4910e+08,
          1.6078e+09, -1.4938e+09, -1.3122e+09, -3.2328e+09, -2.2960e+08],
        [-9.1601e+08,  4.4248e+08,  4.7322e+07,  1.5103e+08, -3.0461e+09,
         -2.0007e+09, -7.6526e+08, -3.0208e+08, -1.3182e+09, -3.8694e+09],
        [ 2.4422e+09, -1.6560e+09,  1.4066e+09,  3.3960e+08,  2.1146e+09,
         -2.6385e+09,  3.3453e+09,  1.5719e+09,  7.5619e+08, -4.3571e+08],
        [-1.4718e+09, -8.9876e+08,  1.2827e+09, -1.7240e+09,  3.3158e+09,
         -1.4785e+09, -1.6297e+09, -3.6033e+09,  5.3719e+08,  2.3083e+09],
        [ 2.7685e+09,  5.9997e+08,  1.2149e+09, -1.9790e+09,  2.1094e+09,
         -3.8770e+08, -4.9042e+08, -1.0570e+09,  1.6754e+09, -2.3323e+09],
        [ 2.0486e+09,  5.0867e+08,  2.0545e+09, -2.0516e+09, -3.1416e+09,
         -8.9486e+08,  1.1894e+09,  2.7133e+09, -2.0673e+09, -1.7280e+09],
        [ 1.1775e+09, -6.4053e+08, -2.9764e+09,  2.1566e+09, -2.0836e+09,
         -1.7578e+08,  2.4210e+08, -2.2484e+09,  4.0535e+08,  2.9678e+09],
        [-1.5786e+09,  2.0427e+09,  6.2596e+08, -2.0636e+09,  7.5832e+08,
          3.7728e+09, -7.1054e+08,  1.8260e+09,  3.1392e+09, -1.5072e+09],
        [-2.0145e+09, -1.6234e+09,  2.8596e+09, -1.6402e+09, -3.4748e+09,
         -2.8934e+09,  1.6613e+09, -9.3635e+08, -2.4147e+08, -7.5668e+08],
        [-1.8046e+09,  2.3954e+09,  1.8267e+09,  1.6385e+09, -3.5509e+09,
          1.3926e+09,  8.9943e+08, -3.4858e+08,  9.2926e+08, -8.7025e+06],
        [-2.1049e+08, -5.0854e+08,  1.9814e+09, -2.8311e+09,  7.8150e+08,
          7.6231e+08,  1.5648e+09, -2.2675e+09, -1.5513e+09,  3.0953e+09],
        [ 5.7701e+08, -2.4946e+09,  2.8045e+09, -2.3454e+09,  2.2159e+09,
          3.9352e+08,  5.5089e+08, -1.7479e+09, -6.0382e+08, -2.0814e+09],
        [-2.4282e+09, -9.4667e+08,  3.2421e+09, -1.5898e+09,  9.4384e+08,
          3.2083e+08,  4.3573e+08, -1.1554e+09,  3.8755e+08,  3.1393e+09]])
y_enc: tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],
        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])
X_enc: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 1., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]])
Model parameters [0]: tensor([[-1.0319e+06,  4.6693e+06,  1.7207e+07,  ..., -1.8152e+07,
         -3.3037e+07,  3.3437e+06],
        [-7.8507e+05, -1.7631e+06, -4.6922e+06,  ..., -1.3591e+07,
         -2.9912e+07,  1.1394e+07],
        [-2.8780e+06, -1.1216e+07, -3.5600e+06,  ...,  1.1988e+07,
         -1.6067e+06, -2.6486e+05],
        ...,
        [ 4.4910e+06, -2.6998e+06, -2.4794e+07,  ...,  4.2600e+09,
         -2.4252e+07,  7.3784e+06],
        [ 2.3938e+06, -1.2199e+07, -2.8357e+06,  ...,  2.2508e+07,
         -3.1621e+06, -4.1794e+06],
        [ 3.4963e+06, -8.1668e+04, -7.0550e+06,  ...,  1.1586e+07,
         -1.5118e+06, -1.1201e+06]])
Previous Model parameters [0]: tensor([[-1.0319e+06,  4.6693e+06,  1.7207e+07,  ..., -1.8152e+07,
         -3.3037e+07,  3.3437e+06],
        [-7.8507e+05, -1.7631e+06, -4.6922e+06,  ..., -1.6529e+07,
         -2.9912e+07,  1.1394e+07],
        [-2.8780e+06, -1.1216e+07, -3.5600e+06,  ...,  1.4495e+07,
         -1.6067e+06, -2.6486e+05],
        ...,
        [ 4.4910e+06, -2.6998e+06, -2.4794e+07,  ...,  4.2600e+09,
         -2.4252e+07,  7.3784e+06],
        [ 2.3938e+06, -1.2199e+07, -2.8357e+06,  ...,  2.2508e+07,
         -3.1621e+06, -4.1794e+06],
        [ 3.4963e+06, -8.1668e+04, -6.1193e+06,  ...,  1.1586e+07,
         -1.5118e+06, -1.1201e+06]])
Previous Previous Model parameters [0]: None
Model parameters [5] did not change
loss: -164993984.0  [6464/60000], time: 2.708829578012228
Model parameters [5] did not change
loss: 497861888.0  [12864/60000], time: 2.7344913879642263
loss: 111977736.0  [19264/60000], time: 2.5990091380663216
loss: 247438784.0  [25664/60000], time: 2.811533082043752
loss: 51458488.0  [32064/60000], time: 2.7306749530835077
loss: -225043328.0  [38464/60000], time: 2.8027381929568946
loss: 69089744.0  [44864/60000], time: 2.7307406769832596
Model parameters [5] did not change
loss: -249844048.0  [51264/60000], time: 2.6865028030006215
loss: -32400762.0  [57664/60000], time: 2.78834198100958
tensor(23478472.)
batch number: 1/157
tensor(31025610.)
batch number: 51/157
tensor(1.1776e+09)
batch number: 101/157
tensor(2.7170e+09)
batch number: 151/157
Test Error: 
 Accuracy: 0.0997, Avg loss: 21832718.0 

Epoch 1 took 2578.5220327370334 seconds
loss: 278562176.0  [64/60000], time: 2.7602298009442165
Loss was negative
Negative loss: -545714240.0
Output: tensor([[-2.6368e+09,  2.0110e+09, -9.4191e+08, -1.8156e+09,  2.4309e+09,
         -1.1712e+08,  5.3648e+08, -2.6033e+09,  1.0647e+09, -2.7879e+09],
        [ 1.3928e+09,  1.9207e+09, -2.6598e+09, -1.2419e+09,  4.4172e+09,
          9.3752e+07, -1.0427e+09,  7.5286e+08, -2.6636e+07,  8.1696e+08],
        [-3.3112e+09, -8.3739e+08,  1.4218e+09, -1.7648e+09,  9.4107e+08,
         -1.9310e+09, -4.8880e+08,  1.1072e+09,  5.3593e+08,  2.3636e+09],
        [-1.1355e+09,  8.2719e+08,  1.8453e+09, -2.9077e+09,  6.3531e+09,
          7.3003e+08, -3.0654e+09,  3.0923e+09, -2.5074e+09,  2.6854e+09],
        [-2.0835e+09,  2.3378e+09,  1.0972e+08, -1.4310e+09,  4.4141e+09,
         -1.7587e+09, -1.6392e+08,  4.4671e+08, -8.5575e+08, -1.7433e+08],
        [ 1.0707e+09, -2.1591e+09, -3.0856e+09,  2.7788e+09,  2.0890e+09,
         -1.0574e+09, -1.8340e+09,  3.6812e+09,  1.7730e+09, -2.1876e+09],
        [ 1.4148e+09, -2.5073e+09,  1.3786e+09,  6.7362e+08,  4.8347e+09,
          6.0030e+08,  1.7609e+09, -4.0092e+09,  5.2628e+08, -1.7986e+09],
        [-1.9136e+09,  1.4060e+09, -2.2186e+09,  2.7603e+08,  4.0066e+09,
          7.7321e+08, -3.0272e+09,  5.4396e+08,  4.0812e+08,  7.3003e+08],
        [-3.0740e+09, -2.6098e+09,  3.0300e+08,  2.6105e+08,  5.7421e+09,
         -1.3750e+08,  7.5978e+08,  1.8659e+09,  2.4785e+09,  1.5751e+09],
        [ 8.8779e+08,  2.9842e+09, -2.6335e+09, -2.4605e+09,  8.0558e+09,
         -1.6980e+09,  1.6047e+08,  2.7635e+09,  1.8193e+08,  6.3991e+08],
        [ 9.4228e+08, -2.8534e+09, -1.1891e+09,  1.4249e+09,  2.9123e+09,
          1.6259e+09, -1.4820e+09, -2.7090e+09,  2.6964e+09, -3.1296e+09],
        [-6.2940e+08,  2.0770e+09,  2.4267e+08, -9.0811e+08,  3.4834e+09,
          4.0506e+08,  6.3987e+08, -2.6096e+09, -1.7242e+09, -9.0696e+08],
        [ 1.6484e+09, -1.0208e+09, -1.0922e+09,  5.4510e+07,  4.0501e+09,
          4.0801e+08, -2.1412e+09, -2.8105e+09,  2.8021e+09, -8.4376e+08],
        [ 1.6274e+09, -6.5692e+08, -4.9947e+08,  3.7700e+09,  2.7754e+09,
          7.2842e+08,  2.8578e+09, -3.5022e+08, -9.0221e+08, -2.8339e+09],
        [ 2.1461e+09, -6.2347e+08, -3.9804e+08,  2.1354e+09,  6.2914e+09,
         -1.8849e+09, -2.5791e+09, -2.6562e+09, -6.6675e+08,  5.3740e+08],
        [ 9.7118e+07,  3.0451e+09,  2.5101e+09,  3.6692e+08,  2.4297e+09,
         -3.5210e+08, -6.3677e+08,  4.6352e+08,  3.4106e+08, -2.3640e+08],
        [-1.3960e+09, -1.4328e+07,  4.4341e+08,  3.9312e+09,  3.8787e+09,
          7.0931e+08,  4.8816e+08, -1.3129e+09,  1.7295e+09, -1.1680e+09],
        [-8.7709e+07, -7.1907e+08, -2.3788e+09, -1.4496e+09,  6.2554e+09,
          2.7187e+09,  1.1196e+09, -2.2014e+07, -2.6576e+09, -3.9282e+09],
        [ 1.4682e+09, -1.2231e+09, -3.0997e+09,  1.1308e+08,  6.1354e+09,
          2.9258e+09, -3.7541e+09, -3.9002e+08, -1.2587e+09, -3.3586e+09],
        [-5.6500e+08,  1.2018e+09, -4.5344e+08,  2.3006e+08,  5.0841e+09,
         -1.6149e+09,  2.4432e+09, -1.6047e+09, -9.7129e+08, -2.5911e+08],
        [ 3.3157e+08, -6.9509e+08, -4.2096e+09,  1.3088e+08,  5.5774e+09,
          1.8881e+09, -2.2268e+09,  8.2813e+07, -2.8277e+08,  3.4954e+08],
        [-1.7700e+09, -3.3412e+09, -2.1196e+09,  2.8758e+08,  3.1125e+09,
          1.0793e+08,  1.1568e+09, -3.5878e+09,  3.6143e+07,  7.7801e+08],
        [ 1.5521e+09,  1.7685e+09,  3.1586e+09, -2.1589e+08,  1.5606e+09,
          1.0529e+09,  2.4434e+09,  7.2146e+08,  6.4553e+08, -5.3018e+08],
        [-2.1724e+09, -2.3360e+09, -1.8955e+09,  2.2142e+09,  3.8687e+09,
         -2.4760e+09,  2.3885e+08, -5.9995e+08,  1.2709e+09,  1.3029e+08],
        [-1.3740e+09, -7.6517e+08,  1.9015e+09, -1.4102e+09,  7.1026e+09,
          1.5307e+09,  1.1903e+09,  1.5066e+09,  2.6966e+09, -1.7387e+09],
        [-1.8259e+09,  1.0611e+09,  9.0945e+08, -3.0714e+08,  5.8952e+09,
          7.1232e+07,  7.2816e+08,  5.2795e+08, -9.9085e+07, -1.7931e+09],
        [ 2.0383e+09, -3.1680e+08,  2.3545e+09,  1.2573e+08,  2.7957e+09,
         -1.9889e+08, -4.4970e+08, -1.0822e+09, -5.8052e+07, -1.1032e+08],
        [ 5.3320e+08,  2.9519e+09, -1.8863e+08, -1.1321e+09,  3.5047e+09,
         -9.6367e+08, -1.1805e+08, -1.2020e+09, -7.8343e+08, -2.8156e+09],
        [ 2.4256e+09, -8.8028e+08, -1.5957e+08, -1.8111e+09,  1.4594e+09,
          3.1481e+08,  1.0382e+09,  1.6194e+09, -3.0721e+09,  2.1658e+09],
        [ 9.1983e+08, -2.0986e+08,  1.2138e+09,  1.0908e+08,  2.2613e+09,
         -1.4836e+09,  2.7621e+08,  8.3362e+08,  8.9101e+08, -3.1613e+09],
        [-4.3308e+08, -1.0015e+09,  9.9197e+08,  4.8019e+08,  3.7819e+09,
          3.1907e+08,  2.5483e+09,  2.7600e+09,  3.1614e+09, -8.7308e+08],
        [ 6.2871e+08,  2.7557e+07,  1.0469e+09, -5.4600e+08,  1.8906e+09,
          2.5368e+09,  3.2342e+09,  9.4537e+07, -2.4922e+09, -1.9699e+09],
        [ 5.4082e+08, -1.9501e+09, -2.1668e+09, -1.3835e+09,  5.2499e+09,
         -1.4909e+09,  1.9585e+08, -3.2491e+08,  1.5729e+09,  8.9047e+08],
        [ 2.2665e+09,  2.6271e+09,  1.0745e+09, -1.1319e+09,  3.8217e+09,
          1.1109e+09, -8.9598e+08,  1.3218e+09, -1.8781e+09, -3.1498e+09],
        [-1.9281e+09,  2.7005e+08, -1.0989e+09, -3.3335e+09,  6.2018e+09,
          2.0675e+08, -1.7986e+09, -1.4767e+09,  2.3887e+09,  1.3634e+09],
        [ 4.1366e+08,  2.1402e+09, -2.3085e+09,  5.6868e+08,  5.1747e+09,
         -3.5063e+09, -2.7013e+09,  1.0778e+09,  2.1781e+07,  1.7573e+09],
        [ 6.1201e+08,  2.1002e+09, -1.0508e+09,  2.2342e+09,  3.0423e+09,
         -1.5326e+09, -2.3997e+09,  2.1321e+09, -1.4148e+09,  1.3845e+09],
        [ 1.3638e+09,  1.5972e+09,  2.9440e+09,  8.7087e+08,  6.8849e+09,
          9.1034e+08, -1.2608e+09, -4.0863e+08,  6.9268e+08, -2.3616e+08],
        [-8.2950e+08, -1.3625e+09, -5.2498e+08,  2.4966e+08,  6.9093e+09,
          6.0526e+08,  1.3325e+09,  3.0034e+09,  2.3776e+09,  8.1776e+07],
        [ 1.2510e+08,  2.2640e+09, -1.2982e+09,  1.8245e+09,  5.5941e+09,
          2.8717e+09, -2.7092e+08,  8.9789e+08,  2.1868e+09,  4.0432e+09],
        [-2.9018e+09,  3.2852e+09,  7.3873e+08,  3.4920e+09,  5.0188e+09,
         -3.6248e+08,  2.2373e+09, -1.9988e+09,  3.0131e+08,  4.4749e+08],
        [ 2.8935e+08,  2.0584e+09,  2.1664e+09, -1.3200e+09,  2.9068e+09,
          1.8926e+09,  1.0647e+09, -3.9630e+09, -3.1730e+08, -5.2552e+08],
        [ 2.2032e+09, -1.3410e+09, -3.4162e+09, -2.8346e+08,  1.4366e+09,
         -7.4787e+08,  1.1400e+09, -3.7915e+09, -8.5912e+08,  3.0357e+08],
        [ 1.6985e+09,  1.7551e+09, -1.8518e+08, -1.9517e+08,  5.4916e+09,
         -2.8024e+08, -8.7164e+08,  5.5433e+08,  7.7399e+08,  2.8728e+09],
        [-1.8171e+09,  6.9147e+08, -3.4913e+08, -1.2277e+09,  4.2384e+09,
         -4.1306e+08,  1.4138e+09,  2.9589e+09, -2.0432e+09, -2.6695e+09],
        [ 1.7200e+09,  3.5560e+09,  1.8633e+09,  7.7634e+08,  6.0546e+09,
         -1.3069e+09, -2.8407e+08,  1.9650e+09, -1.3875e+09,  5.1829e+08],
        [-2.5925e+09,  1.1088e+09,  2.2659e+09, -2.2144e+09,  5.7988e+09,
         -1.8390e+09, -2.1992e+09, -4.8202e+08,  1.3555e+09,  1.2582e+09],
        [-2.7661e+09,  2.6409e+08, -5.6239e+08,  2.0320e+09,  2.6963e+09,
          2.3105e+09, -6.5235e+08,  2.6834e+09, -3.6760e+08,  5.3406e+08],
        [ 2.1222e+09, -9.4029e+08,  5.7211e+08, -2.3758e+09,  2.4414e+09,
          2.2943e+09, -1.7346e+09,  1.1457e+09, -9.3327e+08,  1.5015e+09],
        [-1.9251e+09, -1.6511e+09,  1.1995e+08,  1.3587e+09,  5.7605e+09,
         -1.3853e+09,  4.2815e+08, -6.5946e+08, -1.1575e+09,  3.4653e+08],
        [-3.6701e+09,  2.7803e+09,  1.3901e+08, -1.1561e+09,  3.4158e+09,
          5.4218e+08, -3.1621e+08, -1.1966e+09, -3.1443e+08,  2.4450e+09],
        [ 3.1346e+07,  1.5695e+09, -4.9651e+08,  2.5921e+09,  3.0566e+09,
         -5.2681e+08, -2.5597e+08,  5.0337e+08,  2.6905e+09,  1.2973e+09],
        [-1.1794e+09,  1.8391e+09, -1.3769e+09,  1.5551e+09,  6.6175e+09,
         -2.2687e+09,  1.0650e+09,  1.0875e+09,  1.8935e+09,  3.0378e+09],
        [-2.8201e+09, -6.9334e+08, -1.3860e+09, -2.6954e+09,  6.9176e+09,
         -2.5329e+09, -2.6144e+09,  4.0128e+09,  5.6933e+08,  8.7573e+08],
        [-5.0567e+08, -3.6515e+07,  1.9995e+09, -2.5911e+09,  4.1970e+09,
         -3.1086e+09, -1.5832e+09,  4.6704e+08,  5.4305e+08,  1.1124e+09],
        [-1.4831e+09,  7.9571e+07,  1.3808e+09,  5.3370e+07,  6.0491e+09,
         -2.4591e+09, -1.1398e+09,  2.6739e+09,  1.7805e+09, -1.0933e+09],
        [ 2.6916e+09,  2.0969e+09,  2.9299e+08,  5.1321e+08,  7.3762e+09,
          2.0712e+09, -1.3106e+09,  1.4371e+08, -2.7485e+09,  1.5804e+09],
        [ 1.6992e+09,  7.4921e+08,  1.2879e+09, -1.5170e+09,  1.0582e+09,
          2.6911e+09, -3.7633e+08, -1.0056e+09,  4.0215e+08,  1.5233e+09],
        [ 1.3210e+09, -2.7767e+09,  4.0887e+08, -2.0227e+09,  7.3912e+09,
         -8.4162e+08, -1.9199e+08, -1.7158e+09,  1.9651e+08, -1.3045e+09],
        [ 1.9056e+09,  1.4800e+09,  1.3933e+09,  2.3223e+08,  3.7893e+09,
          2.7502e+09,  5.4123e+08, -1.5166e+09, -1.6455e+09, -2.0132e+09],
        [-1.5055e+09,  8.9157e+08, -2.3864e+09, -3.0299e+09,  5.8061e+09,
         -1.6108e+09, -1.3891e+09,  1.1261e+09,  3.5744e+09, -9.9721e+08],
        [ 2.9271e+09,  2.2211e+08, -2.9219e+09,  3.0821e+09,  3.0278e+09,
          2.2528e+09,  1.2881e+09, -2.8294e+08,  2.3084e+09,  2.7925e+09],
        [ 2.6307e+09,  2.1104e+09,  2.6835e+09,  2.0150e+08,  5.2850e+09,
         -3.6302e+08,  3.9597e+09, -1.6562e+09, -7.6232e+08,  7.1190e+07],
        [-2.2628e+09,  2.5649e+09, -1.3058e+09, -3.6587e+09,  4.5373e+09,
         -1.7380e+09,  1.3484e+09,  5.4740e+07,  1.2995e+09,  5.4900e+08]])
y_enc: tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],
        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])
X_enc: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]])
Model parameters [0]: tensor([[-9.5806e+05,  1.4101e+07,  4.3371e+07,  ...,  4.2401e+09,
         -2.3992e+07, -1.6152e+06],
        [ 1.9236e+06,  1.1447e+07, -1.2559e+06,  ..., -2.0791e+07,
          4.2488e+09,  3.0537e+06],
        [-1.2118e+06, -1.3579e+07, -3.6106e+07,  ...,  2.6770e+07,
          1.4977e+07, -2.7597e+07],
        ...,
        [ 1.8780e+06, -1.6987e+06,  2.3975e+07,  ...,  4.2762e+09,
         -1.8723e+07,  1.5232e+06],
        [ 8.1520e+05, -1.4872e+07,  1.2969e+07,  ...,  2.9753e+07,
          1.8165e+07, -1.9021e+06],
        [ 3.8671e+06,  7.8552e+06, -1.2581e+07,  ..., -6.2899e+07,
          4.6447e+07, -4.3067e+09]])
Previous Model parameters [0]: tensor([[-9.5806e+05,  1.4101e+07,  4.2371e+07,  ...,  4.2432e+09,
         -2.3992e+07, -1.6152e+06],
        [ 1.9236e+06,  1.1447e+07, -1.2559e+06,  ..., -2.0791e+07,
          4.2488e+09,  3.0537e+06],
        [-1.2118e+06, -1.3579e+07, -3.6106e+07,  ...,  2.6770e+07,
          1.4977e+07, -2.7597e+07],
        ...,
        [ 1.8780e+06, -1.6987e+06,  2.3215e+07,  ...,  4.2772e+09,
         -1.8723e+07,  1.5232e+06],
        [ 8.1520e+05, -1.4872e+07,  1.2969e+07,  ...,  2.9753e+07,
          1.8165e+07, -1.9021e+06],
        [ 3.8671e+06,  7.8552e+06, -9.6668e+06,  ..., -6.2899e+07,
          4.6447e+07, -4.3067e+09]])
Previous Previous Model parameters [0]: tensor([[-9.5806e+05,  1.4728e+07,  4.2998e+07,  ...,  4.2432e+09,
         -2.4949e+07, -2.3090e+06],
        [ 1.9236e+06,  1.1447e+07, -6.9458e+05,  ..., -2.0533e+07,
          4.2464e+09,  3.0537e+06],
        [-1.2118e+06, -1.3579e+07, -3.6106e+07,  ...,  2.9964e+07,
          1.5823e+07, -3.0371e+07],
        ...,
        [ 1.8780e+06, -1.6987e+06,  2.3135e+07,  ...,  4.2765e+09,
         -2.1871e+07,  1.5232e+06],
        [ 8.1520e+05, -1.4053e+07,  1.1696e+07,  ...,  2.9753e+07,
          1.7539e+07, -3.3916e+06],
        [ 3.8671e+06,  5.4590e+06, -9.6668e+06,  ..., -6.1456e+07,
          4.6447e+07, -4.3061e+09]])
loss: -31239296.0  [6464/60000], time: 2.6946662219706923
loss: 30895394.0  [12864/60000], time: 2.7391804850194603
loss: 223941968.0  [19264/60000], time: 2.686482335906476
loss: -251170992.0  [25664/60000], time: 2.827401126967743
Model parameters [5] did not change
loss: -250022992.0  [32064/60000], time: 2.83504792698659
loss: -201435024.0  [38464/60000], time: 2.7866575769148767
loss: -108801584.0  [44864/60000], time: 2.7446328949881718
loss: 348171840.0  [51264/60000], time: 2.6780523139750585
loss: -390437792.0  [57664/60000], time: 2.6902679179329425
tensor(1.2400e+08)
batch number: 1/157
tensor(-3.0375e+09)
batch number: 51/157
tensor(-2.6178e+09)
batch number: 101/157
tensor(3.9150e+08)
batch number: 151/157
Test Error: 
 Accuracy: 0.0954, Avg loss: 1284238.625 

Epoch 2 took 2579.1856703020167 seconds
trial 0 batch size 128
/home/cc/chz_sok_experiments/chz-sok-nn-experiments/nn_venv/lib/python3.8/site-packages/crypten/nn/onnx_converter.py:176: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
  param = torch.from_numpy(numpy_helper.to_array(node))
/home/cc/chz_sok_experiments/chz-sok-nn-experiments/nn_venv/lib/python3.8/site-packages/crypten/nn/onnx_converter.py:176: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
  param = torch.from_numpy(numpy_helper.to_array(node))
ready to train
loss: 2.3214263916015625  [128/60000], time: 3.892635316005908
loss: 2.300048828125  [12928/60000], time: 3.9870002371026203
loss: 2.2865753173828125  [25728/60000], time: 3.977890374022536
loss: 2.2748260498046875  [38528/60000], time: 3.9255446870811284
loss: 2.258026123046875  [51328/60000], time: 3.8396293700207025
tensor(2.2469)
batch number: 1/157
tensor(114.6667)
batch number: 51/157
tensor(227.0292)
batch number: 101/157
tensor(339.5052)
batch number: 151/157
Test Error: 
 Accuracy: 0.242, Avg loss: 2.2485852241516113 

Epoch 0 took 1828.7724821410375 seconds
loss: 2.25372314453125  [128/60000], time: 3.8505772090284154
loss: 2.2250213623046875  [12928/60000], time: 3.819586016004905
loss: 2.2131805419921875  [25728/60000], time: 3.929269435000606
loss: 2.2050933837890625  [38528/60000], time: 3.915897617000155
loss: 2.182861328125  [51328/60000], time: 3.8524415479041636
tensor(2.1694)
batch number: 1/157
tensor(110.6161)
batch number: 51/157
tensor(218.9662)
batch number: 101/157
tensor(327.4979)
batch number: 151/157
Test Error: 
 Accuracy: 0.3628, Avg loss: 2.1690821647644043 

Epoch 1 took 1824.9449436999857 seconds
loss: 2.1739501953125  [128/60000], time: 3.903740516048856
loss: 2.13519287109375  [12928/60000], time: 3.8328425989020616
loss: 2.118316650390625  [25728/60000], time: 3.9439817450474948
loss: 2.1093597412109375  [38528/60000], time: 3.944787437096238
loss: 2.0801544189453125  [51328/60000], time: 3.9408023230498657
tensor(2.0624)
batch number: 1/157
tensor(105.0190)
batch number: 51/157
tensor(207.8047)
batch number: 101/157
tensor(310.8069)
batch number: 151/157
Test Error: 
 Accuracy: 0.4926, Avg loss: 2.0585274696350098 

Epoch 2 took 1838.3174219219945 seconds
/home/cc/chz_sok_experiments/chz-sok-nn-experiments/nn_venv/lib/python3.8/site-packages/crypten/nn/onnx_converter.py:176: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
  param = torch.from_numpy(numpy_helper.to_array(node))
/home/cc/chz_sok_experiments/chz-sok-nn-experiments/nn_venv/lib/python3.8/site-packages/crypten/nn/onnx_converter.py:176: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
  param = torch.from_numpy(numpy_helper.to_array(node))
ready to train
loss: 9.983352661132812  [128/60000], time: 3.935890321037732
loss: 0.668792724609375  [12928/60000], time: 3.802087304997258
Model parameters [1] did not change
loss: 0.5241546630859375  [25728/60000], time: 3.8686136489268392
Model parameters [1] did not change
loss: 0.5213470458984375  [38528/60000], time: 3.887720772996545
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.68408203125  [51328/60000], time: 3.895590908941813
Model parameters [1] did not change
Model parameters [1] did not change
tensor(2.2767)
batch number: 1/157
tensor(116.2099)
batch number: 51/157
tensor(230.1493)
batch number: 101/157
tensor(344.0728)
batch number: 151/157
Test Error: 
 Accuracy: 0.2541, Avg loss: 2.2786548137664795 

Epoch 0 took 1801.970059053041 seconds
loss: 0.4462890625  [128/60000], time: 3.788584814989008
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.362884521484375  [12928/60000], time: 3.8633807679871097
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.3928375244140625  [25728/60000], time: 3.893249429995194
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.415069580078125  [38528/60000], time: 3.8720355530967936
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.5448455810546875  [51328/60000], time: 3.866404063999653
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
tensor(2.2741)
batch number: 1/157
tensor(116.0946)
batch number: 51/157
tensor(229.9210)
batch number: 101/157
tensor(343.7285)
batch number: 151/157
Test Error: 
 Accuracy: 0.2781, Avg loss: 2.276366710662842 

Epoch 1 took 1807.5385993249947 seconds
loss: 0.38909912109375  [128/60000], time: 3.872271904023364
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.3120574951171875  [12928/60000], time: 3.89003423799295
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.3535308837890625  [25728/60000], time: 3.868771467008628
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.3753662109375  [38528/60000], time: 3.8717420210596174
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.4710845947265625  [51328/60000], time: 3.8434043639572337
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Loss was negative
Negative loss: -216649232.0
Output: tensor([[-2.0248e+09,  7.4940e+08, -1.5886e+09,  ..., -1.0636e+09,
          6.3957e+08, -5.6106e+08],
        [-1.0551e+09, -4.6438e+08,  4.3907e+08,  ..., -9.2446e+08,
          2.2518e+09, -1.1977e+08],
        [ 2.7263e+09,  1.7506e+09,  6.0806e+07,  ..., -4.3077e+08,
         -1.8436e+09,  1.6818e+09],
        ...,
        [-2.1116e+09, -4.7181e+08,  1.3549e+09,  ...,  2.9435e+09,
         -1.1884e+09, -1.3830e+09],
        [ 3.6478e+09, -1.1386e+09,  2.0555e+08,  ..., -3.1376e+09,
         -1.1712e+09,  5.2221e+08],
        [ 1.0875e+09,  1.3088e+08, -1.4706e+09,  ...,  6.7754e+08,
          5.4533e+08,  1.5707e+09]])
y_enc: tensor([[0., 0., 0.,  ..., 0., 1., 0.],
        [0., 0., 1.,  ..., 0., 0., 0.],
        [1., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [1., 0., 0.,  ..., 0., 0., 0.],
        [0., 1., 0.,  ..., 0., 0., 0.]])
X_enc: tensor([[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],

        [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0.,  ...,  10.,   0.,   0.],
         [  0.,   0.,   0.,  ...,  54.,   0.,   0.],
         [  0.,   0.,   0.,  ...,  38.,   0.,   0.]],

        [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   2.,   0.,   0.]],

        ...,

        [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0.,  ...,  55., 102.,   0.],
         [ 59.,  89.,  74.,  ...,  57., 121.,   0.],
         [ 29.,  78.,  87.,  ...,  87., 255.,   0.]],

        [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],

        [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]])
Model parameters [0]: tensor([[-8.7738e-03, -2.9022e-02, -1.8799e-02,  ...,  6.0493e+05,
         -4.4126e+05, -9.0942e-03],
        [-3.5065e-02,  2.3010e-02, -3.1693e-02,  ...,  1.3756e+04,
          8.7238e+05, -1.9821e-02],
        [-3.3912e+05,  4.3102e+05,  3.0853e-02,  ..., -1.0651e-02,
         -3.3264e-03, -3.3142e-02],
        ...,
        [-3.0869e-02,  2.7252e-02,  1.2299e-02,  ...,  3.5708e+06,
          1.4709e+06, -1.4832e-02],
        [-1.0086e-02,  1.3641e-02,  3.4424e-02,  ..., -6.1035e-03,
         -1.3901e-02, -6.5308e-03],
        [ 2.9721e+06,  1.6479e-03,  1.7426e-02,  ..., -2.4692e+06,
         -8.1160e+05, -2.8336e-02]])
Previous Model parameters [0]: tensor([[-8.7738e-03, -2.9022e-02, -1.8799e-02,  ...,  6.0493e+05,
         -6.4764e+05, -9.0942e-03],
        [-3.5065e-02,  2.3010e-02, -3.1693e-02,  ..., -9.0332e-03,
          3.2196e-02, -1.9821e-02],
        [ 2.7191e-02, -2.5681e-02,  3.0853e-02,  ..., -1.0651e-02,
         -3.3264e-03, -3.3142e-02],
        ...,
        [-3.0869e-02,  2.7252e-02,  1.2299e-02,  ...,  7.6270e+05,
         -2.5859e+05, -1.4832e-02],
        [-1.0086e-02,  1.3641e-02,  3.4424e-02,  ..., -6.1035e-03,
         -1.3901e-02, -6.5308e-03],
        [ 2.8717e-02,  1.6479e-03,  1.7426e-02,  ...,  2.4414e-02,
          1.9440e-02, -2.8336e-02]])
Previous Previous Model parameters [0]: tensor([[-0.0088, -0.0290, -0.0188,  ..., -0.0240, -0.0266, -0.0091],
        [-0.0351,  0.0230, -0.0317,  ..., -0.0090,  0.0322, -0.0198],
        [ 0.0272, -0.0257,  0.0309,  ..., -0.0107, -0.0033, -0.0331],
        ...,
        [-0.0309,  0.0273,  0.0123,  ..., -0.0314, -0.0315, -0.0148],
        [-0.0101,  0.0136,  0.0344,  ..., -0.0061, -0.0139, -0.0065],
        [ 0.0287,  0.0016,  0.0174,  ...,  0.0244,  0.0194, -0.0283]])
tensor(-2.6740e+08)
batch number: 1/157
tensor(1.0265e+09)
batch number: 51/157
tensor(1.3084e+09)
batch number: 101/157
tensor(3.2125e+09)
batch number: 151/157
Test Error: 
 Accuracy: 0.101, Avg loss: 20896702.0 

Epoch 2 took 1804.362251590006 seconds
trial 0 batch size 256
/home/cc/chz_sok_experiments/chz-sok-nn-experiments/nn_venv/lib/python3.8/site-packages/crypten/nn/onnx_converter.py:176: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
  param = torch.from_numpy(numpy_helper.to_array(node))
/home/cc/chz_sok_experiments/chz-sok-nn-experiments/nn_venv/lib/python3.8/site-packages/crypten/nn/onnx_converter.py:176: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
  param = torch.from_numpy(numpy_helper.to_array(node))
ready to train
loss: 2.320220947265625  [256/60000], time: 6.124272331013344
loss: 2.2955780029296875  [25856/60000], time: 6.162609195918776
loss: 2.2861328125  [51456/60000], time: 6.201001062989235
tensor(2.2868)
batch number: 1/157
tensor(116.3592)
batch number: 51/157
tensor(230.3878)
batch number: 101/157
tensor(344.4612)
batch number: 151/157
Test Error: 
 Accuracy: 0.2014, Avg loss: 2.2811684608459473 

Epoch 0 took 1440.2722878539935 seconds
loss: 2.2841033935546875  [256/60000], time: 6.116829992039129
loss: 2.261138916015625  [25856/60000], time: 6.126463691005483
loss: 2.2530670166015625  [51456/60000], time: 6.174777060979977
tensor(2.2524)
batch number: 1/157
tensor(114.5732)
batch number: 51/157
tensor(226.8451)
batch number: 101/157
tensor(339.1951)
batch number: 151/157
Test Error: 
 Accuracy: 0.2122, Avg loss: 2.2463254928588867 

Epoch 1 took 1436.1919161380501 seconds
loss: 2.2489471435546875  [256/60000], time: 6.096204619971104
loss: 2.2255401611328125  [25856/60000], time: 6.221885551000014
loss: 2.2171478271484375  [51456/60000], time: 6.156838821945712
tensor(2.2148)
batch number: 1/157
tensor(112.6228)
batch number: 51/157
tensor(222.9649)
batch number: 101/157
tensor(333.4146)
batch number: 151/157
Test Error: 
 Accuracy: 0.2717, Avg loss: 2.2080724239349365 

Epoch 2 took 1442.0655429000035 seconds
/home/cc/chz_sok_experiments/chz-sok-nn-experiments/nn_venv/lib/python3.8/site-packages/crypten/nn/onnx_converter.py:176: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
  param = torch.from_numpy(numpy_helper.to_array(node))
/home/cc/chz_sok_experiments/chz-sok-nn-experiments/nn_venv/lib/python3.8/site-packages/crypten/nn/onnx_converter.py:176: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
  param = torch.from_numpy(numpy_helper.to_array(node))
ready to train
loss: 10.947052001953125  [256/60000], time: 6.033535222988576
loss: 0.5476226806640625  [25856/60000], time: 6.043761524022557
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.6193389892578125  [51456/60000], time: 5.99809659505263
Model parameters [1] did not change
Model parameters [1] did not change
tensor(2.2782)
batch number: 1/157
tensor(116.3074)
batch number: 51/157
tensor(230.3023)
batch number: 101/157
tensor(344.3850)
batch number: 151/157
Test Error: 
 Accuracy: 0.3837, Avg loss: 2.2805328369140625 

Epoch 0 took 1415.624986357987 seconds
loss: 0.538604736328125  [256/60000], time: 6.10600471496582
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.42950439453125  [25856/60000], time: 6.089827068033628
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.472412109375  [51456/60000], time: 6.009129399084486
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
tensor(2.2767)
batch number: 1/157
tensor(116.2077)
batch number: 51/157
tensor(230.1068)
batch number: 101/157
tensor(344.0914)
batch number: 151/157
Test Error: 
 Accuracy: 0.4073, Avg loss: 2.278594732284546 

Epoch 1 took 1411.9994903099723 seconds
loss: 0.4545745849609375  [256/60000], time: 6.0195842939428985
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [5] did not change
loss: 0.3767852783203125  [25856/60000], time: 6.064743739087135
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
Model parameters [1] did not change
loss: 0.418060302734375  [51456/60000], time: 6.0678220390109345
Model parameters [1] did not change
Model parameters [1] did not change
tensor(2.2756)
batch number: 1/157
tensor(116.1603)
batch number: 51/157
tensor(230.0124)
batch number: 101/157
tensor(343.9507)
batch number: 151/157
Test Error: 
 Accuracy: 0.4071, Avg loss: 2.2776546478271484 

Epoch 2 took 1413.562157219043 seconds
