{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an Encrypted Neural Network\n",
    "\n",
    "In this tutorial, we will walk through an example of how we can train a neural network with CrypTen. This is particularly relevant for the <i>Feature Aggregation</i>, <i>Data Labeling</i> and <i>Data Augmentation</i> use cases. We will focus on the usual two-party setting and show how we can train an accurate neural network for digit classification on the MNIST data.\n",
    "\n",
    "For concreteness, this tutorial will step through the <i>Feature Aggregation</i> use cases: Alice and Bob each have part of the features of the data set, and wish to train a neural network on their combined data, while keeping their data private. \n",
    "\n",
    "## Setup\n",
    "As usual, we'll begin by importing and initializing the `crypten` and `torch` libraries.  \n",
    "\n",
    "We will use the MNIST dataset to demonstrate how Alice and Bob can learn without revealing protected information. For reference, the feature size of each example in the MNIST data is `28 x 28`. Let's assume Alice has the first `28 x 20` features and Bob has last `28 x 8` features. One way to think of this split is that Alice has the (roughly) top 2/3rds of each image, while Bob has the bottom 1/3rd of each image. We'll again use our helper script `mnist_utils.py` that downloads the publicly available MNIST data, and splits the data as required.\n",
    "\n",
    "For simplicity, we will restrict our problem to binary classification: we'll simply learn how to distinguish between 0 and non-zero digits. For speed of execution in the notebook, we will only create a dataset of a 100 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crypten\n",
    "import torch\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "crypten.init()\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 0, 4,  ..., 5, 6, 8])\n",
      "features /tmp/alice_train.pth\n"
     ]
    }
   ],
   "source": [
    "%run ./mnist_utils.py --option features --reduced 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define the network architecture below, and then describe how to train it on encrypted data in the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Define an example network\n",
    "class ExampleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "crypten.common.serial.register_safe_class(ExampleNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encrypted Training\n",
    "\n",
    "After all the material we've covered in earlier tutorials, we only need to know a few additional items for encrypted training. We'll first discuss how the training loop in CrypTen differs from PyTorch. Then, we'll go through a complete example to illustrate training on encrypted data from end-to-end.\n",
    "\n",
    "### How does CrypTen training differ from PyTorch training?\n",
    "\n",
    "There are two main ways implementing a CrypTen training loop differs from a PyTorch training loop. We'll describe these items first, and then illustrate them with small examples below.\n",
    "\n",
    "<i>(1) Use one-hot encoding</i>: CrypTen training requires all labels to use one-hot encoding. This means that when using standard datasets such as MNIST, we need to modify the labels to use one-hot encoding.\n",
    "\n",
    "<i>(2) Directly update parameters</i>: CrypTen does not use the PyTorch optimizers. Instead, CrypTen implements encrypted SGD by implementing its own `backward` function, followed by directly updating the parameters. As we will see below, using SGD in CrypTen is very similar to using the PyTorch optimizers.\n",
    "\n",
    "We now show some small examples to illustrate these differences. As before, we will assume Alice has the rank 0 process and Bob has the rank 1 process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define source argument values for Alice and Bob\n",
    "ALICE = 0\n",
    "BOB = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Alice's data \n",
    "data_alice_enc = crypten.load_from_party('/tmp/alice_train.pth', src=ALICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Complete Example\n",
    "\n",
    "We now put these pieces together for a complete example of training a network in a multi-party setting. \n",
    "\n",
    "As in Tutorial 3, we'll assume Alice has the rank 0 process, and Bob has the rank 1 process; so we'll load and encrypt Alice's data with `src=0`, and load and encrypt Bob's data with `src=1`. We'll then initialize a plaintext model and convert it to an encrypted model, just as we did in Tutorial 4. We'll finally define our loss function, training parameters, and run SGD on the encrypted data. For the purposes of this tutorial we train on 100 samples; training should complete in ~3 minutes per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph encrypted module\n"
     ]
    }
   ],
   "source": [
    "loaded_model = crypten.load(\"model.pth\")\n",
    "print(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    y_eye = torch.eye(10)\n",
    "    \n",
    "    batch_num = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            X = crypten.cryptensor(X)\n",
    "            pred = model(X)\n",
    "            y_one_hot = crypten.cryptensor(y_eye[y])\n",
    "\n",
    "            test_loss += loss_fn(pred, y_one_hot)\n",
    "            print(test_loss.get_plain_text())\n",
    "            \n",
    "            plaintext_pred = pred.get_plain_text()\n",
    "            correct += (plaintext_pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            batch_num += 1\n",
    "            crypten.print(f\"batch number: {batch_num}/{num_batches}\")\n",
    "    test_loss = test_loss.get_plain_text() / num_batches\n",
    "    correct /= size\n",
    "    crypten.print(f\"Test Error: \\n Accuracy: {correct}, Avg loss: {test_loss} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 28, 20])\n",
      "torch.Size([200, 28, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/chz_sok_experiments/nn_experiments/nn_venv/lib/python3.8/site-packages/crypten/nn/onnx_converter.py:176: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  param = torch.from_numpy(numpy_helper.to_array(node))\n",
      "/home/cc/chz_sok_experiments/nn_experiments/nn_venv/lib/python3.8/site-packages/crypten/nn/onnx_converter.py:176: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  param = torch.from_numpy(numpy_helper.to_array(node))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model <class 'crypten.nn.module.Graph'>\n",
      "encrypted model Graph encrypted module\n",
      "train model Graph encrypted module\n",
      "Epoch 0 in progress:\n",
      "\tBatch 1 of 3 Loss 2.3146 [64/200]\n",
      "batch time taken: 2.6067699720151722, total time so far: 2.8189547969959676\n",
      "\tBatch 2 of 3 Loss 2.3190 [128/200]\n",
      "batch time taken: 2.659256358863786, total time so far: 5.48044631886296\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 104\u001b[0m\n\u001b[1;32m    100\u001b[0m     crypten\u001b[38;5;241m.\u001b[39mprint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain model\u001b[39m\u001b[38;5;124m\"\u001b[39m, model)\n\u001b[1;32m    102\u001b[0m     crypten\u001b[38;5;241m.\u001b[39msave(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 104\u001b[0m \u001b[43mrun_encrypted_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/chz_sok_experiments/nn_experiments/nn_venv/lib/python3.8/site-packages/crypten/mpc/context.py:97\u001b[0m, in \u001b[0;36mrun_multiprocess.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     process\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m process \u001b[38;5;129;01min\u001b[39;00m processes:\n\u001b[0;32m---> 97\u001b[0m     \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m was_initialized:\n\u001b[1;32m    100\u001b[0m     crypten\u001b[38;5;241m.\u001b[39minit()\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/popen_fork.py:47\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWNOHANG\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/popen_fork.py:27\u001b[0m, in \u001b[0;36mPopen.poll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m         pid, sts \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;66;03m# Child process not yet created. See #1731717\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;66;03m# e.errno == errno.ECHILD == 10\u001b[39;00m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBatch 3 of 3 Loss 2.3203 [192/200]\n",
      "batch time taken: 2.5861893019173294, total time so far: 8.06855287286453\n",
      "tensor(2.3141)tensor(2.3141)\n",
      "\n",
      "batch number: 1/157\n",
      "tensor(4.6277)tensor(4.6277)\n",
      "\n",
      "batch number: 2/157\n",
      "tensor(6.9527)tensor(6.9527)\n",
      "\n",
      "batch number: 3/157\n",
      "tensor(9.2738)tensor(9.2738)\n",
      "\n",
      "batch number: 4/157\n",
      "tensor(11.5864)tensor(11.5864)\n",
      "\n",
      "batch number: 5/157\n",
      "tensor(13.9028)tensor(13.9028)\n",
      "\n",
      "batch number: 6/157\n",
      "tensor(16.2282)tensor(16.2282)\n",
      "\n",
      "batch number: 7/157\n",
      "tensor(18.5490)tensor(18.5490)\n",
      "\n",
      "batch number: 8/157\n",
      "tensor(20.8605)tensor(20.8605)\n",
      "\n",
      "batch number: 9/157\n",
      "tensor(23.1850)tensor(23.1850)\n",
      "\n",
      "batch number: 10/157\n",
      "tensor(25.5125)tensor(25.5125)\n",
      "\n",
      "batch number: 11/157\n",
      "tensor(27.8234)tensor(27.8234)\n",
      "\n",
      "batch number: 12/157\n",
      "tensor(30.1405)tensor(30.1405)\n",
      "\n",
      "batch number: 13/157\n",
      "tensor(32.4464)tensor(32.4464)\n",
      "\n",
      "batch number: 14/157\n",
      "tensor(34.7699)tensor(34.7699)\n",
      "\n",
      "batch number: 15/157\n",
      "tensor(37.0873)tensor(37.0873)\n",
      "\n",
      "batch number: 16/157\n",
      "tensor(39.4036)tensor(39.4036)\n",
      "\n",
      "batch number: 17/157\n",
      "tensor(41.7250)tensor(41.7250)\n",
      "\n",
      "batch number: 18/157\n",
      "tensor(44.0516)tensor(44.0516)\n",
      "\n",
      "batch number: 19/157\n",
      "tensor(46.3798)tensor(46.3798)\n",
      "\n",
      "batch number: 20/157\n",
      "tensor(48.6990)tensor(48.6990)\n",
      "\n",
      "batch number: 21/157\n",
      "tensor(51.0181)tensor(51.0181)\n",
      "\n",
      "batch number: 22/157\n",
      "tensor(53.3395)tensor(53.3395)\n",
      "\n",
      "batch number: 23/157\n",
      "tensor(55.6490)tensor(55.6490)\n",
      "\n",
      "batch number: 24/157\n",
      "tensor(57.9617)tensor(57.9617)\n",
      "\n",
      "batch number: 25/157\n",
      "tensor(60.2679)tensor(60.2679)\n",
      "\n",
      "batch number: 26/157\n",
      "tensor(62.5879)tensor(62.5879)\n",
      "\n",
      "batch number: 27/157\n",
      "tensor(64.9128)tensor(64.9128)\n",
      "\n",
      "batch number: 28/157\n",
      "tensor(67.2240)tensor(67.2240)\n",
      "\n",
      "batch number: 29/157\n",
      "tensor(69.5323)tensor(69.5323)\n",
      "\n",
      "batch number: 30/157\n",
      "tensor(71.8409)tensor(71.8409)\n",
      "\n",
      "batch number: 31/157\n",
      "tensor(74.1451)tensor(74.1451)\n",
      "\n",
      "batch number: 32/157\n",
      "tensor(76.4677)tensor(76.4677)\n",
      "\n",
      "batch number: 33/157\n",
      "tensor(78.7822)tensor(78.7822)\n",
      "\n",
      "batch number: 34/157\n",
      "tensor(81.0927)tensor(81.0927)\n",
      "\n",
      "batch number: 35/157\n",
      "tensor(83.4174)tensor(83.4174)\n",
      "\n",
      "batch number: 36/157\n",
      "tensor(85.7214)tensor(85.7214)\n",
      "\n",
      "batch number: 37/157\n",
      "tensor(88.0310)tensor(88.0310)\n",
      "\n",
      "batch number: 38/157\n",
      "tensor(90.3561)tensor(90.3561)\n",
      "\n",
      "batch number: 39/157\n",
      "tensor(92.6761)tensor(92.6761)\n",
      "\n",
      "batch number: 40/157\n",
      "tensor(94.9978)tensor(94.9978)\n",
      "\n",
      "batch number: 41/157\n",
      "tensor(97.3179)tensor(97.3179)\n",
      "\n",
      "batch number: 42/157\n",
      "tensor(99.6255)tensor(99.6255)\n",
      "\n",
      "batch number: 43/157\n",
      "tensor(101.9333)tensor(101.9333)\n",
      "\n",
      "batch number: 44/157\n",
      "tensor(104.2442)tensor(104.2442)\n",
      "\n",
      "batch number: 45/157\n",
      "tensor(106.5701)tensor(106.5701)\n",
      "\n",
      "batch number: 46/157\n",
      "tensor(108.8832)tensor(108.8832)\n",
      "\n",
      "batch number: 47/157\n",
      "tensor(111.1882)tensor(111.1882)\n",
      "\n",
      "batch number: 48/157\n",
      "tensor(113.4998)tensor(113.4998)\n",
      "\n",
      "batch number: 49/157\n",
      "tensor(115.8167)tensor(115.8167)\n",
      "\n",
      "batch number: 50/157\n",
      "tensor(118.1377)tensor(118.1377)\n",
      "\n",
      "batch number: 51/157\n",
      "tensor(120.4586)tensor(120.4586)\n",
      "\n",
      "batch number: 52/157\n",
      "tensor(122.7711)tensor(122.7711)\n",
      "\n",
      "batch number: 53/157\n",
      "tensor(125.0896)tensor(125.0896)\n",
      "\n",
      "batch number: 54/157\n",
      "tensor(127.4075)tensor(127.4075)\n",
      "\n",
      "batch number: 55/157\n",
      "tensor(129.7231)tensor(129.7231)\n",
      "\n",
      "batch number: 56/157\n",
      "tensor(132.0430)tensor(132.0430)\n",
      "\n",
      "batch number: 57/157\n",
      "tensor(134.3614)tensor(134.3614)\n",
      "\n",
      "batch number: 58/157\n",
      "tensor(136.6723)tensor(136.6723)\n",
      "\n",
      "batch number: 59/157\n",
      "tensor(138.9876)tensor(138.9876)\n",
      "\n",
      "batch number: 60/157\n",
      "tensor(141.3027)tensor(141.3027)\n",
      "\n",
      "batch number: 61/157\n",
      "tensor(143.6078)tensor(143.6078)\n",
      "\n",
      "batch number: 62/157\n",
      "tensor(145.9262)tensor(145.9262)\n",
      "\n",
      "batch number: 63/157\n",
      "tensor(148.2501)tensor(148.2501)\n",
      "\n",
      "batch number: 64/157\n",
      "tensor(150.5785)tensor(150.5785)\n",
      "\n",
      "batch number: 65/157\n",
      "tensor(152.8943)tensor(152.8943)\n",
      "\n",
      "batch number: 66/157\n",
      "tensor(155.2171)tensor(155.2171)\n",
      "\n",
      "batch number: 67/157\n",
      "tensor(157.5225)tensor(157.5225)\n",
      "\n",
      "batch number: 68/157\n",
      "tensor(159.8387)tensor(159.8387)\n",
      "\n",
      "batch number: 69/157\n",
      "tensor(162.1517)tensor(162.1517)\n",
      "\n",
      "batch number: 70/157\n",
      "tensor(164.4702)tensor(164.4702)\n",
      "\n",
      "batch number: 71/157\n",
      "tensor(166.7866)tensor(166.7866)\n",
      "\n",
      "batch number: 72/157\n",
      "tensor(169.1017)tensor(169.1017)\n",
      "\n",
      "batch number: 73/157\n",
      "tensor(171.4168)tensor(171.4168)\n",
      "\n",
      "batch number: 74/157\n",
      "tensor(173.7296)tensor(173.7296)\n",
      "\n",
      "batch number: 75/157\n",
      "tensor(176.0478)tensor(176.0478)\n",
      "\n",
      "batch number: 76/157\n",
      "tensor(178.3752)tensor(178.3752)\n",
      "\n",
      "batch number: 77/157\n",
      "tensor(180.6730)tensor(180.6730)\n",
      "\n",
      "batch number: 78/157\n",
      "tensor(182.9951)tensor(182.9951)\n",
      "\n",
      "batch number: 79/157\n",
      "tensor(185.3191)\n",
      "tensor(185.3191)\n",
      "batch number: 80/157\n",
      "tensor(187.6456)tensor(187.6456)\n",
      "\n",
      "batch number: 81/157\n",
      "tensor(189.9587)tensor(189.9587)\n",
      "\n",
      "batch number: 82/157\n",
      "tensor(192.2697)tensor(192.2697)\n",
      "\n",
      "batch number: 83/157\n",
      "tensor(194.5851)tensor(194.5851)\n",
      "\n",
      "batch number: 84/157\n",
      "tensor(196.9065)tensor(196.9065)\n",
      "\n",
      "batch number: 85/157\n",
      "tensor(199.2418)tensor(199.2418)\n",
      "\n",
      "batch number: 86/157\n",
      "tensor(201.5461)tensor(201.5461)\n",
      "\n",
      "batch number: 87/157\n",
      "tensor(203.8587)tensor(203.8587)\n",
      "\n",
      "batch number: 88/157\n",
      "tensor(206.1602)tensor(206.1602)\n",
      "\n",
      "batch number: 89/157\n",
      "tensor(208.4878)tensor(208.4878)\n",
      "\n",
      "batch number: 90/157\n",
      "tensor(210.8041)tensor(210.8041)\n",
      "\n",
      "batch number: 91/157\n",
      "tensor(213.1269)tensor(213.1269)\n",
      "\n",
      "batch number: 92/157\n",
      "tensor(215.4356)tensor(215.4356)\n",
      "\n",
      "batch number: 93/157\n",
      "tensor(217.7613)tensor(217.7613)\n",
      "\n",
      "batch number: 94/157\n",
      "tensor(220.0752)tensor(220.0752)\n",
      "\n",
      "batch number: 95/157\n",
      "tensor(222.3816)tensor(222.3816)\n",
      "\n",
      "batch number: 96/157\n",
      "tensor(224.6945)\n",
      "tensor(224.6945)\n",
      "batch number: 97/157\n",
      "tensor(227.0116)tensor(227.0116)\n",
      "\n",
      "batch number: 98/157\n",
      "tensor(229.3292)tensor(229.3292)\n",
      "\n",
      "batch number: 99/157\n",
      "tensor(231.6334)tensor(231.6334)\n",
      "\n",
      "batch number: 100/157\n",
      "tensor(233.9445)tensor(233.9445)\n",
      "\n",
      "batch number: 101/157\n",
      "tensor(236.2607)tensor(236.2607)\n",
      "\n",
      "batch number: 102/157\n",
      "tensor(238.5727)tensor(238.5727)\n",
      "\n",
      "batch number: 103/157\n",
      "tensor(240.8944)tensor(240.8944)\n",
      "\n",
      "batch number: 104/157\n",
      "tensor(243.2012)tensor(243.2012)\n",
      "\n",
      "batch number: 105/157\n",
      "tensor(245.5184)tensor(245.5184)\n",
      "\n",
      "batch number: 106/157\n",
      "tensor(247.8332)tensor(247.8332)\n",
      "\n",
      "batch number: 107/157\n",
      "tensor(250.1368)tensor(250.1368)\n",
      "\n",
      "batch number: 108/157\n",
      "tensor(252.4620)tensor(252.4620)\n",
      "\n",
      "batch number: 109/157\n",
      "tensor(254.7761)tensor(254.7761)\n",
      "\n",
      "batch number: 110/157\n",
      "tensor(257.1014)tensor(257.1014)\n",
      "\n",
      "batch number: 111/157\n",
      "tensor(259.4179)tensor(259.4179)\n",
      "\n",
      "batch number: 112/157\n",
      "tensor(261.7279)tensor(261.7279)\n",
      "\n",
      "batch number: 113/157\n",
      "tensor(264.0449)tensor(264.0449)\n",
      "\n",
      "batch number: 114/157\n",
      "tensor(266.3686)tensor(266.3686)\n",
      "\n",
      "batch number: 115/157\n",
      "tensor(268.6842)tensor(268.6842)\n",
      "\n",
      "batch number: 116/157\n",
      "tensor(270.9990)tensor(270.9990)\n",
      "\n",
      "batch number: 117/157\n",
      "tensor(273.3139)tensor(273.3139)\n",
      "\n",
      "batch number: 118/157\n",
      "tensor(275.6213)tensor(275.6213)\n",
      "\n",
      "batch number: 119/157\n",
      "tensor(277.9277)tensor(277.9277)\n",
      "\n",
      "batch number: 120/157\n",
      "tensor(280.2485)tensor(280.2485)\n",
      "\n",
      "batch number: 121/157\n",
      "tensor(282.5623)tensor(282.5623)\n",
      "\n",
      "batch number: 122/157\n",
      "tensor(284.8747)tensor(284.8747)\n",
      "\n",
      "batch number: 123/157\n",
      "tensor(287.1813)tensor(287.1813)\n",
      "\n",
      "batch number: 124/157\n",
      "tensor(289.5032)tensor(289.5032)\n",
      "\n",
      "batch number: 125/157\n",
      "tensor(291.8261)tensor(291.8261)\n",
      "\n",
      "batch number: 126/157\n",
      "tensor(294.1429)tensor(294.1429)\n",
      "\n",
      "batch number: 127/157\n",
      "tensor(296.4540)tensor(296.4540)\n",
      "\n",
      "batch number: 128/157\n",
      "tensor(298.7688)tensor(298.7688)\n",
      "\n",
      "batch number: 129/157\n",
      "tensor(301.0923)tensor(301.0923)\n",
      "\n",
      "batch number: 130/157\n",
      "tensor(303.4000)tensor(303.4000)\n",
      "\n",
      "batch number: 131/157\n",
      "tensor(305.7162)tensor(305.7162)\n",
      "\n",
      "batch number: 132/157\n",
      "tensor(308.0304)tensor(308.0304)\n",
      "\n",
      "batch number: 133/157\n",
      "tensor(310.3511)tensor(310.3511)\n",
      "\n",
      "batch number: 134/157\n",
      "tensor(312.6553)tensor(312.6553)\n",
      "\n",
      "batch number: 135/157\n",
      "tensor(314.9641)tensor(314.9641)\n",
      "\n",
      "batch number: 136/157\n",
      "tensor(317.2795)tensor(317.2795)\n",
      "\n",
      "batch number: 137/157\n",
      "tensor(319.5836)tensor(319.5836)\n",
      "\n",
      "batch number: 138/157\n",
      "tensor(321.8961)tensor(321.8961)\n",
      "\n",
      "batch number: 139/157\n",
      "tensor(324.2103)tensor(324.2103)\n",
      "\n",
      "batch number: 140/157\n",
      "tensor(326.5185)tensor(326.5185)\n",
      "\n",
      "batch number: 141/157\n",
      "tensor(328.8403)tensor(328.8403)\n",
      "\n",
      "batch number: 142/157\n",
      "tensor(331.1550)tensor(331.1550)\n",
      "\n",
      "batch number: 143/157\n",
      "tensor(333.4721)tensor(333.4721)\n",
      "\n",
      "batch number: 144/157\n",
      "tensor(335.7840)tensor(335.7840)\n",
      "\n",
      "batch number: 145/157\n",
      "tensor(338.1102)tensor(338.1102)\n",
      "\n",
      "batch number: 146/157\n",
      "tensor(340.4312)tensor(340.4312)\n",
      "\n",
      "batch number: 147/157\n",
      "tensor(342.7442)tensor(342.7442)\n",
      "\n",
      "batch number: 148/157\n",
      "tensor(345.0595)tensor(345.0595)\n",
      "\n",
      "batch number: 149/157\n",
      "tensor(347.3752)tensor(347.3752)\n",
      "\n",
      "batch number: 150/157\n",
      "tensor(349.6979)tensor(349.6979)\n",
      "\n",
      "batch number: 151/157\n",
      "tensor(352.0140)tensor(352.0140)\n",
      "\n",
      "batch number: 152/157\n",
      "tensor(354.3363)tensor(354.3363)\n",
      "\n",
      "batch number: 153/157\n",
      "tensor(356.6385)tensor(356.6385)\n",
      "\n",
      "batch number: 154/157\n",
      "tensor(358.9476)tensor(358.9476)\n",
      "\n",
      "batch number: 155/157\n",
      "tensor(361.2651)tensor(361.2651)\n",
      "\n",
      "batch number: 156/157\n",
      "tensor(363.5715)tensor(363.5715)\n",
      "\n",
      "batch number: 157/157\n",
      "Test Error: \n",
      " Accuracy: 0.1174, Avg loss: 2.315742254257202 \n",
      "\n",
      "Epoch 1 in progress:\n",
      "\tBatch 1 of 3 Loss 2.3121 [64/200]\n",
      "batch time taken: 2.692300124792382, total time so far: 131.6714946758002\n",
      "\tBatch 2 of 3 Loss 2.3158 [128/200]\n",
      "batch time taken: 2.7231540428474545, total time so far: 134.39673060178757\n",
      "\tBatch 3 of 3 Loss 2.3181 [192/200]\n",
      "batch time taken: 2.7111401460133493, total time so far: 137.11011264380068\n",
      "tensor(2.3143)tensor(2.3143)\n",
      "\n",
      "batch number: 1/157\n",
      "tensor(4.6282)tensor(4.6282)\n",
      "\n",
      "batch number: 2/157\n",
      "tensor(6.9538)tensor(6.9538)\n",
      "\n",
      "batch number: 3/157\n",
      "tensor(9.2753)tensor(9.2753)\n",
      "\n",
      "batch number: 4/157\n",
      "tensor(11.5879)tensor(11.5879)\n",
      "\n",
      "batch number: 5/157\n",
      "tensor(13.9040)tensor(13.9040)\n",
      "\n",
      "batch number: 6/157\n",
      "tensor(16.2305)tensor(16.2305)\n",
      "\n",
      "batch number: 7/157\n",
      "tensor(18.5513)tensor(18.5513)\n",
      "\n",
      "batch number: 8/157\n",
      "tensor(20.8622)tensor(20.8622)\n",
      "\n",
      "batch number: 9/157\n",
      "tensor(23.1865)tensor(23.1865)\n",
      "\n",
      "batch number: 10/157\n",
      "tensor(25.5139)tensor(25.5139)\n",
      "\n",
      "batch number: 11/157\n",
      "tensor(27.8250)tensor(27.8250)\n",
      "\n",
      "batch number: 12/157\n",
      "tensor(30.1426)tensor(30.1426)\n",
      "\n",
      "batch number: 13/157\n",
      "tensor(32.4483)tensor(32.4483)\n",
      "\n",
      "batch number: 14/157\n",
      "tensor(34.7710)tensor(34.7710)\n",
      "\n",
      "batch number: 15/157\n",
      "tensor(37.0887)tensor(37.0887)\n",
      "\n",
      "batch number: 16/157\n",
      "tensor(39.4047)tensor(39.4047)\n",
      "\n",
      "batch number: 17/157\n",
      "tensor(41.7264)tensor(41.7264)\n",
      "\n",
      "batch number: 18/157\n",
      "tensor(44.0534)tensor(44.0534)\n",
      "\n",
      "batch number: 19/157\n",
      "tensor(46.3813)tensor(46.3813)\n",
      "\n",
      "batch number: 20/157\n",
      "tensor(48.7008)tensor(48.7008)\n",
      "\n",
      "batch number: 21/157\n",
      "tensor(51.0197)tensor(51.0197)\n",
      "\n",
      "batch number: 22/157\n",
      "tensor(53.3411)tensor(53.3411)\n",
      "\n",
      "batch number: 23/157\n",
      "tensor(55.6507)tensor(55.6507)\n",
      "\n",
      "batch number: 24/157\n",
      "tensor(57.9639)tensor(57.9639)\n",
      "\n",
      "batch number: 25/157\n",
      "tensor(60.2699)tensor(60.2699)\n",
      "\n",
      "batch number: 26/157\n",
      "tensor(62.5891)tensor(62.5891)\n",
      "\n",
      "batch number: 27/157\n",
      "tensor(64.9146)tensor(64.9146)\n",
      "\n",
      "batch number: 28/157\n",
      "tensor(67.2264)tensor(67.2264)\n",
      "\n",
      "batch number: 29/157\n",
      "tensor(69.5346)tensor(69.5346)\n",
      "\n",
      "batch number: 30/157\n",
      "tensor(71.8428)tensor(71.8428)\n",
      "\n",
      "batch number: 31/157\n",
      "tensor(74.1468)tensor(74.1468)\n",
      "\n",
      "batch number: 32/157\n",
      "tensor(76.4690)tensor(76.4690)\n",
      "\n",
      "batch number: 33/157\n",
      "tensor(78.7836)tensor(78.7836)\n",
      "\n",
      "batch number: 34/157\n",
      "tensor(81.0936)tensor(81.0936)\n",
      "\n",
      "batch number: 35/157\n",
      "tensor(83.4184)tensor(83.4184)\n",
      "\n",
      "batch number: 36/157\n",
      "tensor(85.7223)tensor(85.7223)\n",
      "\n",
      "batch number: 37/157\n",
      "tensor(88.0325)tensor(88.0325)\n",
      "\n",
      "batch number: 38/157\n",
      "tensor(90.3578)tensor(90.3578)\n",
      "\n",
      "batch number: 39/157\n",
      "tensor(92.6774)tensor(92.6774)\n",
      "\n",
      "batch number: 40/157\n",
      "tensor(94.9994)tensor(94.9994)\n",
      "\n",
      "batch number: 41/157\n",
      "tensor(97.3190)tensor(97.3190)\n",
      "\n",
      "batch number: 42/157\n"
     ]
    }
   ],
   "source": [
    "import crypten.mpc as mpc\n",
    "import crypten.communicator as comm\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "# Since labels are public in this use case, we will simply use them from loaded torch tensors\n",
    "labels = torch.load('/tmp/train_labels.pth')\n",
    "labels = labels.long()\n",
    "\n",
    "dummy_input = torch.empty(1, 1, 28, 28)\n",
    "\n",
    "label_eye = torch.eye(10)\n",
    "labels_one_hot = label_eye[labels]\n",
    "\n",
    "@mpc.run_multiprocess(world_size=2)\n",
    "def run_encrypted_training():\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    # Load data:\n",
    "    x_alice_enc = crypten.load_from_party('/tmp/alice_train.pth', src=ALICE)\n",
    "    x_bob_enc = crypten.load_from_party('/tmp/bob_train.pth', src=BOB)\n",
    "    \n",
    "    crypten.print(x_alice_enc.size())\n",
    "    crypten.print(x_bob_enc.size())\n",
    "    \n",
    "    # Combine the feature sets: identical to Tutorial 3\n",
    "    x_combined_enc = crypten.cat([x_alice_enc, x_bob_enc], dim=2)\n",
    "    \n",
    "    # Reshape to match the network architecture\n",
    "    x_combined_enc = x_combined_enc.unsqueeze(1)\n",
    "    \n",
    "    \n",
    "    # Commenting out due to intermittent failure in PyTorch codebase\n",
    "    \n",
    "    # Initialize a plaintext model and convert to CrypTen model\n",
    "    pytorch_model = ExampleNet()\n",
    "    model = crypten.nn.from_pytorch(pytorch_model, dummy_input)\n",
    "    crypten.print(\"model\", type(model))\n",
    "    model.encrypt()\n",
    "    crypten.print(\"encrypted model\", model)\n",
    "    # Set train mode\n",
    "    model.train()\n",
    "    crypten.print(\"train model\", model)\n",
    "  \n",
    "    # Define a loss function\n",
    "    loss = crypten.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Define training parameters\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 3\n",
    "    batch_size = 64\n",
    "    total_data_size = x_combined_enc.size(0)\n",
    "    num_batches = total_data_size // batch_size\n",
    "    \n",
    "    rank = comm.get().get_rank()\n",
    "    for i in range(num_epochs): \n",
    "        crypten.print(f\"Epoch {i} in progress:\")       \n",
    "        for batch in range(num_batches):\n",
    "            t_prev = time.perf_counter()\n",
    "            # define the start and end of the training mini-batch\n",
    "            start, end = batch * batch_size, (batch + 1) * batch_size\n",
    "                                    \n",
    "            # construct CrypTensors out of training examples / labels\n",
    "            x_train = x_combined_enc[start:end]\n",
    "            y_batch = labels_one_hot[start:end]\n",
    "            y_train = crypten.cryptensor(y_batch, requires_grad=True)\n",
    "            \n",
    "            # perform forward pass:\n",
    "            output = model(x_train)\n",
    "            loss_value = loss(output, y_train)\n",
    "            \n",
    "            # set gradients to \"zero\" \n",
    "            model.zero_grad()\n",
    "\n",
    "            # perform backward pass: \n",
    "            loss_value.backward()\n",
    "\n",
    "            # update parameters\n",
    "            model.update_parameters(learning_rate)\n",
    "            \n",
    "            # Print progress every batch:\n",
    "            batch_loss = loss_value.get_plain_text()\n",
    "            crypten.print(f\"\\tBatch {(batch + 1)} of {num_batches} Loss {batch_loss.item():.4f} [{end}/{total_data_size}]\")\n",
    "            t_now = time.perf_counter()\n",
    "            crypten.print(f\"batch time taken: {t_now - t_prev}, total time so far: {t_now - start_time}\")\n",
    "        test(test_dataloader, model, crypten.nn.CrossEntropyLoss())\n",
    "        model.train()\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    print(\"total time:\", end_time-start_time)\n",
    "\n",
    "    # # Convert encrypted tensors to PyTorch tensors\n",
    "    # torch_model = ExampleNet()  # Define equivalent PyTorch model architecture\n",
    "\n",
    "    # # Copy the weights and biases from Crypten to PyTorch\n",
    "    # with torch.no_grad():\n",
    "    #     torch_model.weight.copy_(model.linear.weight.get_plain_text())\n",
    "    #     torch_model.bias.copy_(model.linear.bias.get_plain_text())\n",
    "\n",
    "    crypten.print(\"train model\", model)\n",
    "    \n",
    "    crypten.save(model, \"model.pth\")\n",
    "\n",
    "run_encrypted_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the average batch loss decreases across the epochs, as we expect during training.\n",
    "\n",
    "This completes our tutorial. Before exiting this tutorial, please clean up the files generated using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "filenames = ['/tmp/alice_train.pth', \n",
    "             '/tmp/bob_train.pth', \n",
    "             '/tmp/alice_test.pth',\n",
    "             '/tmp/bob_test.pth', \n",
    "             '/tmp/train_labels.pth',\n",
    "             '/tmp/test_labels.pth']\n",
    "\n",
    "for fn in filenames:\n",
    "    if os.path.exists(fn): os.remove(fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
