{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor, PILToTensor, Compose, Normalize\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import csv\n",
    "torch.set_num_threads(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    # transform=PILToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "class CifarNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X.float())\n",
    "        # print(\"pred: \",pred.shape)\n",
    "        # print(\"y:\",y.shape)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = Compose(\n",
    "    [\n",
    "        ToTensor(),\n",
    "        Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Download training data from open datasets.\n",
    "training_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    # transform=PILToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.301096  [    4/50000]\n",
      "loss: 2.232884  [  404/50000]\n",
      "loss: 2.212031  [  804/50000]\n",
      "loss: 2.292862  [ 1204/50000]\n",
      "loss: 2.292806  [ 1604/50000]\n",
      "loss: 2.293068  [ 2004/50000]\n",
      "loss: 2.355945  [ 2404/50000]\n",
      "loss: 2.309655  [ 2804/50000]\n",
      "loss: 2.289961  [ 3204/50000]\n",
      "loss: 2.277548  [ 3604/50000]\n",
      "loss: 2.355159  [ 4004/50000]\n",
      "loss: 2.307506  [ 4404/50000]\n",
      "loss: 2.344777  [ 4804/50000]\n",
      "loss: 2.280547  [ 5204/50000]\n",
      "loss: 2.279599  [ 5604/50000]\n",
      "loss: 2.298560  [ 6004/50000]\n",
      "loss: 2.323618  [ 6404/50000]\n",
      "loss: 2.273469  [ 6804/50000]\n",
      "loss: 2.296541  [ 7204/50000]\n",
      "loss: 2.318025  [ 7604/50000]\n",
      "loss: 2.283073  [ 8004/50000]\n",
      "loss: 2.334833  [ 8404/50000]\n",
      "loss: 2.304602  [ 8804/50000]\n",
      "loss: 2.291366  [ 9204/50000]\n",
      "loss: 2.262896  [ 9604/50000]\n",
      "loss: 2.323586  [10004/50000]\n",
      "loss: 2.317422  [10404/50000]\n",
      "loss: 2.278340  [10804/50000]\n",
      "loss: 2.296737  [11204/50000]\n",
      "loss: 2.306054  [11604/50000]\n",
      "loss: 2.289551  [12004/50000]\n",
      "loss: 2.332035  [12404/50000]\n",
      "loss: 2.334979  [12804/50000]\n",
      "loss: 2.283500  [13204/50000]\n",
      "loss: 2.287015  [13604/50000]\n",
      "loss: 2.320676  [14004/50000]\n",
      "loss: 2.296513  [14404/50000]\n",
      "loss: 2.289750  [14804/50000]\n",
      "loss: 2.308275  [15204/50000]\n",
      "loss: 2.301921  [15604/50000]\n",
      "loss: 2.295664  [16004/50000]\n",
      "loss: 2.278330  [16404/50000]\n",
      "loss: 2.304062  [16804/50000]\n",
      "loss: 2.290451  [17204/50000]\n",
      "loss: 2.305685  [17604/50000]\n",
      "loss: 2.305011  [18004/50000]\n",
      "loss: 2.309138  [18404/50000]\n",
      "loss: 2.290882  [18804/50000]\n",
      "loss: 2.287652  [19204/50000]\n",
      "loss: 2.299820  [19604/50000]\n",
      "loss: 2.329758  [20004/50000]\n",
      "loss: 2.285643  [20404/50000]\n",
      "loss: 2.309907  [20804/50000]\n",
      "loss: 2.315710  [21204/50000]\n",
      "loss: 2.299213  [21604/50000]\n",
      "loss: 2.315403  [22004/50000]\n",
      "loss: 2.315304  [22404/50000]\n",
      "loss: 2.283879  [22804/50000]\n",
      "loss: 2.315556  [23204/50000]\n",
      "loss: 2.289322  [23604/50000]\n",
      "loss: 2.276385  [24004/50000]\n",
      "loss: 2.278582  [24404/50000]\n",
      "loss: 2.292679  [24804/50000]\n",
      "loss: 2.276441  [25204/50000]\n",
      "loss: 2.287286  [25604/50000]\n",
      "loss: 2.272755  [26004/50000]\n",
      "loss: 2.299424  [26404/50000]\n",
      "loss: 2.286505  [26804/50000]\n",
      "loss: 2.263484  [27204/50000]\n",
      "loss: 2.274213  [27604/50000]\n",
      "loss: 2.296747  [28004/50000]\n",
      "loss: 2.296101  [28404/50000]\n",
      "loss: 2.321527  [28804/50000]\n",
      "loss: 2.289414  [29204/50000]\n",
      "loss: 2.298222  [29604/50000]\n",
      "loss: 2.293268  [30004/50000]\n",
      "loss: 2.266540  [30404/50000]\n",
      "loss: 2.313055  [30804/50000]\n",
      "loss: 2.270895  [31204/50000]\n",
      "loss: 2.319735  [31604/50000]\n",
      "loss: 2.272068  [32004/50000]\n",
      "loss: 2.266197  [32404/50000]\n",
      "loss: 2.319330  [32804/50000]\n",
      "loss: 2.289751  [33204/50000]\n",
      "loss: 2.299864  [33604/50000]\n",
      "loss: 2.282422  [34004/50000]\n",
      "loss: 2.261746  [34404/50000]\n",
      "loss: 2.271572  [34804/50000]\n",
      "loss: 2.217884  [35204/50000]\n",
      "loss: 2.316430  [35604/50000]\n",
      "loss: 2.265515  [36004/50000]\n",
      "loss: 2.240574  [36404/50000]\n",
      "loss: 2.224707  [36804/50000]\n",
      "loss: 2.247742  [37204/50000]\n",
      "loss: 2.276913  [37604/50000]\n",
      "loss: 2.117554  [38004/50000]\n",
      "loss: 2.297543  [38404/50000]\n",
      "loss: 2.214096  [38804/50000]\n",
      "loss: 2.228218  [39204/50000]\n",
      "loss: 2.282301  [39604/50000]\n",
      "loss: 2.169163  [40004/50000]\n",
      "loss: 2.189383  [40404/50000]\n",
      "loss: 2.330074  [40804/50000]\n",
      "loss: 2.058486  [41204/50000]\n",
      "loss: 2.255843  [41604/50000]\n",
      "loss: 2.197781  [42004/50000]\n",
      "loss: 2.144701  [42404/50000]\n",
      "loss: 2.245401  [42804/50000]\n",
      "loss: 2.312289  [43204/50000]\n",
      "loss: 2.174952  [43604/50000]\n",
      "loss: 2.337849  [44004/50000]\n",
      "loss: 2.259054  [44404/50000]\n",
      "loss: 2.311653  [44804/50000]\n",
      "loss: 2.303189  [45204/50000]\n",
      "loss: 2.080524  [45604/50000]\n",
      "loss: 2.083992  [46004/50000]\n",
      "loss: 2.352371  [46404/50000]\n",
      "loss: 2.288224  [46804/50000]\n",
      "loss: 2.059451  [47204/50000]\n",
      "loss: 2.257810  [47604/50000]\n",
      "loss: 1.921940  [48004/50000]\n",
      "loss: 2.296245  [48404/50000]\n",
      "loss: 2.529252  [48804/50000]\n",
      "loss: 2.049063  [49204/50000]\n",
      "loss: 2.186139  [49604/50000]\n",
      "Test Error: \n",
      " Accuracy: 22.0%, Avg loss: 2.148331 \n",
      "\n",
      "Epoch 1 took 35.600398145848885 seconds\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.063119  [    4/50000]\n",
      "loss: 2.075672  [  404/50000]\n",
      "loss: 2.353058  [  804/50000]\n",
      "loss: 2.265026  [ 1204/50000]\n",
      "loss: 1.873394  [ 1604/50000]\n",
      "loss: 1.993261  [ 2004/50000]\n",
      "loss: 1.804320  [ 2404/50000]\n",
      "loss: 1.764477  [ 2804/50000]\n",
      "loss: 2.239489  [ 3204/50000]\n",
      "loss: 2.037411  [ 3604/50000]\n",
      "loss: 2.032537  [ 4004/50000]\n",
      "loss: 2.150561  [ 4404/50000]\n",
      "loss: 2.064310  [ 4804/50000]\n",
      "loss: 2.153198  [ 5204/50000]\n",
      "loss: 1.865112  [ 5604/50000]\n",
      "loss: 1.957558  [ 6004/50000]\n",
      "loss: 2.024503  [ 6404/50000]\n",
      "loss: 2.189948  [ 6804/50000]\n",
      "loss: 2.244224  [ 7204/50000]\n",
      "loss: 2.323534  [ 7604/50000]\n",
      "loss: 2.028268  [ 8004/50000]\n",
      "loss: 2.137782  [ 8404/50000]\n",
      "loss: 2.314806  [ 8804/50000]\n",
      "loss: 2.164328  [ 9204/50000]\n",
      "loss: 2.053618  [ 9604/50000]\n",
      "loss: 2.138846  [10004/50000]\n",
      "loss: 1.589720  [10404/50000]\n",
      "loss: 2.242689  [10804/50000]\n",
      "loss: 2.031851  [11204/50000]\n",
      "loss: 2.042326  [11604/50000]\n",
      "loss: 2.033469  [12004/50000]\n",
      "loss: 1.605265  [12404/50000]\n",
      "loss: 1.767932  [12804/50000]\n",
      "loss: 2.016786  [13204/50000]\n",
      "loss: 1.990877  [13604/50000]\n",
      "loss: 2.216190  [14004/50000]\n",
      "loss: 1.828850  [14404/50000]\n",
      "loss: 1.921676  [14804/50000]\n",
      "loss: 2.397930  [15204/50000]\n",
      "loss: 2.370185  [15604/50000]\n",
      "loss: 2.200281  [16004/50000]\n",
      "loss: 2.205737  [16404/50000]\n",
      "loss: 1.474632  [16804/50000]\n",
      "loss: 2.019649  [17204/50000]\n",
      "loss: 1.716330  [17604/50000]\n",
      "loss: 1.841123  [18004/50000]\n",
      "loss: 1.847090  [18404/50000]\n",
      "loss: 1.526361  [18804/50000]\n",
      "loss: 1.776507  [19204/50000]\n",
      "loss: 1.585787  [19604/50000]\n",
      "loss: 2.848566  [20004/50000]\n",
      "loss: 2.136876  [20404/50000]\n",
      "loss: 1.587000  [20804/50000]\n",
      "loss: 1.995214  [21204/50000]\n",
      "loss: 1.801300  [21604/50000]\n",
      "loss: 1.985734  [22004/50000]\n",
      "loss: 2.147397  [22404/50000]\n",
      "loss: 1.811812  [22804/50000]\n",
      "loss: 2.697414  [23204/50000]\n",
      "loss: 1.286980  [23604/50000]\n",
      "loss: 1.532851  [24004/50000]\n",
      "loss: 1.508090  [24404/50000]\n",
      "loss: 1.559469  [24804/50000]\n",
      "loss: 1.978202  [25204/50000]\n",
      "loss: 2.543208  [25604/50000]\n",
      "loss: 1.786557  [26004/50000]\n",
      "loss: 2.516579  [26404/50000]\n",
      "loss: 1.682562  [26804/50000]\n",
      "loss: 1.915340  [27204/50000]\n",
      "loss: 1.896592  [27604/50000]\n",
      "loss: 2.012919  [28004/50000]\n",
      "loss: 2.702411  [28404/50000]\n",
      "loss: 2.454509  [28804/50000]\n",
      "loss: 1.771786  [29204/50000]\n",
      "loss: 2.147460  [29604/50000]\n",
      "loss: 1.511347  [30004/50000]\n",
      "loss: 1.503340  [30404/50000]\n",
      "loss: 1.800051  [30804/50000]\n",
      "loss: 1.450053  [31204/50000]\n",
      "loss: 2.433756  [31604/50000]\n",
      "loss: 1.656371  [32004/50000]\n",
      "loss: 1.367470  [32404/50000]\n",
      "loss: 2.316846  [32804/50000]\n",
      "loss: 2.026300  [33204/50000]\n",
      "loss: 2.157289  [33604/50000]\n",
      "loss: 1.974169  [34004/50000]\n",
      "loss: 1.485137  [34404/50000]\n",
      "loss: 1.540616  [34804/50000]\n",
      "loss: 1.821701  [35204/50000]\n",
      "loss: 2.218373  [35604/50000]\n",
      "loss: 1.567711  [36004/50000]\n",
      "loss: 1.911645  [36404/50000]\n",
      "loss: 2.233847  [36804/50000]\n",
      "loss: 1.370028  [37204/50000]\n",
      "loss: 1.318130  [37604/50000]\n",
      "loss: 1.711402  [38004/50000]\n",
      "loss: 1.683592  [38404/50000]\n",
      "loss: 1.594238  [38804/50000]\n",
      "loss: 1.586807  [39204/50000]\n",
      "loss: 1.414980  [39604/50000]\n",
      "loss: 1.396369  [40004/50000]\n",
      "loss: 2.071390  [40404/50000]\n",
      "loss: 2.466212  [40804/50000]\n",
      "loss: 0.956748  [41204/50000]\n",
      "loss: 1.868743  [41604/50000]\n",
      "loss: 1.948217  [42004/50000]\n",
      "loss: 1.461226  [42404/50000]\n",
      "loss: 1.775869  [42804/50000]\n",
      "loss: 1.729935  [43204/50000]\n",
      "loss: 1.742710  [43604/50000]\n",
      "loss: 1.773765  [44004/50000]\n",
      "loss: 1.244709  [44404/50000]\n",
      "loss: 1.880449  [44804/50000]\n",
      "loss: 1.751058  [45204/50000]\n",
      "loss: 1.388296  [45604/50000]\n",
      "loss: 1.713549  [46004/50000]\n",
      "loss: 2.297582  [46404/50000]\n",
      "loss: 1.841711  [46804/50000]\n",
      "loss: 1.868559  [47204/50000]\n",
      "loss: 1.499231  [47604/50000]\n",
      "loss: 1.925824  [48004/50000]\n",
      "loss: 1.508904  [48404/50000]\n",
      "loss: 2.549690  [48804/50000]\n",
      "loss: 1.181669  [49204/50000]\n",
      "loss: 1.543784  [49604/50000]\n",
      "Test Error: \n",
      " Accuracy: 37.1%, Avg loss: 1.720077 \n",
      "\n",
      "Epoch 2 took 35.94723295303993 seconds\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.665371  [    4/50000]\n",
      "loss: 2.115365  [  404/50000]\n",
      "loss: 2.261427  [  804/50000]\n",
      "loss: 2.076957  [ 1204/50000]\n",
      "loss: 1.156126  [ 1604/50000]\n",
      "loss: 1.488165  [ 2004/50000]\n",
      "loss: 1.179521  [ 2404/50000]\n",
      "loss: 1.218803  [ 2804/50000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m epoch_time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m epoch_time_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     16\u001b[0m test_start_time \u001b[38;5;241m=\u001b[39m epoch_time_end\n",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      5\u001b[0m X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Compute prediction error\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# print(\"pred: \",pred.shape)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# print(\"y:\",y.shape)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n",
      "File \u001b[0;32m~/chz_sok_experiments/chz-sok-nn-experiments/crypten_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/chz_sok_experiments/chz-sok-nn-experiments/crypten_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m, in \u001b[0;36mCifarNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 29\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[1;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# flatten all dimensions except batch\u001b[39;00m\n",
      "File \u001b[0;32m~/chz_sok_experiments/chz-sok-nn-experiments/crypten_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/chz_sok_experiments/chz-sok-nn-experiments/crypten_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/chz_sok_experiments/chz-sok-nn-experiments/crypten_venv/lib/python3.8/site-packages/torch/nn/modules/pooling.py:164\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/chz_sok_experiments/chz-sok-nn-experiments/crypten_venv/lib/python3.8/site-packages/torch/_jit_internal.py:497\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/chz_sok_experiments/chz-sok-nn-experiments/crypten_venv/lib/python3.8/site-packages/torch/nn/functional.py:796\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    795\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "\n",
    "epochs = 3\n",
    "for bsize in [4,8]:\n",
    "    model = CifarNetwork().to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train_dataloader_subset = DataLoader(training_data, batch_size=bsize)\n",
    "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "        epoch_time_start = time.perf_counter()\n",
    "        train(train_dataloader_subset, model, loss_fn, optimizer)\n",
    "        epoch_time_end = time.perf_counter()\n",
    "\n",
    "        test_start_time = epoch_time_end\n",
    "        accuracy = test(test_dataloader, model, loss_fn)\n",
    "        test_end_time = time.perf_counter()\n",
    "\n",
    "        epoch_duration = epoch_time_end - epoch_time_start\n",
    "        test_duration = test_end_time - test_start_time\n",
    "\n",
    "        print(f\"Epoch {epoch+1} took {epoch_duration} seconds\")\n",
    "        with open(\"minibatch_cifar_nn.csv\", \"a\") as fp:\n",
    "            wr = csv.writer(fp, dialect='excel')\n",
    "            # epoch_duration, epoch, batch_size, data_size, accuracy, test_duration\n",
    "            wr.writerow([epoch_duration, epoch, bsize, 50000, accuracy, test_duration])\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.369915  [    4/ 7500]\n",
      "loss: 2.293217  [  404/ 7500]\n",
      "loss: 2.344877  [  804/ 7500]\n",
      "loss: 2.348217  [ 1204/ 7500]\n",
      "loss: 2.343471  [ 1604/ 7500]\n",
      "loss: 2.325528  [ 2004/ 7500]\n",
      "loss: 2.276552  [ 2404/ 7500]\n",
      "loss: 2.232662  [ 2804/ 7500]\n",
      "loss: 2.381746  [ 3204/ 7500]\n",
      "loss: 2.339646  [ 3604/ 7500]\n",
      "loss: 2.281080  [ 4004/ 7500]\n",
      "loss: 2.333158  [ 4404/ 7500]\n",
      "loss: 2.269186  [ 4804/ 7500]\n",
      "loss: 2.312050  [ 5204/ 7500]\n",
      "loss: 2.323677  [ 5604/ 7500]\n",
      "loss: 2.351292  [ 6004/ 7500]\n",
      "loss: 2.286300  [ 6404/ 7500]\n",
      "loss: 2.284042  [ 6804/ 7500]\n",
      "loss: 2.312445  [ 7204/ 7500]\n",
      "Test Error: \n",
      " Accuracy: 10.3%, Avg loss: 2.302857 \n",
      "\n",
      "Epoch 1 took 3.894230006961152 seconds\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.346515  [    4/ 7500]\n",
      "loss: 2.292500  [  404/ 7500]\n",
      "loss: 2.327963  [  804/ 7500]\n",
      "loss: 2.333357  [ 1204/ 7500]\n",
      "loss: 2.329401  [ 1604/ 7500]\n",
      "loss: 2.318557  [ 2004/ 7500]\n",
      "loss: 2.279516  [ 2404/ 7500]\n",
      "loss: 2.249833  [ 2804/ 7500]\n",
      "loss: 2.351635  [ 3204/ 7500]\n",
      "loss: 2.328320  [ 3604/ 7500]\n",
      "loss: 2.283685  [ 4004/ 7500]\n",
      "loss: 2.319319  [ 4404/ 7500]\n",
      "loss: 2.277082  [ 4804/ 7500]\n",
      "loss: 2.313898  [ 5204/ 7500]\n",
      "loss: 2.310215  [ 5604/ 7500]\n",
      "loss: 2.329504  [ 6004/ 7500]\n",
      "loss: 2.283031  [ 6404/ 7500]\n",
      "loss: 2.284751  [ 6804/ 7500]\n",
      "loss: 2.305447  [ 7204/ 7500]\n",
      "Test Error: \n",
      " Accuracy: 13.0%, Avg loss: 2.299308 \n",
      "\n",
      "Epoch 2 took 3.8757104569813237 seconds\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.327953  [    4/ 7500]\n",
      "loss: 2.290595  [  404/ 7500]\n",
      "loss: 2.316960  [  804/ 7500]\n",
      "loss: 2.322896  [ 1204/ 7500]\n",
      "loss: 2.313989  [ 1604/ 7500]\n",
      "loss: 2.310303  [ 2004/ 7500]\n",
      "loss: 2.275344  [ 2404/ 7500]\n",
      "loss: 2.261139  [ 2804/ 7500]\n",
      "loss: 2.327800  [ 3204/ 7500]\n",
      "loss: 2.319187  [ 3604/ 7500]\n",
      "loss: 2.278904  [ 4004/ 7500]\n",
      "loss: 2.308618  [ 4404/ 7500]\n",
      "loss: 2.278057  [ 4804/ 7500]\n",
      "loss: 2.314978  [ 5204/ 7500]\n",
      "loss: 2.299002  [ 5604/ 7500]\n",
      "loss: 2.307062  [ 6004/ 7500]\n",
      "loss: 2.271385  [ 6404/ 7500]\n",
      "loss: 2.287858  [ 6804/ 7500]\n",
      "loss: 2.297861  [ 7204/ 7500]\n",
      "Test Error: \n",
      " Accuracy: 13.5%, Avg loss: 2.292146 \n",
      "\n",
      "Epoch 3 took 3.9103467679815367 seconds\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.334721  [    4/15000]\n",
      "loss: 2.272677  [  404/15000]\n",
      "loss: 2.286291  [  804/15000]\n",
      "loss: 2.308948  [ 1204/15000]\n",
      "loss: 2.284832  [ 1604/15000]\n",
      "loss: 2.277133  [ 2004/15000]\n",
      "loss: 2.305600  [ 2404/15000]\n",
      "loss: 2.301285  [ 2804/15000]\n",
      "loss: 2.314533  [ 3204/15000]\n",
      "loss: 2.310390  [ 3604/15000]\n",
      "loss: 2.302936  [ 4004/15000]\n",
      "loss: 2.318783  [ 4404/15000]\n",
      "loss: 2.341143  [ 4804/15000]\n",
      "loss: 2.302368  [ 5204/15000]\n",
      "loss: 2.282264  [ 5604/15000]\n",
      "loss: 2.315982  [ 6004/15000]\n",
      "loss: 2.265967  [ 6404/15000]\n",
      "loss: 2.275810  [ 6804/15000]\n",
      "loss: 2.276615  [ 7204/15000]\n",
      "loss: 2.338350  [ 7604/15000]\n",
      "loss: 2.309049  [ 8004/15000]\n",
      "loss: 2.326756  [ 8404/15000]\n",
      "loss: 2.305565  [ 8804/15000]\n",
      "loss: 2.317947  [ 9204/15000]\n",
      "loss: 2.296371  [ 9604/15000]\n",
      "loss: 2.314276  [10004/15000]\n",
      "loss: 2.293548  [10404/15000]\n",
      "loss: 2.313646  [10804/15000]\n",
      "loss: 2.287678  [11204/15000]\n",
      "loss: 2.323135  [11604/15000]\n",
      "loss: 2.307273  [12004/15000]\n",
      "loss: 2.277467  [12404/15000]\n",
      "loss: 2.314056  [12804/15000]\n",
      "loss: 2.290707  [13204/15000]\n",
      "loss: 2.296954  [13604/15000]\n",
      "loss: 2.330007  [14004/15000]\n",
      "loss: 2.273849  [14404/15000]\n",
      "loss: 2.295549  [14804/15000]\n",
      "Test Error: \n",
      " Accuracy: 12.2%, Avg loss: 2.301243 \n",
      "\n",
      "Epoch 1 took 7.8217668319121 seconds\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.319282  [    4/15000]\n",
      "loss: 2.273501  [  404/15000]\n",
      "loss: 2.283958  [  804/15000]\n",
      "loss: 2.304428  [ 1204/15000]\n",
      "loss: 2.293134  [ 1604/15000]\n",
      "loss: 2.284626  [ 2004/15000]\n",
      "loss: 2.303208  [ 2404/15000]\n",
      "loss: 2.294710  [ 2804/15000]\n",
      "loss: 2.313154  [ 3204/15000]\n",
      "loss: 2.305405  [ 3604/15000]\n",
      "loss: 2.305476  [ 4004/15000]\n",
      "loss: 2.308421  [ 4404/15000]\n",
      "loss: 2.326721  [ 4804/15000]\n",
      "loss: 2.302905  [ 5204/15000]\n",
      "loss: 2.282881  [ 5604/15000]\n",
      "loss: 2.309152  [ 6004/15000]\n",
      "loss: 2.272440  [ 6404/15000]\n",
      "loss: 2.278541  [ 6804/15000]\n",
      "loss: 2.281347  [ 7204/15000]\n",
      "loss: 2.329244  [ 7604/15000]\n",
      "loss: 2.302950  [ 8004/15000]\n",
      "loss: 2.319556  [ 8404/15000]\n",
      "loss: 2.296311  [ 8804/15000]\n",
      "loss: 2.308406  [ 9204/15000]\n",
      "loss: 2.290994  [ 9604/15000]\n",
      "loss: 2.302396  [10004/15000]\n",
      "loss: 2.284657  [10404/15000]\n",
      "loss: 2.312397  [10804/15000]\n",
      "loss: 2.291165  [11204/15000]\n",
      "loss: 2.321781  [11604/15000]\n",
      "loss: 2.300183  [12004/15000]\n",
      "loss: 2.271700  [12404/15000]\n",
      "loss: 2.305583  [12804/15000]\n",
      "loss: 2.297825  [13204/15000]\n",
      "loss: 2.298227  [13604/15000]\n",
      "loss: 2.326066  [14004/15000]\n",
      "loss: 2.279744  [14404/15000]\n",
      "loss: 2.310107  [14804/15000]\n",
      "Test Error: \n",
      " Accuracy: 15.3%, Avg loss: 2.294824 \n",
      "\n",
      "Epoch 2 took 7.795514096971601 seconds\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.295913  [    4/15000]\n",
      "loss: 2.265400  [  404/15000]\n",
      "loss: 2.284811  [  804/15000]\n",
      "loss: 2.296638  [ 1204/15000]\n",
      "loss: 2.278127  [ 1604/15000]\n",
      "loss: 2.275196  [ 2004/15000]\n",
      "loss: 2.282299  [ 2404/15000]\n",
      "loss: 2.271044  [ 2804/15000]\n",
      "loss: 2.319404  [ 3204/15000]\n",
      "loss: 2.293455  [ 3604/15000]\n",
      "loss: 2.291047  [ 4004/15000]\n",
      "loss: 2.307567  [ 4404/15000]\n",
      "loss: 2.302850  [ 4804/15000]\n",
      "loss: 2.293775  [ 5204/15000]\n",
      "loss: 2.283505  [ 5604/15000]\n",
      "loss: 2.293584  [ 6004/15000]\n",
      "loss: 2.240272  [ 6404/15000]\n",
      "loss: 2.286405  [ 6804/15000]\n",
      "loss: 2.275240  [ 7204/15000]\n",
      "loss: 2.335241  [ 7604/15000]\n",
      "loss: 2.293480  [ 8004/15000]\n",
      "loss: 2.307991  [ 8404/15000]\n",
      "loss: 2.294017  [ 8804/15000]\n",
      "loss: 2.300249  [ 9204/15000]\n",
      "loss: 2.268812  [ 9604/15000]\n",
      "loss: 2.284690  [10004/15000]\n",
      "loss: 2.207287  [10404/15000]\n",
      "loss: 2.312403  [10804/15000]\n",
      "loss: 2.280937  [11204/15000]\n",
      "loss: 2.316668  [11604/15000]\n",
      "loss: 2.299643  [12004/15000]\n",
      "loss: 2.118245  [12404/15000]\n",
      "loss: 2.279910  [12804/15000]\n",
      "loss: 2.336337  [13204/15000]\n",
      "loss: 2.366690  [13604/15000]\n",
      "loss: 2.348475  [14004/15000]\n",
      "loss: 2.254886  [14404/15000]\n",
      "loss: 2.376010  [14804/15000]\n",
      "Test Error: \n",
      " Accuracy: 15.8%, Avg loss: 2.239672 \n",
      "\n",
      "Epoch 3 took 7.7869532980257645 seconds\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.336985  [    4/30000]\n",
      "loss: 2.366110  [  404/30000]\n",
      "loss: 2.323725  [  804/30000]\n",
      "loss: 2.284811  [ 1204/30000]\n",
      "loss: 2.292126  [ 1604/30000]\n",
      "loss: 2.351375  [ 2004/30000]\n",
      "loss: 2.341133  [ 2404/30000]\n",
      "loss: 2.295016  [ 2804/30000]\n",
      "loss: 2.313863  [ 3204/30000]\n",
      "loss: 2.339664  [ 3604/30000]\n",
      "loss: 2.281412  [ 4004/30000]\n",
      "loss: 2.276881  [ 4404/30000]\n",
      "loss: 2.271948  [ 4804/30000]\n",
      "loss: 2.297693  [ 5204/30000]\n",
      "loss: 2.312128  [ 5604/30000]\n",
      "loss: 2.310081  [ 6004/30000]\n",
      "loss: 2.307266  [ 6404/30000]\n",
      "loss: 2.326911  [ 6804/30000]\n",
      "loss: 2.313104  [ 7204/30000]\n",
      "loss: 2.244591  [ 7604/30000]\n",
      "loss: 2.299908  [ 8004/30000]\n",
      "loss: 2.301626  [ 8404/30000]\n",
      "loss: 2.319816  [ 8804/30000]\n",
      "loss: 2.306238  [ 9204/30000]\n",
      "loss: 2.301980  [ 9604/30000]\n",
      "loss: 2.306682  [10004/30000]\n",
      "loss: 2.312599  [10404/30000]\n",
      "loss: 2.293684  [10804/30000]\n",
      "loss: 2.296865  [11204/30000]\n",
      "loss: 2.294411  [11604/30000]\n",
      "loss: 2.333314  [12004/30000]\n",
      "loss: 2.312957  [12404/30000]\n",
      "loss: 2.303045  [12804/30000]\n",
      "loss: 2.256753  [13204/30000]\n",
      "loss: 2.260344  [13604/30000]\n",
      "loss: 2.284653  [14004/30000]\n",
      "loss: 2.314946  [14404/30000]\n",
      "loss: 2.280860  [14804/30000]\n",
      "loss: 2.269871  [15204/30000]\n",
      "loss: 2.261127  [15604/30000]\n",
      "loss: 2.327255  [16004/30000]\n",
      "loss: 2.277913  [16404/30000]\n",
      "loss: 2.308536  [16804/30000]\n",
      "loss: 2.330130  [17204/30000]\n",
      "loss: 2.297679  [17604/30000]\n",
      "loss: 2.285195  [18004/30000]\n",
      "loss: 2.287198  [18404/30000]\n",
      "loss: 2.302339  [18804/30000]\n",
      "loss: 2.293991  [19204/30000]\n",
      "loss: 2.299556  [19604/30000]\n",
      "loss: 2.324033  [20004/30000]\n",
      "loss: 2.311074  [20404/30000]\n",
      "loss: 2.210757  [20804/30000]\n",
      "loss: 2.299220  [21204/30000]\n",
      "loss: 2.319420  [21604/30000]\n",
      "loss: 2.287709  [22004/30000]\n",
      "loss: 2.291355  [22404/30000]\n",
      "loss: 2.291003  [22804/30000]\n",
      "loss: 2.283276  [23204/30000]\n",
      "loss: 2.267880  [23604/30000]\n",
      "loss: 2.269375  [24004/30000]\n",
      "loss: 2.264630  [24404/30000]\n",
      "loss: 2.239668  [24804/30000]\n",
      "loss: 2.296699  [25204/30000]\n",
      "loss: 2.293118  [25604/30000]\n",
      "loss: 2.327679  [26004/30000]\n",
      "loss: 2.268389  [26404/30000]\n",
      "loss: 2.274791  [26804/30000]\n",
      "loss: 2.266060  [27204/30000]\n",
      "loss: 2.267923  [27604/30000]\n",
      "loss: 2.343932  [28004/30000]\n",
      "loss: 2.422004  [28404/30000]\n",
      "loss: 2.340822  [28804/30000]\n",
      "loss: 2.267312  [29204/30000]\n",
      "loss: 2.358383  [29604/30000]\n",
      "Test Error: \n",
      " Accuracy: 17.1%, Avg loss: 2.257339 \n",
      "\n",
      "Epoch 1 took 15.624297069967724 seconds\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.153515  [    4/30000]\n",
      "loss: 2.297521  [  404/30000]\n",
      "loss: 2.288255  [  804/30000]\n",
      "loss: 2.238816  [ 1204/30000]\n",
      "loss: 2.202907  [ 1604/30000]\n",
      "loss: 2.248061  [ 2004/30000]\n",
      "loss: 2.112682  [ 2404/30000]\n",
      "loss: 2.231133  [ 2804/30000]\n",
      "loss: 2.069347  [ 3204/30000]\n",
      "loss: 2.140748  [ 3604/30000]\n",
      "loss: 2.273627  [ 4004/30000]\n",
      "loss: 2.169988  [ 4404/30000]\n",
      "loss: 2.269547  [ 4804/30000]\n",
      "loss: 2.295895  [ 5204/30000]\n",
      "loss: 2.237075  [ 5604/30000]\n",
      "loss: 2.029149  [ 6004/30000]\n",
      "loss: 2.049683  [ 6404/30000]\n",
      "loss: 2.294696  [ 6804/30000]\n",
      "loss: 2.235375  [ 7204/30000]\n",
      "loss: 2.279009  [ 7604/30000]\n",
      "loss: 2.165842  [ 8004/30000]\n",
      "loss: 2.377422  [ 8404/30000]\n",
      "loss: 2.487117  [ 8804/30000]\n",
      "loss: 2.118855  [ 9204/30000]\n",
      "loss: 2.222982  [ 9604/30000]\n",
      "loss: 2.143965  [10004/30000]\n",
      "loss: 1.792380  [10404/30000]\n",
      "loss: 2.333419  [10804/30000]\n",
      "loss: 2.118897  [11204/30000]\n",
      "loss: 2.125592  [11604/30000]\n",
      "loss: 2.271030  [12004/30000]\n",
      "loss: 1.776703  [12404/30000]\n",
      "loss: 2.326287  [12804/30000]\n",
      "loss: 2.052015  [13204/30000]\n",
      "loss: 1.969138  [13604/30000]\n",
      "loss: 2.137908  [14004/30000]\n",
      "loss: 2.084306  [14404/30000]\n",
      "loss: 2.042300  [14804/30000]\n",
      "loss: 2.253348  [15204/30000]\n",
      "loss: 2.052647  [15604/30000]\n",
      "loss: 2.388870  [16004/30000]\n",
      "loss: 2.200909  [16404/30000]\n",
      "loss: 1.873399  [16804/30000]\n",
      "loss: 1.821178  [17204/30000]\n",
      "loss: 1.815006  [17604/30000]\n",
      "loss: 1.805630  [18004/30000]\n",
      "loss: 2.025957  [18404/30000]\n",
      "loss: 1.630602  [18804/30000]\n",
      "loss: 2.014717  [19204/30000]\n",
      "loss: 1.794221  [19604/30000]\n",
      "loss: 3.179661  [20004/30000]\n",
      "loss: 2.324170  [20404/30000]\n",
      "loss: 1.631623  [20804/30000]\n",
      "loss: 1.763259  [21204/30000]\n",
      "loss: 2.125827  [21604/30000]\n",
      "loss: 2.365985  [22004/30000]\n",
      "loss: 1.994220  [22404/30000]\n",
      "loss: 1.839401  [22804/30000]\n",
      "loss: 2.351626  [23204/30000]\n",
      "loss: 1.617311  [23604/30000]\n",
      "loss: 1.646647  [24004/30000]\n",
      "loss: 1.967205  [24404/30000]\n",
      "loss: 1.818343  [24804/30000]\n",
      "loss: 2.130924  [25204/30000]\n",
      "loss: 2.215900  [25604/30000]\n",
      "loss: 1.818439  [26004/30000]\n",
      "loss: 2.421342  [26404/30000]\n",
      "loss: 1.886919  [26804/30000]\n",
      "loss: 1.893540  [27204/30000]\n",
      "loss: 2.081700  [27604/30000]\n",
      "loss: 2.126091  [28004/30000]\n",
      "loss: 2.502543  [28404/30000]\n",
      "loss: 2.498246  [28804/30000]\n",
      "loss: 2.109634  [29204/30000]\n",
      "loss: 2.310360  [29604/30000]\n",
      "Test Error: \n",
      " Accuracy: 26.9%, Avg loss: 2.000214 \n",
      "\n",
      "Epoch 2 took 15.642263774992898 seconds\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.574090  [    4/30000]\n",
      "loss: 2.238695  [  404/30000]\n",
      "loss: 2.227696  [  804/30000]\n",
      "loss: 2.042599  [ 1204/30000]\n",
      "loss: 1.469088  [ 1604/30000]\n",
      "loss: 2.109100  [ 2004/30000]\n",
      "loss: 1.507227  [ 2404/30000]\n",
      "loss: 1.753144  [ 2804/30000]\n",
      "loss: 2.254769  [ 3204/30000]\n",
      "loss: 1.782053  [ 3604/30000]\n",
      "loss: 1.824778  [ 4004/30000]\n",
      "loss: 1.904636  [ 4404/30000]\n",
      "loss: 2.161743  [ 4804/30000]\n",
      "loss: 2.202518  [ 5204/30000]\n",
      "loss: 1.522298  [ 5604/30000]\n",
      "loss: 1.632564  [ 6004/30000]\n",
      "loss: 1.849892  [ 6404/30000]\n",
      "loss: 2.127074  [ 6804/30000]\n",
      "loss: 2.189343  [ 7204/30000]\n",
      "loss: 2.154310  [ 7604/30000]\n",
      "loss: 1.851305  [ 8004/30000]\n",
      "loss: 2.343881  [ 8404/30000]\n",
      "loss: 2.349339  [ 8804/30000]\n",
      "loss: 2.053383  [ 9204/30000]\n",
      "loss: 2.258643  [ 9604/30000]\n",
      "loss: 2.254631  [10004/30000]\n",
      "loss: 1.285359  [10404/30000]\n",
      "loss: 2.012378  [10804/30000]\n",
      "loss: 2.024976  [11204/30000]\n",
      "loss: 1.935183  [11604/30000]\n",
      "loss: 1.907502  [12004/30000]\n",
      "loss: 1.591338  [12404/30000]\n",
      "loss: 1.716896  [12804/30000]\n",
      "loss: 1.756400  [13204/30000]\n",
      "loss: 1.882744  [13604/30000]\n",
      "loss: 2.437580  [14004/30000]\n",
      "loss: 1.910338  [14404/30000]\n",
      "loss: 1.959420  [14804/30000]\n",
      "loss: 2.428600  [15204/30000]\n",
      "loss: 2.218055  [15604/30000]\n",
      "loss: 2.077845  [16004/30000]\n",
      "loss: 2.423208  [16404/30000]\n",
      "loss: 1.469081  [16804/30000]\n",
      "loss: 1.888896  [17204/30000]\n",
      "loss: 1.418095  [17604/30000]\n",
      "loss: 1.782610  [18004/30000]\n",
      "loss: 1.853149  [18404/30000]\n",
      "loss: 1.574182  [18804/30000]\n",
      "loss: 1.644805  [19204/30000]\n",
      "loss: 1.573454  [19604/30000]\n",
      "loss: 2.737504  [20004/30000]\n",
      "loss: 2.084396  [20404/30000]\n",
      "loss: 1.256359  [20804/30000]\n",
      "loss: 2.018643  [21204/30000]\n",
      "loss: 1.687143  [21604/30000]\n",
      "loss: 1.951230  [22004/30000]\n",
      "loss: 1.974119  [22404/30000]\n",
      "loss: 1.906899  [22804/30000]\n",
      "loss: 2.676219  [23204/30000]\n",
      "loss: 1.315732  [23604/30000]\n",
      "loss: 1.458716  [24004/30000]\n",
      "loss: 1.308544  [24404/30000]\n",
      "loss: 1.494929  [24804/30000]\n",
      "loss: 2.045480  [25204/30000]\n",
      "loss: 2.539982  [25604/30000]\n",
      "loss: 1.718992  [26004/30000]\n",
      "loss: 2.477634  [26404/30000]\n",
      "loss: 1.614475  [26804/30000]\n",
      "loss: 1.605322  [27204/30000]\n",
      "loss: 2.216935  [27604/30000]\n",
      "loss: 2.128446  [28004/30000]\n",
      "loss: 2.532308  [28404/30000]\n",
      "loss: 2.478574  [28804/30000]\n",
      "loss: 1.880504  [29204/30000]\n",
      "loss: 1.957148  [29604/30000]\n",
      "Test Error: \n",
      " Accuracy: 34.1%, Avg loss: 1.816151 \n",
      "\n",
      "Epoch 3 took 15.664754846016876 seconds\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.263508  [    4/50000]\n",
      "loss: 2.391235  [  404/50000]\n",
      "loss: 2.345329  [  804/50000]\n",
      "loss: 2.256898  [ 1204/50000]\n",
      "loss: 2.325436  [ 1604/50000]\n",
      "loss: 2.294384  [ 2004/50000]\n",
      "loss: 2.328834  [ 2404/50000]\n",
      "loss: 2.264990  [ 2804/50000]\n",
      "loss: 2.252046  [ 3204/50000]\n",
      "loss: 2.255107  [ 3604/50000]\n",
      "loss: 2.383463  [ 4004/50000]\n",
      "loss: 2.255831  [ 4404/50000]\n",
      "loss: 2.312120  [ 4804/50000]\n",
      "loss: 2.281002  [ 5204/50000]\n",
      "loss: 2.305686  [ 5604/50000]\n",
      "loss: 2.288136  [ 6004/50000]\n",
      "loss: 2.311311  [ 6404/50000]\n",
      "loss: 2.341445  [ 6804/50000]\n",
      "loss: 2.281265  [ 7204/50000]\n",
      "loss: 2.282365  [ 7604/50000]\n",
      "loss: 2.251383  [ 8004/50000]\n",
      "loss: 2.329731  [ 8404/50000]\n",
      "loss: 2.302333  [ 8804/50000]\n",
      "loss: 2.291127  [ 9204/50000]\n",
      "loss: 2.277182  [ 9604/50000]\n",
      "loss: 2.272963  [10004/50000]\n",
      "loss: 2.295994  [10404/50000]\n",
      "loss: 2.373157  [10804/50000]\n",
      "loss: 2.339963  [11204/50000]\n",
      "loss: 2.320791  [11604/50000]\n",
      "loss: 2.271536  [12004/50000]\n",
      "loss: 2.319479  [12404/50000]\n",
      "loss: 2.304546  [12804/50000]\n",
      "loss: 2.281402  [13204/50000]\n",
      "loss: 2.252158  [13604/50000]\n",
      "loss: 2.295646  [14004/50000]\n",
      "loss: 2.292083  [14404/50000]\n",
      "loss: 2.292345  [14804/50000]\n",
      "loss: 2.295488  [15204/50000]\n",
      "loss: 2.284101  [15604/50000]\n",
      "loss: 2.288308  [16004/50000]\n",
      "loss: 2.306781  [16404/50000]\n",
      "loss: 2.291402  [16804/50000]\n",
      "loss: 2.306878  [17204/50000]\n",
      "loss: 2.288836  [17604/50000]\n",
      "loss: 2.323850  [18004/50000]\n",
      "loss: 2.294330  [18404/50000]\n",
      "loss: 2.293871  [18804/50000]\n",
      "loss: 2.286528  [19204/50000]\n",
      "loss: 2.302799  [19604/50000]\n",
      "loss: 2.329396  [20004/50000]\n",
      "loss: 2.301673  [20404/50000]\n",
      "loss: 2.291883  [20804/50000]\n",
      "loss: 2.290821  [21204/50000]\n",
      "loss: 2.293341  [21604/50000]\n",
      "loss: 2.301585  [22004/50000]\n",
      "loss: 2.295026  [22404/50000]\n",
      "loss: 2.309394  [22804/50000]\n",
      "loss: 2.286841  [23204/50000]\n",
      "loss: 2.281011  [23604/50000]\n",
      "loss: 2.272940  [24004/50000]\n",
      "loss: 2.278904  [24404/50000]\n",
      "loss: 2.292998  [24804/50000]\n",
      "loss: 2.326732  [25204/50000]\n",
      "loss: 2.304802  [25604/50000]\n",
      "loss: 2.318898  [26004/50000]\n",
      "loss: 2.309441  [26404/50000]\n",
      "loss: 2.298330  [26804/50000]\n",
      "loss: 2.290293  [27204/50000]\n",
      "loss: 2.255085  [27604/50000]\n",
      "loss: 2.301276  [28004/50000]\n",
      "loss: 2.343520  [28404/50000]\n",
      "loss: 2.311965  [28804/50000]\n",
      "loss: 2.277762  [29204/50000]\n",
      "loss: 2.294348  [29604/50000]\n",
      "loss: 2.330096  [30004/50000]\n",
      "loss: 2.317040  [30404/50000]\n",
      "loss: 2.296143  [30804/50000]\n",
      "loss: 2.288298  [31204/50000]\n",
      "loss: 2.308790  [31604/50000]\n",
      "loss: 2.283462  [32004/50000]\n",
      "loss: 2.279127  [32404/50000]\n",
      "loss: 2.313373  [32804/50000]\n",
      "loss: 2.287753  [33204/50000]\n",
      "loss: 2.315468  [33604/50000]\n",
      "loss: 2.301751  [34004/50000]\n",
      "loss: 2.263073  [34404/50000]\n",
      "loss: 2.266865  [34804/50000]\n",
      "loss: 2.265179  [35204/50000]\n",
      "loss: 2.308720  [35604/50000]\n",
      "loss: 2.274423  [36004/50000]\n",
      "loss: 2.251967  [36404/50000]\n",
      "loss: 2.294329  [36804/50000]\n",
      "loss: 2.236690  [37204/50000]\n",
      "loss: 2.255761  [37604/50000]\n",
      "loss: 2.254203  [38004/50000]\n",
      "loss: 2.278819  [38404/50000]\n",
      "loss: 2.255416  [38804/50000]\n",
      "loss: 2.271060  [39204/50000]\n",
      "loss: 2.250433  [39604/50000]\n",
      "loss: 2.209957  [40004/50000]\n",
      "loss: 2.281704  [40404/50000]\n",
      "loss: 2.287425  [40804/50000]\n",
      "loss: 2.206836  [41204/50000]\n",
      "loss: 2.290283  [41604/50000]\n",
      "loss: 2.236001  [42004/50000]\n",
      "loss: 2.256742  [42404/50000]\n",
      "loss: 2.292740  [42804/50000]\n",
      "loss: 2.267171  [43204/50000]\n",
      "loss: 2.217331  [43604/50000]\n",
      "loss: 2.281311  [44004/50000]\n",
      "loss: 2.262514  [44404/50000]\n",
      "loss: 2.304260  [44804/50000]\n",
      "loss: 2.321404  [45204/50000]\n",
      "loss: 2.155526  [45604/50000]\n",
      "loss: 2.200832  [46004/50000]\n",
      "loss: 2.315905  [46404/50000]\n",
      "loss: 2.374625  [46804/50000]\n",
      "loss: 2.141185  [47204/50000]\n",
      "loss: 2.360725  [47604/50000]\n",
      "loss: 2.074275  [48004/50000]\n",
      "loss: 2.374588  [48404/50000]\n",
      "loss: 2.509401  [48804/50000]\n",
      "loss: 2.211478  [49204/50000]\n",
      "loss: 2.203402  [49604/50000]\n",
      "Test Error: \n",
      " Accuracy: 17.5%, Avg loss: 2.207799 \n",
      "\n",
      "Epoch 1 took 25.90612597600557 seconds\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.113675  [    4/50000]\n",
      "loss: 2.202816  [  404/50000]\n",
      "loss: 2.399135  [  804/50000]\n",
      "loss: 2.283534  [ 1204/50000]\n",
      "loss: 1.981093  [ 1604/50000]\n",
      "loss: 2.057654  [ 2004/50000]\n",
      "loss: 1.999290  [ 2404/50000]\n",
      "loss: 1.942957  [ 2804/50000]\n",
      "loss: 2.383870  [ 3204/50000]\n",
      "loss: 2.161774  [ 3604/50000]\n",
      "loss: 2.010865  [ 4004/50000]\n",
      "loss: 2.330960  [ 4404/50000]\n",
      "loss: 2.096622  [ 4804/50000]\n",
      "loss: 2.205557  [ 5204/50000]\n",
      "loss: 2.039641  [ 5604/50000]\n",
      "loss: 2.174130  [ 6004/50000]\n",
      "loss: 1.925123  [ 6404/50000]\n",
      "loss: 2.364992  [ 6804/50000]\n",
      "loss: 2.196249  [ 7204/50000]\n",
      "loss: 2.377002  [ 7604/50000]\n",
      "loss: 2.154176  [ 8004/50000]\n",
      "loss: 2.233005  [ 8404/50000]\n",
      "loss: 2.297951  [ 8804/50000]\n",
      "loss: 2.238748  [ 9204/50000]\n",
      "loss: 2.140774  [ 9604/50000]\n",
      "loss: 2.212531  [10004/50000]\n",
      "loss: 1.771461  [10404/50000]\n",
      "loss: 2.321070  [10804/50000]\n",
      "loss: 2.189452  [11204/50000]\n",
      "loss: 2.225775  [11604/50000]\n",
      "loss: 2.237377  [12004/50000]\n",
      "loss: 1.683216  [12404/50000]\n",
      "loss: 2.067194  [12804/50000]\n",
      "loss: 2.120187  [13204/50000]\n",
      "loss: 2.165564  [13604/50000]\n",
      "loss: 2.259192  [14004/50000]\n",
      "loss: 2.040903  [14404/50000]\n",
      "loss: 2.171181  [14804/50000]\n",
      "loss: 2.274037  [15204/50000]\n",
      "loss: 2.306626  [15604/50000]\n",
      "loss: 2.308835  [16004/50000]\n",
      "loss: 2.279017  [16404/50000]\n",
      "loss: 1.619155  [16804/50000]\n",
      "loss: 1.976542  [17204/50000]\n",
      "loss: 1.864908  [17604/50000]\n",
      "loss: 1.989331  [18004/50000]\n",
      "loss: 1.845772  [18404/50000]\n",
      "loss: 1.648620  [18804/50000]\n",
      "loss: 1.863638  [19204/50000]\n",
      "loss: 1.626925  [19604/50000]\n",
      "loss: 2.804256  [20004/50000]\n",
      "loss: 2.250903  [20404/50000]\n",
      "loss: 1.682173  [20804/50000]\n",
      "loss: 1.914391  [21204/50000]\n",
      "loss: 1.816120  [21604/50000]\n",
      "loss: 2.147530  [22004/50000]\n",
      "loss: 2.178807  [22404/50000]\n",
      "loss: 1.729538  [22804/50000]\n",
      "loss: 2.468033  [23204/50000]\n",
      "loss: 1.399118  [23604/50000]\n",
      "loss: 1.674791  [24004/50000]\n",
      "loss: 1.704622  [24404/50000]\n",
      "loss: 1.609046  [24804/50000]\n",
      "loss: 2.176864  [25204/50000]\n",
      "loss: 2.772710  [25604/50000]\n",
      "loss: 1.977082  [26004/50000]\n",
      "loss: 2.537587  [26404/50000]\n",
      "loss: 1.678391  [26804/50000]\n",
      "loss: 1.663048  [27204/50000]\n",
      "loss: 2.113569  [27604/50000]\n",
      "loss: 2.006526  [28004/50000]\n",
      "loss: 2.646389  [28404/50000]\n",
      "loss: 2.476793  [28804/50000]\n",
      "loss: 2.016177  [29204/50000]\n",
      "loss: 2.254189  [29604/50000]\n",
      "loss: 1.667915  [30004/50000]\n",
      "loss: 1.559543  [30404/50000]\n",
      "loss: 1.853159  [30804/50000]\n",
      "loss: 1.557153  [31204/50000]\n",
      "loss: 2.445044  [31604/50000]\n",
      "loss: 1.745165  [32004/50000]\n",
      "loss: 1.507603  [32404/50000]\n",
      "loss: 2.449259  [32804/50000]\n",
      "loss: 2.156895  [33204/50000]\n",
      "loss: 2.119225  [33604/50000]\n",
      "loss: 1.807281  [34004/50000]\n",
      "loss: 1.715489  [34404/50000]\n",
      "loss: 1.546072  [34804/50000]\n",
      "loss: 1.722772  [35204/50000]\n",
      "loss: 2.047076  [35604/50000]\n",
      "loss: 1.793576  [36004/50000]\n",
      "loss: 1.957266  [36404/50000]\n",
      "loss: 2.369075  [36804/50000]\n",
      "loss: 1.709203  [37204/50000]\n",
      "loss: 1.384340  [37604/50000]\n",
      "loss: 1.591700  [38004/50000]\n",
      "loss: 1.718258  [38404/50000]\n",
      "loss: 1.621754  [38804/50000]\n",
      "loss: 1.480752  [39204/50000]\n",
      "loss: 1.614482  [39604/50000]\n",
      "loss: 1.505169  [40004/50000]\n",
      "loss: 2.134086  [40404/50000]\n",
      "loss: 2.628758  [40804/50000]\n",
      "loss: 1.012112  [41204/50000]\n",
      "loss: 2.161239  [41604/50000]\n",
      "loss: 1.906419  [42004/50000]\n",
      "loss: 1.651091  [42404/50000]\n",
      "loss: 1.863615  [42804/50000]\n",
      "loss: 1.709827  [43204/50000]\n",
      "loss: 1.653462  [43604/50000]\n",
      "loss: 1.694487  [44004/50000]\n",
      "loss: 1.425665  [44404/50000]\n",
      "loss: 1.884413  [44804/50000]\n",
      "loss: 1.776222  [45204/50000]\n",
      "loss: 1.440193  [45604/50000]\n",
      "loss: 1.622352  [46004/50000]\n",
      "loss: 2.242028  [46404/50000]\n",
      "loss: 2.047845  [46804/50000]\n",
      "loss: 1.798197  [47204/50000]\n",
      "loss: 1.588511  [47604/50000]\n",
      "loss: 1.909690  [48004/50000]\n",
      "loss: 1.634902  [48404/50000]\n",
      "loss: 2.489983  [48804/50000]\n",
      "loss: 1.306658  [49204/50000]\n",
      "loss: 1.695948  [49604/50000]\n",
      "Test Error: \n",
      " Accuracy: 35.0%, Avg loss: 1.790047 \n",
      "\n",
      "Epoch 2 took 25.910405711038038 seconds\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.617802  [    4/50000]\n",
      "loss: 2.113297  [  404/50000]\n",
      "loss: 2.085808  [  804/50000]\n",
      "loss: 1.938645  [ 1204/50000]\n",
      "loss: 1.227327  [ 1604/50000]\n",
      "loss: 1.817974  [ 2004/50000]\n",
      "loss: 1.265576  [ 2404/50000]\n",
      "loss: 1.559834  [ 2804/50000]\n",
      "loss: 2.240376  [ 3204/50000]\n",
      "loss: 1.748387  [ 3604/50000]\n",
      "loss: 1.652507  [ 4004/50000]\n",
      "loss: 1.678970  [ 4404/50000]\n",
      "loss: 1.756174  [ 4804/50000]\n",
      "loss: 1.457538  [ 5204/50000]\n",
      "loss: 1.362498  [ 5604/50000]\n",
      "loss: 1.479893  [ 6004/50000]\n",
      "loss: 1.616642  [ 6404/50000]\n",
      "loss: 2.177735  [ 6804/50000]\n",
      "loss: 1.702680  [ 7204/50000]\n",
      "loss: 1.765145  [ 7604/50000]\n",
      "loss: 1.612266  [ 8004/50000]\n",
      "loss: 2.084001  [ 8404/50000]\n",
      "loss: 2.539800  [ 8804/50000]\n",
      "loss: 2.004314  [ 9204/50000]\n",
      "loss: 1.821058  [ 9604/50000]\n",
      "loss: 1.824412  [10004/50000]\n",
      "loss: 0.910191  [10404/50000]\n",
      "loss: 1.793304  [10804/50000]\n",
      "loss: 2.422447  [11204/50000]\n",
      "loss: 2.102745  [11604/50000]\n",
      "loss: 2.468339  [12004/50000]\n",
      "loss: 1.803384  [12404/50000]\n",
      "loss: 2.070907  [12804/50000]\n",
      "loss: 1.775889  [13204/50000]\n",
      "loss: 2.067845  [13604/50000]\n",
      "loss: 2.258690  [14004/50000]\n",
      "loss: 1.863279  [14404/50000]\n",
      "loss: 1.949183  [14804/50000]\n",
      "loss: 2.088263  [15204/50000]\n",
      "loss: 2.494277  [15604/50000]\n",
      "loss: 2.232187  [16004/50000]\n",
      "loss: 1.818019  [16404/50000]\n",
      "loss: 1.610361  [16804/50000]\n",
      "loss: 2.125104  [17204/50000]\n",
      "loss: 1.573539  [17604/50000]\n",
      "loss: 1.812073  [18004/50000]\n",
      "loss: 1.755463  [18404/50000]\n",
      "loss: 1.290803  [18804/50000]\n",
      "loss: 1.264784  [19204/50000]\n",
      "loss: 1.288883  [19604/50000]\n",
      "loss: 2.688569  [20004/50000]\n",
      "loss: 1.942710  [20404/50000]\n",
      "loss: 1.517259  [20804/50000]\n",
      "loss: 1.689050  [21204/50000]\n",
      "loss: 1.308305  [21604/50000]\n",
      "loss: 1.672644  [22004/50000]\n",
      "loss: 2.145552  [22404/50000]\n",
      "loss: 1.716608  [22804/50000]\n",
      "loss: 2.484789  [23204/50000]\n",
      "loss: 1.182771  [23604/50000]\n",
      "loss: 1.137237  [24004/50000]\n",
      "loss: 1.170849  [24404/50000]\n",
      "loss: 1.359322  [24804/50000]\n",
      "loss: 1.506094  [25204/50000]\n",
      "loss: 1.961829  [25604/50000]\n",
      "loss: 1.662377  [26004/50000]\n",
      "loss: 2.633161  [26404/50000]\n",
      "loss: 1.500054  [26804/50000]\n",
      "loss: 1.559423  [27204/50000]\n",
      "loss: 2.449963  [27604/50000]\n",
      "loss: 2.109899  [28004/50000]\n",
      "loss: 2.897360  [28404/50000]\n",
      "loss: 2.239818  [28804/50000]\n",
      "loss: 1.892913  [29204/50000]\n",
      "loss: 1.581287  [29604/50000]\n",
      "loss: 1.701877  [30004/50000]\n",
      "loss: 1.466901  [30404/50000]\n",
      "loss: 1.413033  [30804/50000]\n",
      "loss: 1.378854  [31204/50000]\n",
      "loss: 2.327143  [31604/50000]\n",
      "loss: 1.372510  [32004/50000]\n",
      "loss: 1.132100  [32404/50000]\n",
      "loss: 2.240085  [32804/50000]\n",
      "loss: 1.950540  [33204/50000]\n",
      "loss: 1.869469  [33604/50000]\n",
      "loss: 1.634950  [34004/50000]\n",
      "loss: 1.285635  [34404/50000]\n",
      "loss: 1.437645  [34804/50000]\n",
      "loss: 1.571627  [35204/50000]\n",
      "loss: 1.765072  [35604/50000]\n",
      "loss: 1.645069  [36004/50000]\n",
      "loss: 1.936182  [36404/50000]\n",
      "loss: 1.313644  [36804/50000]\n",
      "loss: 1.218625  [37204/50000]\n",
      "loss: 1.014088  [37604/50000]\n",
      "loss: 1.178723  [38004/50000]\n",
      "loss: 1.542866  [38404/50000]\n",
      "loss: 1.324522  [38804/50000]\n",
      "loss: 1.798463  [39204/50000]\n",
      "loss: 1.335420  [39604/50000]\n",
      "loss: 0.949586  [40004/50000]\n",
      "loss: 2.178612  [40404/50000]\n",
      "loss: 2.095113  [40804/50000]\n",
      "loss: 0.968320  [41204/50000]\n",
      "loss: 1.662709  [41604/50000]\n",
      "loss: 1.700479  [42004/50000]\n",
      "loss: 1.235394  [42404/50000]\n",
      "loss: 1.712447  [42804/50000]\n",
      "loss: 1.584682  [43204/50000]\n",
      "loss: 1.485755  [43604/50000]\n",
      "loss: 1.657251  [44004/50000]\n",
      "loss: 1.152632  [44404/50000]\n",
      "loss: 1.718234  [44804/50000]\n",
      "loss: 1.615847  [45204/50000]\n",
      "loss: 1.223268  [45604/50000]\n",
      "loss: 1.477147  [46004/50000]\n",
      "loss: 2.138575  [46404/50000]\n",
      "loss: 1.823680  [46804/50000]\n",
      "loss: 1.978885  [47204/50000]\n",
      "loss: 1.245967  [47604/50000]\n",
      "loss: 2.594063  [48004/50000]\n",
      "loss: 1.633543  [48404/50000]\n",
      "loss: 1.996421  [48804/50000]\n",
      "loss: 1.077442  [49204/50000]\n",
      "loss: 1.392504  [49604/50000]\n",
      "Test Error: \n",
      " Accuracy: 41.3%, Avg loss: 1.596128 \n",
      "\n",
      "Epoch 3 took 25.935430436045863 seconds\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "test_dataloader = DataLoader(test_data, batch_size=4)\n",
    "\n",
    "\n",
    "epochs = 3\n",
    "for datasize in [7500,15000,30000,50000]:\n",
    "    model = CifarNetwork().to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        subset_training_data = Subset(training_data, range(datasize))\n",
    "        train_dataloader_subset = DataLoader(subset_training_data, batch_size=batch_size)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "        epoch_time_start = time.perf_counter()\n",
    "        train(train_dataloader_subset, model, loss_fn, optimizer)\n",
    "        epoch_time_end = time.perf_counter()\n",
    "\n",
    "        test_start_time = epoch_time_end\n",
    "        accuracy = test(test_dataloader, model, loss_fn)\n",
    "        test_end_time = time.perf_counter()\n",
    "\n",
    "        epoch_duration = epoch_time_end - epoch_time_start\n",
    "        test_duration = test_end_time - test_start_time\n",
    "\n",
    "        print(f\"Epoch {epoch+1} took {epoch_duration} seconds\")\n",
    "        with open(\"datasize_cifar_nn.csv\", \"a\") as fp:\n",
    "            wr = csv.writer(fp, dialect='excel')\n",
    "            # epoch_duration, epoch, batch_size, data_size, accuracy, test_duration\n",
    "            wr.writerow([epoch_duration, epoch, batch_size, datasize, accuracy, test_duration])\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"cifar_model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for NeuralNetwork:\n\tMissing key(s) in state_dict: \"linear_relu_stack.0.weight\", \"linear_relu_stack.0.bias\", \"linear_relu_stack.2.weight\", \"linear_relu_stack.2.bias\", \"linear_relu_stack.4.weight\", \"linear_relu_stack.4.bias\". \n\tUnexpected key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m NeuralNetwork()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/chz_sok_experiments/chz-sok-nn-experiments/crypten_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for NeuralNetwork:\n\tMissing key(s) in state_dict: \"linear_relu_stack.0.weight\", \"linear_relu_stack.0.bias\", \"linear_relu_stack.2.weight\", \"linear_relu_stack.2.bias\", \"linear_relu_stack.4.weight\", \"linear_relu_stack.4.bias\". \n\tUnexpected key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\". "
     ]
    }
   ],
   "source": [
    "model = CifarNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"cifar_model.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralnet_experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
