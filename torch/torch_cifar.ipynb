{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor, PILToTensor, Compose, Normalize\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import csv\n",
    "torch.set_num_threads(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    # transform=PILToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "class CifarNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X.float())\n",
    "        # print(\"pred: \",pred.shape)\n",
    "        # print(\"y:\",y.shape)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = Compose(\n",
    "    [\n",
    "        ToTensor(),\n",
    "        Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Download training data from open datasets.\n",
    "training_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    # transform=PILToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.313524  [    4/50000]\n",
      "loss: 2.270159  [  404/50000]\n",
      "loss: 2.288055  [  804/50000]\n",
      "loss: 2.293899  [ 1204/50000]\n",
      "loss: 2.325410  [ 1604/50000]\n",
      "loss: 2.304053  [ 2004/50000]\n",
      "loss: 2.274299  [ 2404/50000]\n",
      "loss: 2.324052  [ 2804/50000]\n",
      "loss: 2.316264  [ 3204/50000]\n",
      "loss: 2.344781  [ 3604/50000]\n",
      "loss: 2.306680  [ 4004/50000]\n",
      "loss: 2.276045  [ 4404/50000]\n",
      "loss: 2.313386  [ 4804/50000]\n",
      "loss: 2.334394  [ 5204/50000]\n",
      "loss: 2.297205  [ 5604/50000]\n",
      "loss: 2.309811  [ 6004/50000]\n",
      "loss: 2.281672  [ 6404/50000]\n",
      "loss: 2.273612  [ 6804/50000]\n",
      "loss: 2.285515  [ 7204/50000]\n",
      "loss: 2.324778  [ 7604/50000]\n",
      "loss: 2.328305  [ 8004/50000]\n",
      "loss: 2.309227  [ 8404/50000]\n",
      "loss: 2.307688  [ 8804/50000]\n",
      "loss: 2.265779  [ 9204/50000]\n",
      "loss: 2.323225  [ 9604/50000]\n",
      "loss: 2.300496  [10004/50000]\n",
      "loss: 2.294869  [10404/50000]\n",
      "loss: 2.311247  [10804/50000]\n",
      "loss: 2.284938  [11204/50000]\n",
      "loss: 2.321894  [11604/50000]\n",
      "loss: 2.312114  [12004/50000]\n",
      "loss: 2.263813  [12404/50000]\n",
      "loss: 2.309257  [12804/50000]\n",
      "loss: 2.298076  [13204/50000]\n",
      "loss: 2.292076  [13604/50000]\n",
      "loss: 2.292429  [14004/50000]\n",
      "loss: 2.283114  [14404/50000]\n",
      "loss: 2.305643  [14804/50000]\n",
      "loss: 2.271184  [15204/50000]\n",
      "loss: 2.318242  [15604/50000]\n",
      "loss: 2.301028  [16004/50000]\n",
      "loss: 2.315570  [16404/50000]\n",
      "loss: 2.297789  [16804/50000]\n",
      "loss: 2.300474  [17204/50000]\n",
      "loss: 2.288793  [17604/50000]\n",
      "loss: 2.312898  [18004/50000]\n",
      "loss: 2.267788  [18404/50000]\n",
      "loss: 2.268070  [18804/50000]\n",
      "loss: 2.301301  [19204/50000]\n",
      "loss: 2.290756  [19604/50000]\n",
      "loss: 2.344262  [20004/50000]\n",
      "loss: 2.306407  [20404/50000]\n",
      "loss: 2.259551  [20804/50000]\n",
      "loss: 2.263346  [21204/50000]\n",
      "loss: 2.279902  [21604/50000]\n",
      "loss: 2.309713  [22004/50000]\n",
      "loss: 2.301401  [22404/50000]\n",
      "loss: 2.287549  [22804/50000]\n",
      "loss: 2.271358  [23204/50000]\n",
      "loss: 2.234629  [23604/50000]\n",
      "loss: 2.250742  [24004/50000]\n",
      "loss: 2.245142  [24404/50000]\n",
      "loss: 2.257605  [24804/50000]\n",
      "loss: 2.304949  [25204/50000]\n",
      "loss: 2.288852  [25604/50000]\n",
      "loss: 2.306609  [26004/50000]\n",
      "loss: 2.305519  [26404/50000]\n",
      "loss: 2.256403  [26804/50000]\n",
      "loss: 2.252797  [27204/50000]\n",
      "loss: 2.193410  [27604/50000]\n",
      "loss: 2.259393  [28004/50000]\n",
      "loss: 2.514931  [28404/50000]\n",
      "loss: 2.353153  [28804/50000]\n",
      "loss: 2.180861  [29204/50000]\n",
      "loss: 2.495147  [29604/50000]\n",
      "loss: 2.315675  [30004/50000]\n",
      "loss: 2.217551  [30404/50000]\n",
      "loss: 2.250232  [30804/50000]\n",
      "loss: 2.249166  [31204/50000]\n",
      "loss: 2.351725  [31604/50000]\n",
      "loss: 2.187094  [32004/50000]\n",
      "loss: 2.261446  [32404/50000]\n",
      "loss: 2.431430  [32804/50000]\n",
      "loss: 2.234818  [33204/50000]\n",
      "loss: 2.324707  [33604/50000]\n",
      "loss: 2.259831  [34004/50000]\n",
      "loss: 2.202968  [34404/50000]\n",
      "loss: 1.986565  [34804/50000]\n",
      "loss: 2.068605  [35204/50000]\n",
      "loss: 2.473871  [35604/50000]\n",
      "loss: 2.080593  [36004/50000]\n",
      "loss: 2.235058  [36404/50000]\n",
      "loss: 2.361797  [36804/50000]\n",
      "loss: 1.841282  [37204/50000]\n",
      "loss: 1.933740  [37604/50000]\n",
      "loss: 2.023474  [38004/50000]\n",
      "loss: 2.071563  [38404/50000]\n",
      "loss: 1.846109  [38804/50000]\n",
      "loss: 1.746320  [39204/50000]\n",
      "loss: 2.062445  [39604/50000]\n",
      "loss: 1.869277  [40004/50000]\n",
      "loss: 2.137028  [40404/50000]\n",
      "loss: 2.881180  [40804/50000]\n",
      "loss: 1.778254  [41204/50000]\n",
      "loss: 2.409314  [41604/50000]\n",
      "loss: 2.039559  [42004/50000]\n",
      "loss: 2.172217  [42404/50000]\n",
      "loss: 2.018269  [42804/50000]\n",
      "loss: 1.888155  [43204/50000]\n",
      "loss: 1.811828  [43604/50000]\n",
      "loss: 1.908945  [44004/50000]\n",
      "loss: 1.938882  [44404/50000]\n",
      "loss: 2.089050  [44804/50000]\n",
      "loss: 2.170902  [45204/50000]\n",
      "loss: 1.671546  [45604/50000]\n",
      "loss: 1.960238  [46004/50000]\n",
      "loss: 2.107355  [46404/50000]\n",
      "loss: 2.307967  [46804/50000]\n",
      "loss: 1.928214  [47204/50000]\n",
      "loss: 2.044113  [47604/50000]\n",
      "loss: 1.823912  [48004/50000]\n",
      "loss: 1.923013  [48404/50000]\n",
      "loss: 2.588043  [48804/50000]\n",
      "loss: 1.706074  [49204/50000]\n",
      "loss: 1.765696  [49604/50000]\n",
      "Test Error: \n",
      " Accuracy: 28.6%, Avg loss: 1.992194 \n",
      "\n",
      "Epoch 1 took 25.675968174939044 seconds\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.620201  [    4/50000]\n",
      "loss: 2.210566  [  404/50000]\n",
      "loss: 2.189333  [  804/50000]\n",
      "loss: 2.148813  [ 1204/50000]\n",
      "loss: 1.508588  [ 1604/50000]\n",
      "loss: 1.830748  [ 2004/50000]\n",
      "loss: 1.548936  [ 2404/50000]\n",
      "loss: 1.669400  [ 2804/50000]\n",
      "loss: 2.376079  [ 3204/50000]\n",
      "loss: 1.589311  [ 3604/50000]\n",
      "loss: 1.750474  [ 4004/50000]\n",
      "loss: 1.856241  [ 4404/50000]\n",
      "loss: 1.920117  [ 4804/50000]\n",
      "loss: 2.119507  [ 5204/50000]\n",
      "loss: 1.509499  [ 5604/50000]\n",
      "loss: 1.667408  [ 6004/50000]\n",
      "loss: 1.659680  [ 6404/50000]\n",
      "loss: 2.168604  [ 6804/50000]\n",
      "loss: 2.245175  [ 7204/50000]\n",
      "loss: 2.206446  [ 7604/50000]\n",
      "loss: 1.814118  [ 8004/50000]\n",
      "loss: 2.386961  [ 8404/50000]\n",
      "loss: 2.184028  [ 8804/50000]\n",
      "loss: 2.009807  [ 9204/50000]\n",
      "loss: 2.191037  [ 9604/50000]\n",
      "loss: 2.544877  [10004/50000]\n",
      "loss: 1.282982  [10404/50000]\n",
      "loss: 1.984251  [10804/50000]\n",
      "loss: 2.198255  [11204/50000]\n",
      "loss: 1.978202  [11604/50000]\n",
      "loss: 1.764432  [12004/50000]\n",
      "loss: 1.514673  [12404/50000]\n",
      "loss: 1.997469  [12804/50000]\n",
      "loss: 2.041359  [13204/50000]\n",
      "loss: 1.916984  [13604/50000]\n",
      "loss: 2.441479  [14004/50000]\n",
      "loss: 1.789545  [14404/50000]\n",
      "loss: 1.901834  [14804/50000]\n",
      "loss: 2.221178  [15204/50000]\n",
      "loss: 2.441368  [15604/50000]\n",
      "loss: 2.094326  [16004/50000]\n",
      "loss: 2.454098  [16404/50000]\n",
      "loss: 1.577153  [16804/50000]\n",
      "loss: 1.887728  [17204/50000]\n",
      "loss: 1.457049  [17604/50000]\n",
      "loss: 1.821062  [18004/50000]\n",
      "loss: 1.640025  [18404/50000]\n",
      "loss: 1.355648  [18804/50000]\n",
      "loss: 1.774378  [19204/50000]\n",
      "loss: 1.440576  [19604/50000]\n",
      "loss: 3.035667  [20004/50000]\n",
      "loss: 2.127132  [20404/50000]\n",
      "loss: 1.025830  [20804/50000]\n",
      "loss: 2.054511  [21204/50000]\n",
      "loss: 1.568188  [21604/50000]\n",
      "loss: 1.779648  [22004/50000]\n",
      "loss: 2.052694  [22404/50000]\n",
      "loss: 2.203878  [22804/50000]\n",
      "loss: 2.628743  [23204/50000]\n",
      "loss: 1.280525  [23604/50000]\n",
      "loss: 1.453092  [24004/50000]\n",
      "loss: 1.489661  [24404/50000]\n",
      "loss: 1.608696  [24804/50000]\n",
      "loss: 2.190506  [25204/50000]\n",
      "loss: 2.835793  [25604/50000]\n",
      "loss: 1.933974  [26004/50000]\n",
      "loss: 2.249460  [26404/50000]\n",
      "loss: 1.523659  [26804/50000]\n",
      "loss: 1.628389  [27204/50000]\n",
      "loss: 1.992862  [27604/50000]\n",
      "loss: 1.991814  [28004/50000]\n",
      "loss: 3.183121  [28404/50000]\n",
      "loss: 2.397468  [28804/50000]\n",
      "loss: 1.810126  [29204/50000]\n",
      "loss: 2.033267  [29604/50000]\n",
      "loss: 1.536598  [30004/50000]\n",
      "loss: 1.451050  [30404/50000]\n",
      "loss: 1.746846  [30804/50000]\n",
      "loss: 1.761781  [31204/50000]\n",
      "loss: 2.464736  [31604/50000]\n",
      "loss: 1.682889  [32004/50000]\n",
      "loss: 1.650314  [32404/50000]\n",
      "loss: 2.165820  [32804/50000]\n",
      "loss: 2.034296  [33204/50000]\n",
      "loss: 2.251498  [33604/50000]\n",
      "loss: 2.076197  [34004/50000]\n",
      "loss: 1.580654  [34404/50000]\n",
      "loss: 1.426383  [34804/50000]\n",
      "loss: 1.691461  [35204/50000]\n",
      "loss: 2.124775  [35604/50000]\n",
      "loss: 1.540072  [36004/50000]\n",
      "loss: 1.884291  [36404/50000]\n",
      "loss: 2.245219  [36804/50000]\n",
      "loss: 1.446568  [37204/50000]\n",
      "loss: 1.388296  [37604/50000]\n",
      "loss: 1.361092  [38004/50000]\n",
      "loss: 1.846541  [38404/50000]\n",
      "loss: 1.627239  [38804/50000]\n",
      "loss: 1.381124  [39204/50000]\n",
      "loss: 1.458790  [39604/50000]\n",
      "loss: 1.386376  [40004/50000]\n",
      "loss: 2.007904  [40404/50000]\n",
      "loss: 2.475245  [40804/50000]\n",
      "loss: 0.927092  [41204/50000]\n",
      "loss: 2.048641  [41604/50000]\n",
      "loss: 1.886511  [42004/50000]\n",
      "loss: 1.676689  [42404/50000]\n",
      "loss: 1.920876  [42804/50000]\n",
      "loss: 1.620104  [43204/50000]\n",
      "loss: 1.723592  [43604/50000]\n",
      "loss: 1.775542  [44004/50000]\n",
      "loss: 1.385757  [44404/50000]\n",
      "loss: 1.783800  [44804/50000]\n",
      "loss: 1.921397  [45204/50000]\n",
      "loss: 1.361767  [45604/50000]\n",
      "loss: 1.528455  [46004/50000]\n",
      "loss: 2.311335  [46404/50000]\n",
      "loss: 1.982400  [46804/50000]\n",
      "loss: 1.880700  [47204/50000]\n",
      "loss: 1.663544  [47604/50000]\n",
      "loss: 1.816842  [48004/50000]\n",
      "loss: 1.669436  [48404/50000]\n",
      "loss: 2.338529  [48804/50000]\n",
      "loss: 1.269055  [49204/50000]\n",
      "loss: 1.771428  [49604/50000]\n",
      "Test Error: \n",
      " Accuracy: 36.7%, Avg loss: 1.740342 \n",
      "\n",
      "Epoch 2 took 25.627358533092774 seconds\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.526657  [    4/50000]\n",
      "loss: 2.210052  [  404/50000]\n",
      "loss: 2.002135  [  804/50000]\n",
      "loss: 2.235496  [ 1204/50000]\n",
      "loss: 1.198614  [ 1604/50000]\n",
      "loss: 1.761040  [ 2004/50000]\n",
      "loss: 1.328274  [ 2404/50000]\n",
      "loss: 1.457469  [ 2804/50000]\n",
      "loss: 2.179502  [ 3204/50000]\n",
      "loss: 1.611639  [ 3604/50000]\n",
      "loss: 1.298636  [ 4004/50000]\n",
      "loss: 1.597065  [ 4404/50000]\n",
      "loss: 1.477136  [ 4804/50000]\n",
      "loss: 1.368725  [ 5204/50000]\n",
      "loss: 1.189053  [ 5604/50000]\n",
      "loss: 1.337620  [ 6004/50000]\n",
      "loss: 1.648614  [ 6404/50000]\n",
      "loss: 1.828971  [ 6804/50000]\n",
      "loss: 1.914545  [ 7204/50000]\n",
      "loss: 1.758837  [ 7604/50000]\n",
      "loss: 1.450709  [ 8004/50000]\n",
      "loss: 2.334194  [ 8404/50000]\n",
      "loss: 2.537095  [ 8804/50000]\n",
      "loss: 1.677566  [ 9204/50000]\n",
      "loss: 1.789539  [ 9604/50000]\n",
      "loss: 2.087145  [10004/50000]\n",
      "loss: 0.925170  [10404/50000]\n",
      "loss: 1.592767  [10804/50000]\n",
      "loss: 2.427805  [11204/50000]\n",
      "loss: 2.060631  [11604/50000]\n",
      "loss: 2.240545  [12004/50000]\n",
      "loss: 1.330169  [12404/50000]\n",
      "loss: 1.834695  [12804/50000]\n",
      "loss: 1.946051  [13204/50000]\n",
      "loss: 1.961928  [13604/50000]\n",
      "loss: 2.315762  [14004/50000]\n",
      "loss: 1.545097  [14404/50000]\n",
      "loss: 2.101295  [14804/50000]\n",
      "loss: 1.976554  [15204/50000]\n",
      "loss: 2.041547  [15604/50000]\n",
      "loss: 2.254243  [16004/50000]\n",
      "loss: 1.882016  [16404/50000]\n",
      "loss: 1.580014  [16804/50000]\n",
      "loss: 2.300991  [17204/50000]\n",
      "loss: 1.337204  [17604/50000]\n",
      "loss: 1.974702  [18004/50000]\n",
      "loss: 1.453366  [18404/50000]\n",
      "loss: 1.232596  [18804/50000]\n",
      "loss: 1.105311  [19204/50000]\n",
      "loss: 1.136063  [19604/50000]\n",
      "loss: 2.782051  [20004/50000]\n",
      "loss: 1.836360  [20404/50000]\n",
      "loss: 1.482045  [20804/50000]\n",
      "loss: 1.712747  [21204/50000]\n",
      "loss: 1.194842  [21604/50000]\n",
      "loss: 1.464655  [22004/50000]\n",
      "loss: 1.658826  [22404/50000]\n",
      "loss: 1.776366  [22804/50000]\n",
      "loss: 2.473782  [23204/50000]\n",
      "loss: 1.109369  [23604/50000]\n",
      "loss: 1.264775  [24004/50000]\n",
      "loss: 1.224222  [24404/50000]\n",
      "loss: 1.597930  [24804/50000]\n",
      "loss: 1.503309  [25204/50000]\n",
      "loss: 2.055772  [25604/50000]\n",
      "loss: 1.487187  [26004/50000]\n",
      "loss: 2.208409  [26404/50000]\n",
      "loss: 1.513083  [26804/50000]\n",
      "loss: 2.066270  [27204/50000]\n",
      "loss: 2.577407  [27604/50000]\n",
      "loss: 2.065928  [28004/50000]\n",
      "loss: 2.972547  [28404/50000]\n",
      "loss: 2.232166  [28804/50000]\n",
      "loss: 1.967025  [29204/50000]\n",
      "loss: 1.192326  [29604/50000]\n",
      "loss: 1.656845  [30004/50000]\n",
      "loss: 1.524707  [30404/50000]\n",
      "loss: 1.529225  [30804/50000]\n",
      "loss: 1.459813  [31204/50000]\n",
      "loss: 2.195130  [31604/50000]\n",
      "loss: 1.320061  [32004/50000]\n",
      "loss: 1.526392  [32404/50000]\n",
      "loss: 2.039873  [32804/50000]\n",
      "loss: 1.688099  [33204/50000]\n",
      "loss: 1.719771  [33604/50000]\n",
      "loss: 1.953393  [34004/50000]\n",
      "loss: 1.161632  [34404/50000]\n",
      "loss: 1.513266  [34804/50000]\n",
      "loss: 1.553325  [35204/50000]\n",
      "loss: 1.804131  [35604/50000]\n",
      "loss: 1.460588  [36004/50000]\n",
      "loss: 1.876170  [36404/50000]\n",
      "loss: 1.330266  [36804/50000]\n",
      "loss: 0.928455  [37204/50000]\n",
      "loss: 1.132186  [37604/50000]\n",
      "loss: 1.101825  [38004/50000]\n",
      "loss: 1.360765  [38404/50000]\n",
      "loss: 1.336540  [38804/50000]\n",
      "loss: 1.452632  [39204/50000]\n",
      "loss: 1.153823  [39604/50000]\n",
      "loss: 0.883822  [40004/50000]\n",
      "loss: 2.168191  [40404/50000]\n",
      "loss: 2.325424  [40804/50000]\n",
      "loss: 0.732593  [41204/50000]\n",
      "loss: 1.675298  [41604/50000]\n",
      "loss: 1.628951  [42004/50000]\n",
      "loss: 1.314602  [42404/50000]\n",
      "loss: 1.738221  [42804/50000]\n",
      "loss: 1.564577  [43204/50000]\n",
      "loss: 1.330365  [43604/50000]\n",
      "loss: 1.763547  [44004/50000]\n",
      "loss: 1.088046  [44404/50000]\n",
      "loss: 1.740775  [44804/50000]\n",
      "loss: 1.584342  [45204/50000]\n",
      "loss: 1.060494  [45604/50000]\n",
      "loss: 1.493253  [46004/50000]\n",
      "loss: 2.079179  [46404/50000]\n",
      "loss: 1.637436  [46804/50000]\n",
      "loss: 2.198403  [47204/50000]\n",
      "loss: 1.351469  [47604/50000]\n",
      "loss: 2.557832  [48004/50000]\n",
      "loss: 1.472594  [48404/50000]\n",
      "loss: 1.882817  [48804/50000]\n",
      "loss: 0.998277  [49204/50000]\n",
      "loss: 1.676809  [49604/50000]\n",
      "Test Error: \n",
      " Accuracy: 43.1%, Avg loss: 1.555706 \n",
      "\n",
      "Epoch 3 took 25.6116203579586 seconds\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.298180  [    8/50000]\n",
      "loss: 2.329652  [  808/50000]\n",
      "loss: 2.257122  [ 1608/50000]\n",
      "loss: 2.288721  [ 2408/50000]\n",
      "loss: 2.272850  [ 3208/50000]\n",
      "loss: 2.305509  [ 4008/50000]\n",
      "loss: 2.341621  [ 4808/50000]\n",
      "loss: 2.280161  [ 5608/50000]\n",
      "loss: 2.337011  [ 6408/50000]\n",
      "loss: 2.321339  [ 7208/50000]\n",
      "loss: 2.333628  [ 8008/50000]\n",
      "loss: 2.285545  [ 8808/50000]\n",
      "loss: 2.264444  [ 9608/50000]\n",
      "loss: 2.288113  [10408/50000]\n",
      "loss: 2.350236  [11208/50000]\n",
      "loss: 2.277050  [12008/50000]\n",
      "loss: 2.297816  [12808/50000]\n",
      "loss: 2.323780  [13608/50000]\n",
      "loss: 2.305931  [14408/50000]\n",
      "loss: 2.306068  [15208/50000]\n",
      "loss: 2.287449  [16008/50000]\n",
      "loss: 2.287116  [16808/50000]\n",
      "loss: 2.299632  [17608/50000]\n",
      "loss: 2.303125  [18408/50000]\n",
      "loss: 2.319739  [19208/50000]\n",
      "loss: 2.308263  [20008/50000]\n",
      "loss: 2.287035  [20808/50000]\n",
      "loss: 2.283992  [21608/50000]\n",
      "loss: 2.343891  [22408/50000]\n",
      "loss: 2.330309  [23208/50000]\n",
      "loss: 2.307699  [24008/50000]\n",
      "loss: 2.295332  [24808/50000]\n",
      "loss: 2.287601  [25608/50000]\n",
      "loss: 2.328956  [26408/50000]\n",
      "loss: 2.294295  [27208/50000]\n",
      "loss: 2.280583  [28008/50000]\n",
      "loss: 2.331246  [28808/50000]\n",
      "loss: 2.295679  [29608/50000]\n",
      "loss: 2.308998  [30408/50000]\n",
      "loss: 2.291655  [31208/50000]\n",
      "loss: 2.307721  [32008/50000]\n",
      "loss: 2.302726  [32808/50000]\n",
      "loss: 2.308982  [33608/50000]\n",
      "loss: 2.316004  [34408/50000]\n",
      "loss: 2.272931  [35208/50000]\n",
      "loss: 2.294625  [36008/50000]\n",
      "loss: 2.338294  [36808/50000]\n",
      "loss: 2.269234  [37608/50000]\n",
      "loss: 2.278164  [38408/50000]\n",
      "loss: 2.294240  [39208/50000]\n",
      "loss: 2.312873  [40008/50000]\n",
      "loss: 2.279587  [40808/50000]\n",
      "loss: 2.283891  [41608/50000]\n",
      "loss: 2.275938  [42408/50000]\n",
      "loss: 2.287365  [43208/50000]\n",
      "loss: 2.305645  [44008/50000]\n",
      "loss: 2.299951  [44808/50000]\n",
      "loss: 2.278798  [45608/50000]\n",
      "loss: 2.301797  [46408/50000]\n",
      "loss: 2.298444  [47208/50000]\n",
      "loss: 2.286197  [48008/50000]\n",
      "loss: 2.309470  [48808/50000]\n",
      "loss: 2.302498  [49608/50000]\n",
      "Test Error: \n",
      " Accuracy: 14.4%, Avg loss: 2.299083 \n",
      "\n",
      "Epoch 1 took 19.140137501992285 seconds\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.290483  [    8/50000]\n",
      "loss: 2.311636  [  808/50000]\n",
      "loss: 2.271117  [ 1608/50000]\n",
      "loss: 2.290276  [ 2408/50000]\n",
      "loss: 2.288371  [ 3208/50000]\n",
      "loss: 2.295098  [ 4008/50000]\n",
      "loss: 2.311339  [ 4808/50000]\n",
      "loss: 2.283422  [ 5608/50000]\n",
      "loss: 2.308631  [ 6408/50000]\n",
      "loss: 2.306520  [ 7208/50000]\n",
      "loss: 2.305200  [ 8008/50000]\n",
      "loss: 2.292123  [ 8808/50000]\n",
      "loss: 2.275599  [ 9608/50000]\n",
      "loss: 2.287419  [10408/50000]\n",
      "loss: 2.319959  [11208/50000]\n",
      "loss: 2.284299  [12008/50000]\n",
      "loss: 2.295394  [12808/50000]\n",
      "loss: 2.304008  [13608/50000]\n",
      "loss: 2.302508  [14408/50000]\n",
      "loss: 2.293805  [15208/50000]\n",
      "loss: 2.291042  [16008/50000]\n",
      "loss: 2.290365  [16808/50000]\n",
      "loss: 2.287193  [17608/50000]\n",
      "loss: 2.291628  [18408/50000]\n",
      "loss: 2.306169  [19208/50000]\n",
      "loss: 2.310358  [20008/50000]\n",
      "loss: 2.278220  [20808/50000]\n",
      "loss: 2.283150  [21608/50000]\n",
      "loss: 2.312476  [22408/50000]\n",
      "loss: 2.319585  [23208/50000]\n",
      "loss: 2.294180  [24008/50000]\n",
      "loss: 2.279809  [24808/50000]\n",
      "loss: 2.301740  [25608/50000]\n",
      "loss: 2.303975  [26408/50000]\n",
      "loss: 2.282989  [27208/50000]\n",
      "loss: 2.282724  [28008/50000]\n",
      "loss: 2.305290  [28808/50000]\n",
      "loss: 2.293409  [29608/50000]\n",
      "loss: 2.297733  [30408/50000]\n",
      "loss: 2.280735  [31208/50000]\n",
      "loss: 2.285056  [32008/50000]\n",
      "loss: 2.296429  [32808/50000]\n",
      "loss: 2.291331  [33608/50000]\n",
      "loss: 2.306612  [34408/50000]\n",
      "loss: 2.263810  [35208/50000]\n",
      "loss: 2.276045  [36008/50000]\n",
      "loss: 2.299806  [36808/50000]\n",
      "loss: 2.250235  [37608/50000]\n",
      "loss: 2.252199  [38408/50000]\n",
      "loss: 2.224416  [39208/50000]\n",
      "loss: 2.285049  [40008/50000]\n",
      "loss: 2.309543  [40808/50000]\n",
      "loss: 2.296629  [41608/50000]\n",
      "loss: 2.240437  [42408/50000]\n",
      "loss: 2.250361  [43208/50000]\n",
      "loss: 2.271272  [44008/50000]\n",
      "loss: 2.234443  [44808/50000]\n",
      "loss: 2.203203  [45608/50000]\n",
      "loss: 2.246451  [46408/50000]\n",
      "loss: 2.240431  [47208/50000]\n",
      "loss: 2.221152  [48008/50000]\n",
      "loss: 2.205790  [48808/50000]\n",
      "loss: 2.198591  [49608/50000]\n",
      "Test Error: \n",
      " Accuracy: 21.6%, Avg loss: 2.239722 \n",
      "\n",
      "Epoch 2 took 19.115825241897255 seconds\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.196200  [    8/50000]\n",
      "loss: 2.232049  [  808/50000]\n",
      "loss: 2.154627  [ 1608/50000]\n",
      "loss: 2.112745  [ 2408/50000]\n",
      "loss: 2.246747  [ 3208/50000]\n",
      "loss: 2.206777  [ 4008/50000]\n",
      "loss: 2.167287  [ 4808/50000]\n",
      "loss: 2.176748  [ 5608/50000]\n",
      "loss: 2.082536  [ 6408/50000]\n",
      "loss: 2.198628  [ 7208/50000]\n",
      "loss: 2.059515  [ 8008/50000]\n",
      "loss: 2.322545  [ 8808/50000]\n",
      "loss: 2.167172  [ 9608/50000]\n",
      "loss: 2.035918  [10408/50000]\n",
      "loss: 2.167262  [11208/50000]\n",
      "loss: 2.173868  [12008/50000]\n",
      "loss: 2.193302  [12808/50000]\n",
      "loss: 2.027806  [13608/50000]\n",
      "loss: 2.129658  [14408/50000]\n",
      "loss: 2.098465  [15208/50000]\n",
      "loss: 2.165750  [16008/50000]\n",
      "loss: 2.183597  [16808/50000]\n",
      "loss: 1.999414  [17608/50000]\n",
      "loss: 1.943786  [18408/50000]\n",
      "loss: 2.214832  [19208/50000]\n",
      "loss: 2.536573  [20008/50000]\n",
      "loss: 1.836865  [20808/50000]\n",
      "loss: 2.023577  [21608/50000]\n",
      "loss: 1.864851  [22408/50000]\n",
      "loss: 2.503898  [23208/50000]\n",
      "loss: 1.852646  [24008/50000]\n",
      "loss: 2.073703  [24808/50000]\n",
      "loss: 2.646768  [25608/50000]\n",
      "loss: 2.187032  [26408/50000]\n",
      "loss: 1.962171  [27208/50000]\n",
      "loss: 2.097123  [28008/50000]\n",
      "loss: 2.064742  [28808/50000]\n",
      "loss: 2.207326  [29608/50000]\n",
      "loss: 2.054576  [30408/50000]\n",
      "loss: 1.848219  [31208/50000]\n",
      "loss: 2.057379  [32008/50000]\n",
      "loss: 2.232545  [32808/50000]\n",
      "loss: 2.117051  [33608/50000]\n",
      "loss: 2.006711  [34408/50000]\n",
      "loss: 2.065052  [35208/50000]\n",
      "loss: 2.139772  [36008/50000]\n",
      "loss: 1.973170  [36808/50000]\n",
      "loss: 1.490482  [37608/50000]\n",
      "loss: 2.284843  [38408/50000]\n",
      "loss: 1.414944  [39208/50000]\n",
      "loss: 1.769740  [40008/50000]\n",
      "loss: 2.267352  [40808/50000]\n",
      "loss: 2.188031  [41608/50000]\n",
      "loss: 1.618165  [42408/50000]\n",
      "loss: 1.859888  [43208/50000]\n",
      "loss: 1.793185  [44008/50000]\n",
      "loss: 1.597067  [44808/50000]\n",
      "loss: 1.597782  [45608/50000]\n",
      "loss: 1.975405  [46408/50000]\n",
      "loss: 1.688513  [47208/50000]\n",
      "loss: 1.746040  [48008/50000]\n",
      "loss: 2.088983  [48808/50000]\n",
      "loss: 1.722927  [49608/50000]\n",
      "Test Error: \n",
      " Accuracy: 31.0%, Avg loss: 1.910529 \n",
      "\n",
      "Epoch 3 took 19.14422287803609 seconds\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "\n",
    "epochs = 3\n",
    "for bsize in [4,8]:\n",
    "    model = CifarNetwork().to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train_dataloader_subset = DataLoader(training_data, batch_size=bsize)\n",
    "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "        epoch_time_start = time.perf_counter()\n",
    "        train(train_dataloader_subset, model, loss_fn, optimizer)\n",
    "        epoch_time_end = time.perf_counter()\n",
    "\n",
    "        test_start_time = epoch_time_end\n",
    "        accuracy = test(test_dataloader, model, loss_fn)\n",
    "        test_end_time = time.perf_counter()\n",
    "\n",
    "        epoch_duration = epoch_time_end - epoch_time_start\n",
    "        test_duration = test_end_time - test_start_time\n",
    "\n",
    "        print(f\"Epoch {epoch+1} took {epoch_duration} seconds\")\n",
    "        with open(\"minibatch_cifar_nn.csv\", \"a\") as fp:\n",
    "            wr = csv.writer(fp, dialect='excel')\n",
    "            # epoch_duration, epoch, batch_size, data_size, accuracy, test_duration\n",
    "            wr.writerow([epoch_duration, epoch, bsize, 50000, accuracy, test_duration])\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.327474  [    4/ 7500]\n",
      "loss: 2.261148  [  404/ 7500]\n",
      "loss: 2.245978  [  804/ 7500]\n",
      "loss: 2.303059  [ 1204/ 7500]\n",
      "loss: 2.350530  [ 1604/ 7500]\n",
      "loss: 2.333683  [ 2004/ 7500]\n",
      "loss: 2.311251  [ 2404/ 7500]\n",
      "loss: 2.334387  [ 2804/ 7500]\n",
      "loss: 2.294865  [ 3204/ 7500]\n",
      "loss: 2.355238  [ 3604/ 7500]\n",
      "loss: 2.301446  [ 4004/ 7500]\n",
      "loss: 2.268570  [ 4404/ 7500]\n",
      "loss: 2.306024  [ 4804/ 7500]\n",
      "loss: 2.344612  [ 5204/ 7500]\n",
      "loss: 2.292885  [ 5604/ 7500]\n",
      "loss: 2.306342  [ 6004/ 7500]\n",
      "loss: 2.295149  [ 6404/ 7500]\n",
      "loss: 2.261823  [ 6804/ 7500]\n",
      "loss: 2.296309  [ 7204/ 7500]\n",
      "Test Error: \n",
      " Accuracy: 10.0%, Avg loss: 2.302746 \n",
      "\n",
      "Epoch 1 took 3.8937714600469917 seconds\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.312066  [    4/ 7500]\n",
      "loss: 2.269509  [  404/ 7500]\n",
      "loss: 2.260037  [  804/ 7500]\n",
      "loss: 2.300740  [ 1204/ 7500]\n",
      "loss: 2.340050  [ 1604/ 7500]\n",
      "loss: 2.317119  [ 2004/ 7500]\n",
      "loss: 2.307583  [ 2404/ 7500]\n",
      "loss: 2.323736  [ 2804/ 7500]\n",
      "loss: 2.290844  [ 3204/ 7500]\n",
      "loss: 2.333382  [ 3604/ 7500]\n",
      "loss: 2.298424  [ 4004/ 7500]\n",
      "loss: 2.271605  [ 4404/ 7500]\n",
      "loss: 2.301646  [ 4804/ 7500]\n",
      "loss: 2.335497  [ 5204/ 7500]\n",
      "loss: 2.289192  [ 5604/ 7500]\n",
      "loss: 2.297925  [ 6004/ 7500]\n",
      "loss: 2.294712  [ 6404/ 7500]\n",
      "loss: 2.272392  [ 6804/ 7500]\n",
      "loss: 2.295164  [ 7204/ 7500]\n",
      "Test Error: \n",
      " Accuracy: 12.4%, Avg loss: 2.300218 \n",
      "\n",
      "Epoch 2 took 3.892189232050441 seconds\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.299529  [    4/ 7500]\n",
      "loss: 2.277144  [  404/ 7500]\n",
      "loss: 2.271777  [  804/ 7500]\n",
      "loss: 2.299050  [ 1204/ 7500]\n",
      "loss: 2.330950  [ 1604/ 7500]\n",
      "loss: 2.303397  [ 2004/ 7500]\n",
      "loss: 2.304195  [ 2404/ 7500]\n",
      "loss: 2.315445  [ 2804/ 7500]\n",
      "loss: 2.286585  [ 3204/ 7500]\n",
      "loss: 2.313548  [ 3604/ 7500]\n",
      "loss: 2.293565  [ 4004/ 7500]\n",
      "loss: 2.272710  [ 4404/ 7500]\n",
      "loss: 2.295591  [ 4804/ 7500]\n",
      "loss: 2.327098  [ 5204/ 7500]\n",
      "loss: 2.283478  [ 5604/ 7500]\n",
      "loss: 2.287860  [ 6004/ 7500]\n",
      "loss: 2.292922  [ 6404/ 7500]\n",
      "loss: 2.282351  [ 6804/ 7500]\n",
      "loss: 2.293796  [ 7204/ 7500]\n",
      "Test Error: \n",
      " Accuracy: 15.5%, Avg loss: 2.296821 \n",
      "\n",
      "Epoch 3 took 3.885272447951138 seconds\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.278534  [    4/15000]\n",
      "loss: 2.266256  [  404/15000]\n",
      "loss: 2.285147  [  804/15000]\n",
      "loss: 2.305834  [ 1204/15000]\n",
      "loss: 2.314344  [ 1604/15000]\n",
      "loss: 2.271103  [ 2004/15000]\n",
      "loss: 2.326113  [ 2404/15000]\n",
      "loss: 2.284937  [ 2804/15000]\n",
      "loss: 2.357876  [ 3204/15000]\n",
      "loss: 2.289370  [ 3604/15000]\n",
      "loss: 2.291240  [ 4004/15000]\n",
      "loss: 2.341656  [ 4404/15000]\n",
      "loss: 2.275314  [ 4804/15000]\n",
      "loss: 2.302628  [ 5204/15000]\n",
      "loss: 2.277658  [ 5604/15000]\n",
      "loss: 2.342525  [ 6004/15000]\n",
      "loss: 2.325196  [ 6404/15000]\n",
      "loss: 2.276618  [ 6804/15000]\n",
      "loss: 2.354194  [ 7204/15000]\n",
      "loss: 2.304107  [ 7604/15000]\n",
      "loss: 2.314449  [ 8004/15000]\n",
      "loss: 2.273187  [ 8404/15000]\n",
      "loss: 2.290653  [ 8804/15000]\n",
      "loss: 2.312736  [ 9204/15000]\n",
      "loss: 2.280491  [ 9604/15000]\n",
      "loss: 2.323273  [10004/15000]\n",
      "loss: 2.302975  [10404/15000]\n",
      "loss: 2.304777  [10804/15000]\n",
      "loss: 2.303849  [11204/15000]\n",
      "loss: 2.327983  [11604/15000]\n",
      "loss: 2.316199  [12004/15000]\n",
      "loss: 2.273718  [12404/15000]\n",
      "loss: 2.276484  [12804/15000]\n",
      "loss: 2.345857  [13204/15000]\n",
      "loss: 2.361033  [13604/15000]\n",
      "loss: 2.329839  [14004/15000]\n",
      "loss: 2.299927  [14404/15000]\n",
      "loss: 2.319550  [14804/15000]\n",
      "Test Error: \n",
      " Accuracy: 15.6%, Avg loss: 2.301844 \n",
      "\n",
      "Epoch 1 took 7.69540084793698 seconds\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.288137  [    4/15000]\n",
      "loss: 2.269808  [  404/15000]\n",
      "loss: 2.284713  [  804/15000]\n",
      "loss: 2.302591  [ 1204/15000]\n",
      "loss: 2.308331  [ 1604/15000]\n",
      "loss: 2.281070  [ 2004/15000]\n",
      "loss: 2.308213  [ 2404/15000]\n",
      "loss: 2.284997  [ 2804/15000]\n",
      "loss: 2.332624  [ 3204/15000]\n",
      "loss: 2.293169  [ 3604/15000]\n",
      "loss: 2.302942  [ 4004/15000]\n",
      "loss: 2.321189  [ 4404/15000]\n",
      "loss: 2.287889  [ 4804/15000]\n",
      "loss: 2.305372  [ 5204/15000]\n",
      "loss: 2.284096  [ 5604/15000]\n",
      "loss: 2.328513  [ 6004/15000]\n",
      "loss: 2.305700  [ 6404/15000]\n",
      "loss: 2.279787  [ 6804/15000]\n",
      "loss: 2.323840  [ 7204/15000]\n",
      "loss: 2.308194  [ 7604/15000]\n",
      "loss: 2.305180  [ 8004/15000]\n",
      "loss: 2.290188  [ 8404/15000]\n",
      "loss: 2.288859  [ 8804/15000]\n",
      "loss: 2.303810  [ 9204/15000]\n",
      "loss: 2.281800  [ 9604/15000]\n",
      "loss: 2.307670  [10004/15000]\n",
      "loss: 2.294323  [10404/15000]\n",
      "loss: 2.313540  [10804/15000]\n",
      "loss: 2.305205  [11204/15000]\n",
      "loss: 2.326240  [11604/15000]\n",
      "loss: 2.308250  [12004/15000]\n",
      "loss: 2.273015  [12404/15000]\n",
      "loss: 2.284747  [12804/15000]\n",
      "loss: 2.332304  [13204/15000]\n",
      "loss: 2.334380  [13604/15000]\n",
      "loss: 2.321987  [14004/15000]\n",
      "loss: 2.297285  [14404/15000]\n",
      "loss: 2.314802  [14804/15000]\n",
      "Test Error: \n",
      " Accuracy: 17.8%, Avg loss: 2.297861 \n",
      "\n",
      "Epoch 2 took 7.6901264659827575 seconds\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.291769  [    4/15000]\n",
      "loss: 2.270199  [  404/15000]\n",
      "loss: 2.288414  [  804/15000]\n",
      "loss: 2.302348  [ 1204/15000]\n",
      "loss: 2.296767  [ 1604/15000]\n",
      "loss: 2.285125  [ 2004/15000]\n",
      "loss: 2.288322  [ 2404/15000]\n",
      "loss: 2.276619  [ 2804/15000]\n",
      "loss: 2.319739  [ 3204/15000]\n",
      "loss: 2.290243  [ 3604/15000]\n",
      "loss: 2.306959  [ 4004/15000]\n",
      "loss: 2.310347  [ 4404/15000]\n",
      "loss: 2.292838  [ 4804/15000]\n",
      "loss: 2.301650  [ 5204/15000]\n",
      "loss: 2.288396  [ 5604/15000]\n",
      "loss: 2.319835  [ 6004/15000]\n",
      "loss: 2.279356  [ 6404/15000]\n",
      "loss: 2.285045  [ 6804/15000]\n",
      "loss: 2.301181  [ 7204/15000]\n",
      "loss: 2.314387  [ 7604/15000]\n",
      "loss: 2.296610  [ 8004/15000]\n",
      "loss: 2.298260  [ 8404/15000]\n",
      "loss: 2.289842  [ 8804/15000]\n",
      "loss: 2.299337  [ 9204/15000]\n",
      "loss: 2.277245  [ 9604/15000]\n",
      "loss: 2.297195  [10004/15000]\n",
      "loss: 2.277043  [10404/15000]\n",
      "loss: 2.322245  [10804/15000]\n",
      "loss: 2.305751  [11204/15000]\n",
      "loss: 2.326384  [11604/15000]\n",
      "loss: 2.302008  [12004/15000]\n",
      "loss: 2.247114  [12404/15000]\n",
      "loss: 2.286996  [12804/15000]\n",
      "loss: 2.329686  [13204/15000]\n",
      "loss: 2.326994  [13604/15000]\n",
      "loss: 2.322396  [14004/15000]\n",
      "loss: 2.290129  [14404/15000]\n",
      "loss: 2.313214  [14804/15000]\n",
      "Test Error: \n",
      " Accuracy: 18.7%, Avg loss: 2.284945 \n",
      "\n",
      "Epoch 3 took 7.680131027009338 seconds\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.323091  [    4/30000]\n",
      "loss: 2.311777  [  404/30000]\n",
      "loss: 2.322381  [  804/30000]\n",
      "loss: 2.330384  [ 1204/30000]\n",
      "loss: 2.295148  [ 1604/30000]\n",
      "loss: 2.359537  [ 2004/30000]\n",
      "loss: 2.278408  [ 2404/30000]\n",
      "loss: 2.299903  [ 2804/30000]\n",
      "loss: 2.284879  [ 3204/30000]\n",
      "loss: 2.300221  [ 3604/30000]\n",
      "loss: 2.318557  [ 4004/30000]\n",
      "loss: 2.311508  [ 4404/30000]\n",
      "loss: 2.302529  [ 4804/30000]\n",
      "loss: 2.313783  [ 5204/30000]\n",
      "loss: 2.321036  [ 5604/30000]\n",
      "loss: 2.274089  [ 6004/30000]\n",
      "loss: 2.313615  [ 6404/30000]\n",
      "loss: 2.333107  [ 6804/30000]\n",
      "loss: 2.289743  [ 7204/30000]\n",
      "loss: 2.288088  [ 7604/30000]\n",
      "loss: 2.278099  [ 8004/30000]\n",
      "loss: 2.327255  [ 8404/30000]\n",
      "loss: 2.295574  [ 8804/30000]\n",
      "loss: 2.302947  [ 9204/30000]\n",
      "loss: 2.303915  [ 9604/30000]\n",
      "loss: 2.264751  [10004/30000]\n",
      "loss: 2.278401  [10404/30000]\n",
      "loss: 2.315479  [10804/30000]\n",
      "loss: 2.318579  [11204/30000]\n",
      "loss: 2.295982  [11604/30000]\n",
      "loss: 2.296324  [12004/30000]\n",
      "loss: 2.325210  [12404/30000]\n",
      "loss: 2.305982  [12804/30000]\n",
      "loss: 2.300089  [13204/30000]\n",
      "loss: 2.260816  [13604/30000]\n",
      "loss: 2.305779  [14004/30000]\n",
      "loss: 2.345157  [14404/30000]\n",
      "loss: 2.303565  [14804/30000]\n",
      "loss: 2.306918  [15204/30000]\n",
      "loss: 2.265482  [15604/30000]\n",
      "loss: 2.305594  [16004/30000]\n",
      "loss: 2.281909  [16404/30000]\n",
      "loss: 2.293566  [16804/30000]\n",
      "loss: 2.293953  [17204/30000]\n",
      "loss: 2.298113  [17604/30000]\n",
      "loss: 2.290490  [18004/30000]\n",
      "loss: 2.305382  [18404/30000]\n",
      "loss: 2.287339  [18804/30000]\n",
      "loss: 2.302833  [19204/30000]\n",
      "loss: 2.306597  [19604/30000]\n",
      "loss: 2.319677  [20004/30000]\n",
      "loss: 2.316781  [20404/30000]\n",
      "loss: 2.259578  [20804/30000]\n",
      "loss: 2.284849  [21204/30000]\n",
      "loss: 2.309770  [21604/30000]\n",
      "loss: 2.278514  [22004/30000]\n",
      "loss: 2.302434  [22404/30000]\n",
      "loss: 2.288055  [22804/30000]\n",
      "loss: 2.304611  [23204/30000]\n",
      "loss: 2.270413  [23604/30000]\n",
      "loss: 2.269302  [24004/30000]\n",
      "loss: 2.271366  [24404/30000]\n",
      "loss: 2.256306  [24804/30000]\n",
      "loss: 2.297763  [25204/30000]\n",
      "loss: 2.295181  [25604/30000]\n",
      "loss: 2.290907  [26004/30000]\n",
      "loss: 2.304568  [26404/30000]\n",
      "loss: 2.284953  [26804/30000]\n",
      "loss: 2.254432  [27204/30000]\n",
      "loss: 2.264626  [27604/30000]\n",
      "loss: 2.264730  [28004/30000]\n",
      "loss: 2.331219  [28404/30000]\n",
      "loss: 2.345006  [28804/30000]\n",
      "loss: 2.275921  [29204/30000]\n",
      "loss: 2.350854  [29604/30000]\n",
      "Test Error: \n",
      " Accuracy: 15.9%, Avg loss: 2.281910 \n",
      "\n",
      "Epoch 1 took 15.394649882917292 seconds\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.269794  [    4/30000]\n",
      "loss: 2.287888  [  404/30000]\n",
      "loss: 2.324526  [  804/30000]\n",
      "loss: 2.292610  [ 1204/30000]\n",
      "loss: 2.248195  [ 1604/30000]\n",
      "loss: 2.258662  [ 2004/30000]\n",
      "loss: 2.239117  [ 2404/30000]\n",
      "loss: 2.178728  [ 2804/30000]\n",
      "loss: 2.298802  [ 3204/30000]\n",
      "loss: 2.197410  [ 3604/30000]\n",
      "loss: 2.267030  [ 4004/30000]\n",
      "loss: 2.334271  [ 4404/30000]\n",
      "loss: 2.217451  [ 4804/30000]\n",
      "loss: 2.273094  [ 5204/30000]\n",
      "loss: 2.195302  [ 5604/30000]\n",
      "loss: 2.245650  [ 6004/30000]\n",
      "loss: 2.142299  [ 6404/30000]\n",
      "loss: 2.333849  [ 6804/30000]\n",
      "loss: 2.306775  [ 7204/30000]\n",
      "loss: 2.336040  [ 7604/30000]\n",
      "loss: 2.185249  [ 8004/30000]\n",
      "loss: 2.343121  [ 8404/30000]\n",
      "loss: 2.334527  [ 8804/30000]\n",
      "loss: 2.298911  [ 9204/30000]\n",
      "loss: 2.252879  [ 9604/30000]\n",
      "loss: 2.202253  [10004/30000]\n",
      "loss: 1.919782  [10404/30000]\n",
      "loss: 2.364680  [10804/30000]\n",
      "loss: 2.273757  [11204/30000]\n",
      "loss: 2.242107  [11604/30000]\n",
      "loss: 2.256944  [12004/30000]\n",
      "loss: 1.715683  [12404/30000]\n",
      "loss: 2.645453  [12804/30000]\n",
      "loss: 2.228965  [13204/30000]\n",
      "loss: 2.183496  [13604/30000]\n",
      "loss: 2.337652  [14004/30000]\n",
      "loss: 2.266948  [14404/30000]\n",
      "loss: 2.022748  [14804/30000]\n",
      "loss: 2.190642  [15204/30000]\n",
      "loss: 2.184846  [15604/30000]\n",
      "loss: 2.538512  [16004/30000]\n",
      "loss: 2.087464  [16404/30000]\n",
      "loss: 2.005939  [16804/30000]\n",
      "loss: 2.052966  [17204/30000]\n",
      "loss: 2.206649  [17604/30000]\n",
      "loss: 2.013878  [18004/30000]\n",
      "loss: 2.069363  [18404/30000]\n",
      "loss: 1.714333  [18804/30000]\n",
      "loss: 2.103070  [19204/30000]\n",
      "loss: 1.719770  [19604/30000]\n",
      "loss: 3.187142  [20004/30000]\n",
      "loss: 2.341381  [20404/30000]\n",
      "loss: 1.798305  [20804/30000]\n",
      "loss: 1.753834  [21204/30000]\n",
      "loss: 2.021055  [21604/30000]\n",
      "loss: 2.346105  [22004/30000]\n",
      "loss: 2.051844  [22404/30000]\n",
      "loss: 2.106766  [22804/30000]\n",
      "loss: 2.389861  [23204/30000]\n",
      "loss: 1.628060  [23604/30000]\n",
      "loss: 1.833103  [24004/30000]\n",
      "loss: 2.041231  [24404/30000]\n",
      "loss: 1.640761  [24804/30000]\n",
      "loss: 2.330617  [25204/30000]\n",
      "loss: 2.523447  [25604/30000]\n",
      "loss: 2.073010  [26004/30000]\n",
      "loss: 2.408299  [26404/30000]\n",
      "loss: 1.810851  [26804/30000]\n",
      "loss: 1.688435  [27204/30000]\n",
      "loss: 2.185710  [27604/30000]\n",
      "loss: 1.827866  [28004/30000]\n",
      "loss: 3.022039  [28404/30000]\n",
      "loss: 2.439921  [28804/30000]\n",
      "loss: 2.101682  [29204/30000]\n",
      "loss: 2.491610  [29604/30000]\n",
      "Test Error: \n",
      " Accuracy: 28.3%, Avg loss: 2.002199 \n",
      "\n",
      "Epoch 2 took 15.339957962976769 seconds\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.704118  [    4/30000]\n",
      "loss: 2.254283  [  404/30000]\n",
      "loss: 2.233093  [  804/30000]\n",
      "loss: 2.090312  [ 1204/30000]\n",
      "loss: 1.289468  [ 1604/30000]\n",
      "loss: 2.106893  [ 2004/30000]\n",
      "loss: 1.453289  [ 2404/30000]\n",
      "loss: 1.761128  [ 2804/30000]\n",
      "loss: 2.484919  [ 3204/30000]\n",
      "loss: 1.887562  [ 3604/30000]\n",
      "loss: 1.795763  [ 4004/30000]\n",
      "loss: 1.766419  [ 4404/30000]\n",
      "loss: 2.145364  [ 4804/30000]\n",
      "loss: 2.061296  [ 5204/30000]\n",
      "loss: 1.549588  [ 5604/30000]\n",
      "loss: 1.621563  [ 6004/30000]\n",
      "loss: 1.856463  [ 6404/30000]\n",
      "loss: 2.111830  [ 6804/30000]\n",
      "loss: 2.191877  [ 7204/30000]\n",
      "loss: 2.148440  [ 7604/30000]\n",
      "loss: 1.838021  [ 8004/30000]\n",
      "loss: 2.420234  [ 8404/30000]\n",
      "loss: 2.399136  [ 8804/30000]\n",
      "loss: 2.125114  [ 9204/30000]\n",
      "loss: 2.249680  [ 9604/30000]\n",
      "loss: 2.371981  [10004/30000]\n",
      "loss: 1.355813  [10404/30000]\n",
      "loss: 2.023659  [10804/30000]\n",
      "loss: 2.145304  [11204/30000]\n",
      "loss: 2.113983  [11604/30000]\n",
      "loss: 2.148683  [12004/30000]\n",
      "loss: 1.486261  [12404/30000]\n",
      "loss: 2.409849  [12804/30000]\n",
      "loss: 1.841507  [13204/30000]\n",
      "loss: 2.043488  [13604/30000]\n",
      "loss: 2.425329  [14004/30000]\n",
      "loss: 1.773188  [14404/30000]\n",
      "loss: 1.757371  [14804/30000]\n",
      "loss: 2.302086  [15204/30000]\n",
      "loss: 2.565392  [15604/30000]\n",
      "loss: 2.325579  [16004/30000]\n",
      "loss: 2.090165  [16404/30000]\n",
      "loss: 1.460190  [16804/30000]\n",
      "loss: 2.202421  [17204/30000]\n",
      "loss: 1.744256  [17604/30000]\n",
      "loss: 1.890267  [18004/30000]\n",
      "loss: 1.677835  [18404/30000]\n",
      "loss: 1.451212  [18804/30000]\n",
      "loss: 1.667527  [19204/30000]\n",
      "loss: 1.446650  [19604/30000]\n",
      "loss: 2.715493  [20004/30000]\n",
      "loss: 2.082196  [20404/30000]\n",
      "loss: 1.583219  [20804/30000]\n",
      "loss: 1.810960  [21204/30000]\n",
      "loss: 1.714535  [21604/30000]\n",
      "loss: 1.931593  [22004/30000]\n",
      "loss: 2.038688  [22404/30000]\n",
      "loss: 1.563930  [22804/30000]\n",
      "loss: 2.667722  [23204/30000]\n",
      "loss: 1.487380  [23604/30000]\n",
      "loss: 1.593522  [24004/30000]\n",
      "loss: 1.489578  [24404/30000]\n",
      "loss: 1.644916  [24804/30000]\n",
      "loss: 2.021964  [25204/30000]\n",
      "loss: 2.486307  [25604/30000]\n",
      "loss: 1.853930  [26004/30000]\n",
      "loss: 2.277707  [26404/30000]\n",
      "loss: 1.574070  [26804/30000]\n",
      "loss: 1.694340  [27204/30000]\n",
      "loss: 2.250864  [27604/30000]\n",
      "loss: 2.048146  [28004/30000]\n",
      "loss: 3.125685  [28404/30000]\n",
      "loss: 2.272634  [28804/30000]\n",
      "loss: 1.884357  [29204/30000]\n",
      "loss: 2.178581  [29604/30000]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.855838 \n",
      "\n",
      "Epoch 3 took 15.357803889899515 seconds\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.325996  [    4/60000]\n",
      "loss: 2.259075  [  404/60000]\n",
      "loss: 2.295674  [  804/60000]\n",
      "loss: 2.295729  [ 1204/60000]\n",
      "loss: 2.340973  [ 1604/60000]\n",
      "loss: 2.315989  [ 2004/60000]\n",
      "loss: 2.263542  [ 2404/60000]\n",
      "loss: 2.282730  [ 2804/60000]\n",
      "loss: 2.366196  [ 3204/60000]\n",
      "loss: 2.340714  [ 3604/60000]\n",
      "loss: 2.341918  [ 4004/60000]\n",
      "loss: 2.292431  [ 4404/60000]\n",
      "loss: 2.301159  [ 4804/60000]\n",
      "loss: 2.306125  [ 5204/60000]\n",
      "loss: 2.328618  [ 5604/60000]\n",
      "loss: 2.342939  [ 6004/60000]\n",
      "loss: 2.296218  [ 6404/60000]\n",
      "loss: 2.271972  [ 6804/60000]\n",
      "loss: 2.303225  [ 7204/60000]\n",
      "loss: 2.318414  [ 7604/60000]\n",
      "loss: 2.336765  [ 8004/60000]\n",
      "loss: 2.316800  [ 8404/60000]\n",
      "loss: 2.306967  [ 8804/60000]\n",
      "loss: 2.268952  [ 9204/60000]\n",
      "loss: 2.318991  [ 9604/60000]\n",
      "loss: 2.321527  [10004/60000]\n",
      "loss: 2.287926  [10404/60000]\n",
      "loss: 2.320440  [10804/60000]\n",
      "loss: 2.295634  [11204/60000]\n",
      "loss: 2.343107  [11604/60000]\n",
      "loss: 2.337053  [12004/60000]\n",
      "loss: 2.287931  [12404/60000]\n",
      "loss: 2.274286  [12804/60000]\n",
      "loss: 2.326048  [13204/60000]\n",
      "loss: 2.340231  [13604/60000]\n",
      "loss: 2.297007  [14004/60000]\n",
      "loss: 2.283415  [14404/60000]\n",
      "loss: 2.329290  [14804/60000]\n",
      "loss: 2.292199  [15204/60000]\n",
      "loss: 2.329333  [15604/60000]\n",
      "loss: 2.291597  [16004/60000]\n",
      "loss: 2.315763  [16404/60000]\n",
      "loss: 2.266673  [16804/60000]\n",
      "loss: 2.265598  [17204/60000]\n",
      "loss: 2.313835  [17604/60000]\n",
      "loss: 2.336896  [18004/60000]\n",
      "loss: 2.286390  [18404/60000]\n",
      "loss: 2.278537  [18804/60000]\n",
      "loss: 2.283365  [19204/60000]\n",
      "loss: 2.294187  [19604/60000]\n",
      "loss: 2.305683  [20004/60000]\n",
      "loss: 2.307411  [20404/60000]\n",
      "loss: 2.313389  [20804/60000]\n",
      "loss: 2.288485  [21204/60000]\n",
      "loss: 2.268296  [21604/60000]\n",
      "loss: 2.284450  [22004/60000]\n",
      "loss: 2.301400  [22404/60000]\n",
      "loss: 2.284439  [22804/60000]\n",
      "loss: 2.284410  [23204/60000]\n",
      "loss: 2.215621  [23604/60000]\n",
      "loss: 2.240118  [24004/60000]\n",
      "loss: 2.258800  [24404/60000]\n",
      "loss: 2.310181  [24804/60000]\n",
      "loss: 2.316840  [25204/60000]\n",
      "loss: 2.312477  [25604/60000]\n",
      "loss: 2.253195  [26004/60000]\n",
      "loss: 2.300985  [26404/60000]\n",
      "loss: 2.276882  [26804/60000]\n",
      "loss: 2.242478  [27204/60000]\n",
      "loss: 2.229460  [27604/60000]\n",
      "loss: 2.217030  [28004/60000]\n",
      "loss: 2.313669  [28404/60000]\n",
      "loss: 2.303916  [28804/60000]\n",
      "loss: 2.313553  [29204/60000]\n",
      "loss: 2.250873  [29604/60000]\n",
      "loss: 2.264711  [30004/60000]\n",
      "loss: 2.245772  [30404/60000]\n",
      "loss: 2.298620  [30804/60000]\n",
      "loss: 2.187687  [31204/60000]\n",
      "loss: 2.390738  [31604/60000]\n",
      "loss: 2.179649  [32004/60000]\n",
      "loss: 2.177849  [32404/60000]\n",
      "loss: 2.321774  [32804/60000]\n",
      "loss: 2.183040  [33204/60000]\n",
      "loss: 2.349231  [33604/60000]\n",
      "loss: 2.206615  [34004/60000]\n",
      "loss: 2.235482  [34404/60000]\n",
      "loss: 2.104641  [34804/60000]\n",
      "loss: 2.037737  [35204/60000]\n",
      "loss: 2.375611  [35604/60000]\n",
      "loss: 2.107773  [36004/60000]\n",
      "loss: 2.083897  [36404/60000]\n",
      "loss: 2.240915  [36804/60000]\n",
      "loss: 1.911680  [37204/60000]\n",
      "loss: 2.076561  [37604/60000]\n",
      "loss: 1.873674  [38004/60000]\n",
      "loss: 2.109531  [38404/60000]\n",
      "loss: 1.928652  [38804/60000]\n",
      "loss: 1.868398  [39204/60000]\n",
      "loss: 1.955720  [39604/60000]\n",
      "loss: 1.740627  [40004/60000]\n",
      "loss: 2.126726  [40404/60000]\n",
      "loss: 2.778762  [40804/60000]\n",
      "loss: 1.596856  [41204/60000]\n",
      "loss: 2.332791  [41604/60000]\n",
      "loss: 2.081101  [42004/60000]\n",
      "loss: 1.845510  [42404/60000]\n",
      "loss: 2.040937  [42804/60000]\n",
      "loss: 2.045491  [43204/60000]\n",
      "loss: 1.713372  [43604/60000]\n",
      "loss: 1.986569  [44004/60000]\n",
      "loss: 2.080868  [44404/60000]\n",
      "loss: 2.149551  [44804/60000]\n",
      "loss: 2.126860  [45204/60000]\n",
      "loss: 1.490250  [45604/60000]\n",
      "loss: 1.980244  [46004/60000]\n",
      "loss: 2.113427  [46404/60000]\n",
      "loss: 2.155386  [46804/60000]\n",
      "loss: 1.989582  [47204/60000]\n",
      "loss: 2.073601  [47604/60000]\n",
      "loss: 1.747269  [48004/60000]\n",
      "loss: 2.036027  [48404/60000]\n",
      "loss: 2.585279  [48804/60000]\n",
      "loss: 1.646355  [49204/60000]\n",
      "loss: 1.750637  [49604/60000]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 50000 is out of bounds for axis 0 with size 50000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m epoch_time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m epoch_time_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     19\u001b[0m test_start_time \u001b[38;5;241m=\u001b[39m epoch_time_end\n",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      2\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m      5\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Compute prediction error\u001b[39;00m\n",
      "File \u001b[0;32m~/chz_sok_experiments/chz-sok-nn-experiments/crypten_venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/chz_sok_experiments/chz-sok-nn-experiments/crypten_venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/chz_sok_experiments/chz-sok-nn-experiments/crypten_venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/chz_sok_experiments/chz-sok-nn-experiments/crypten_venv/lib/python3.8/site-packages/torch/utils/data/dataset.py:419\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/chz_sok_experiments/chz-sok-nn-experiments/crypten_venv/lib/python3.8/site-packages/torch/utils/data/dataset.py:419\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/chz_sok_experiments/chz-sok-nn-experiments/crypten_venv/lib/python3.8/site-packages/torchvision/datasets/cifar.py:112\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, Any]:\n\u001b[1;32m    105\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m        index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m        tuple: (image, target) where target is index of the target class.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m     img, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets[index]\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# to return a PIL Image\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 50000 is out of bounds for axis 0 with size 50000"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "test_dataloader = DataLoader(test_data, batch_size=4)\n",
    "\n",
    "\n",
    "epochs = 3\n",
    "for datasize in [7500,15000,30000,50000]:\n",
    "    model = CifarNetwork().to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        subset_training_data = Subset(training_data, range(datasize))\n",
    "        train_dataloader_subset = DataLoader(subset_training_data, batch_size=batch_size)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "        epoch_time_start = time.perf_counter()\n",
    "        train(train_dataloader_subset, model, loss_fn, optimizer)\n",
    "        epoch_time_end = time.perf_counter()\n",
    "\n",
    "        test_start_time = epoch_time_end\n",
    "        accuracy = test(test_dataloader, model, loss_fn)\n",
    "        test_end_time = time.perf_counter()\n",
    "\n",
    "        epoch_duration = epoch_time_end - epoch_time_start\n",
    "        test_duration = test_end_time - test_start_time\n",
    "\n",
    "        print(f\"Epoch {epoch+1} took {epoch_duration} seconds\")\n",
    "        with open(\"datasize_cifar_nn.csv\", \"a\") as fp:\n",
    "            wr = csv.writer(fp, dialect='excel')\n",
    "            # epoch_duration, epoch, batch_size, data_size, accuracy, test_duration\n",
    "            wr.writerow([epoch_duration, epoch, batch_size, datasize, accuracy, test_duration])\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"cifar_model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for NeuralNetwork:\n\tMissing key(s) in state_dict: \"linear_relu_stack.0.weight\", \"linear_relu_stack.0.bias\", \"linear_relu_stack.2.weight\", \"linear_relu_stack.2.bias\", \"linear_relu_stack.4.weight\", \"linear_relu_stack.4.bias\". \n\tUnexpected key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m NeuralNetwork()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/chz_sok_experiments/chz-sok-nn-experiments/crypten_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for NeuralNetwork:\n\tMissing key(s) in state_dict: \"linear_relu_stack.0.weight\", \"linear_relu_stack.0.bias\", \"linear_relu_stack.2.weight\", \"linear_relu_stack.2.bias\", \"linear_relu_stack.4.weight\", \"linear_relu_stack.4.bias\". \n\tUnexpected key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\". "
     ]
    }
   ],
   "source": [
    "model = CifarNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"cifar_model.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralnet_experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
